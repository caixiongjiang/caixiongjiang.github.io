<!DOCTYPE html>
<html><head>
<title>深度学习笔记（4-5节） </title>




<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="">
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
<meta content="telephone=no" name="format-detection">
<meta name="description" content="毕业设计可能会使用深度学习，从暑假开始从头学习">
<meta name="renderer" content="webkit">
<meta name="theme-color" content="#ffffff">



<meta property="og:title" content="深度学习笔记（4-5节） " />
<meta property="og:description" content="毕业设计可能会使用深度学习，从暑假开始从头学习" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2022-07-19T18:18:05+08:00" />
<meta property="article:modified_time" content="2022-07-20T09:19:06+08:00" />












<link type="text/css" rel="stylesheet" href="/vendor/css/bootstrap.min.css">
<script src="/vendor/js/vue.min.js" ></script>


  




<link rel="icon" href="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/avater.jfif">



<link rel="stylesheet" href="https://caixiongjiang.github.io/scss/journal.min.3f72a5fc8f5b5dd732a4b476aced0eece2156958d9d414316494ddb10593ddf7.css" integrity="sha256-P3Kl/I9bXdcypLR2rO0O7OIVaVjZ1BQxZJTdsQWT3fc=" media="screen">



<link rel="stylesheet" href="https://caixiongjiang.github.io/scss/dark-mode.min.c0082f0b082177f6fb3768ff91439a097de49689bd26f4d49f76d94ebb81e02d.css" integrity="sha256-wAgvCwghd/b7N2j/kUOaCX3klom9JvTUn3bZTruB4C0=" media="screen">


<script src="/js/loadCSS.js"></script>
<script>
  loadCSS("https://fonts.googleapis.com/css?family=Fira+Mono|Material+Icons");
</script>


  
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script"
async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
MathJax.Hub.Queue(function() {



var all = MathJax.Hub.getAllJax(), i;
for(i = 0; i < all.length; i += 1) {
all[i].SourceElement().parentNode.className += ' has-jax';
}
});
</script>

<style>
code.has-jax {
font: inherit;
font-size: 100%;
background: inherit;
border: inherit;
color: #515151;
}
</style>





  
    <script src="/js/toc.js"></script>
  











<script src="https://cdn.jsdelivr.net/npm/twikoo@1.5.11/dist/twikoo.all.min.js"></script>




</head>
<body>
    	<div id="app"><div ref="sideContainer" class="side-container">
    
    <a class="a-block nav-head false" href="https://caixiongjiang.github.io/">
    
        <div class="nav-title">
            🌀Jarson Cai&#39;s Blog
        </div>
        
        <div class="nav-subtitle">
            头脑是日用品，不是装饰品
        </div>
        
    </a>

    <div class="nav-link-list">
        
        
            
            
            
                
            
            
            
            <a class="a-block nav-link-item active" href="/blog">
                文章📖
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/categories">
                分类📌
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/tags">
                标签🏷️
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/series">
                系列📚
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/archive">
                归档📃
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/about">
                关于👋
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/friends">
                友链🔗
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/index.xml">
                RSS📢
            </a>
            
        
    </div>

    

    <div class="nav-footer">
        
<a href="https://github.com/caixiongjiang">
    GitHub
</a>
<br>

<a href="mailto:nau_cxj@163.com">
    Email
</a>
<br>

<a href="https://leetcode-cn.com/u/cai-xiong-jiang/">
    Leetcode
</a>
<br>

        <hr>
        
魔改自 <a href="https://github.com/riba2534/hugo-blog">Riba2534</a> by <a href="https://caixiongjiang.github.io">Jarson Cai</a>
<br>

&copy;
	
	2024 🌀Jarson Cai&#39;s Blog
	
    </div>
    
</div><div ref="extraContainer" class="extra-container">
    
    
    <div class="toc animated-visibility" :class="{ invisible: scrollY <= 140 }">


	<div class="toc-content">
	
		
		
		
		<center>- 目录 -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a04-5%e8%8a%82" onclick="onNavClick(`#深度学习4-5节-nav`)" id="深度学习4-5节-nav">
									深度学习（4-5节）
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%a4%9a%e5%b1%82%e7%9a%84%e6%b7%b1%e5%b1%82%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" onclick="onNavClick(`#多层的深层神经网络-nav`)" id="多层的深层神经网络-nav">
									多层的深层神经网络
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%a1%a8%e7%a4%ba" onclick="onNavClick(`#神经网络的表示-nav`)" id="神经网络的表示-nav">
									神经网络的表示
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%b7%b1%e5%b1%82%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad" onclick="onNavClick(`#深层网络中的前向传播-nav`)" id="深层网络中的前向传播-nav">
									深层网络中的前向传播
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a0%b8%e5%af%b9%e7%9f%a9%e9%98%b5%e4%b8%ad%e7%9a%84%e7%bb%b4%e6%95%b0" onclick="onNavClick(`#核对矩阵中的维数-nav`)" id="核对矩阵中的维数-nav">
									核对矩阵中的维数
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%8f%82%e6%95%b0vs%e8%b6%85%e5%8f%82%e6%95%b0" onclick="onNavClick(`#参数vs超参数-nav`)" id="参数vs超参数-nav">
									参数vs超参数
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bd%9c%e4%b8%9a%e4%b8%89" onclick="onNavClick(`#作业三-nav`)" id="作业三-nav">
									作业三
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%9c%89%e6%95%88%e8%bf%90%e8%a1%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" onclick="onNavClick(`#有效运行神经网络-nav`)" id="有效运行神经网络-nav">
									有效运行神经网络
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e8%ae%ad%e7%bb%83%e9%9b%86%e5%92%8c%e6%b5%8b%e8%af%95%e9%9b%86%e5%88%92%e5%88%86" onclick="onNavClick(`#训练集和测试集划分-nav`)" id="训练集和测试集划分-nav">
									训练集和测试集划分
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%81%8f%e5%b7%ae%e5%92%8c%e6%96%b9%e5%b7%ae" onclick="onNavClick(`#偏差和方差-nav`)" id="偏差和方差-nav">
									偏差和方差
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e8%b0%83%e4%bc%98%e7%9a%84%e8%bf%87%e7%a8%8b" onclick="onNavClick(`#模型评估调优的过程-nav`)" id="模型评估调优的过程-nav">
									模型评估调优的过程
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%ad%a3%e5%88%99%e5%8c%96" onclick="onNavClick(`#正则化-nav`)" id="正则化-nav">
									正则化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%ad%a3%e5%88%99%e5%8c%96%e5%a6%82%e4%bd%95%e9%a2%84%e9%98%b2%e8%bf%87%e6%8b%9f%e5%90%88" onclick="onNavClick(`#正则化如何预防过拟合-nav`)" id="正则化如何预防过拟合-nav">
									正则化如何预防过拟合
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#dropout%e6%ad%a3%e5%88%99%e5%8c%96" onclick="onNavClick(`#dropout正则化-nav`)" id="dropout正则化-nav">
									dropout正则化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%95%b0%e6%8d%ae%e6%89%a9%e5%ae%b9" onclick="onNavClick(`#数据扩容-nav`)" id="数据扩容-nav">
									数据扩容
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%bd%92%e4%b8%80%e5%8c%96%e8%be%93%e5%85%a5" onclick="onNavClick(`#归一化输入-nav`)" id="归一化输入-nav">
									归一化输入
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e5%92%8c%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8" onclick="onNavClick(`#梯度消失和梯度爆炸-nav`)" id="梯度消失和梯度爆炸-nav">
									梯度消失和梯度爆炸
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a2%af%e5%ba%a6%e6%a3%80%e9%aa%8c" onclick="onNavClick(`#梯度检验-nav`)" id="梯度检验-nav">
									梯度检验
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bd%9c%e4%b8%9a%e5%9b%9b" onclick="onNavClick(`#作业四-nav`)" id="作业四-nav">
									作业四
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
    
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>
        
        <a class="pagination-action" v-on:click="toggleDarkMode">
            <i class="material-icons pagination-action-icon" v-if="isDarkMode">
                brightness_4
            </i>
            <i class="material-icons pagination-action-icon" v-else="isDarkMode">
                brightness_7
            </i>
        </a>
        
        
    </div>
</div>
<div class="single-column-drawer-container" ref="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            
            
            
                
                
                
                    
                
                
                
                <a class="a-block drawer-menu-item active" href="/blog">
                    文章📖
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/categories">
                    分类📌
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/tags">
                    标签🏷️
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/series">
                    系列📚
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/archive">
                    归档📃
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/about">
                    关于👋
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/friends">
                    友链🔗
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/index.xml">
                    RSS📢
                </a>
                
            
            
            <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- 目录 -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a04-5%e8%8a%82" onclick="onNavClick(`#深度学习4-5节-nav`)" id="深度学习4-5节-nav">
									深度学习（4-5节）
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%a4%9a%e5%b1%82%e7%9a%84%e6%b7%b1%e5%b1%82%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" onclick="onNavClick(`#多层的深层神经网络-nav`)" id="多层的深层神经网络-nav">
									多层的深层神经网络
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%a1%a8%e7%a4%ba" onclick="onNavClick(`#神经网络的表示-nav`)" id="神经网络的表示-nav">
									神经网络的表示
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%b7%b1%e5%b1%82%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad" onclick="onNavClick(`#深层网络中的前向传播-nav`)" id="深层网络中的前向传播-nav">
									深层网络中的前向传播
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a0%b8%e5%af%b9%e7%9f%a9%e9%98%b5%e4%b8%ad%e7%9a%84%e7%bb%b4%e6%95%b0" onclick="onNavClick(`#核对矩阵中的维数-nav`)" id="核对矩阵中的维数-nav">
									核对矩阵中的维数
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%8f%82%e6%95%b0vs%e8%b6%85%e5%8f%82%e6%95%b0" onclick="onNavClick(`#参数vs超参数-nav`)" id="参数vs超参数-nav">
									参数vs超参数
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bd%9c%e4%b8%9a%e4%b8%89" onclick="onNavClick(`#作业三-nav`)" id="作业三-nav">
									作业三
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%9c%89%e6%95%88%e8%bf%90%e8%a1%8c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" onclick="onNavClick(`#有效运行神经网络-nav`)" id="有效运行神经网络-nav">
									有效运行神经网络
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e8%ae%ad%e7%bb%83%e9%9b%86%e5%92%8c%e6%b5%8b%e8%af%95%e9%9b%86%e5%88%92%e5%88%86" onclick="onNavClick(`#训练集和测试集划分-nav`)" id="训练集和测试集划分-nav">
									训练集和测试集划分
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%81%8f%e5%b7%ae%e5%92%8c%e6%96%b9%e5%b7%ae" onclick="onNavClick(`#偏差和方差-nav`)" id="偏差和方差-nav">
									偏差和方差
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e8%b0%83%e4%bc%98%e7%9a%84%e8%bf%87%e7%a8%8b" onclick="onNavClick(`#模型评估调优的过程-nav`)" id="模型评估调优的过程-nav">
									模型评估调优的过程
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%ad%a3%e5%88%99%e5%8c%96" onclick="onNavClick(`#正则化-nav`)" id="正则化-nav">
									正则化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%ad%a3%e5%88%99%e5%8c%96%e5%a6%82%e4%bd%95%e9%a2%84%e9%98%b2%e8%bf%87%e6%8b%9f%e5%90%88" onclick="onNavClick(`#正则化如何预防过拟合-nav`)" id="正则化如何预防过拟合-nav">
									正则化如何预防过拟合
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#dropout%e6%ad%a3%e5%88%99%e5%8c%96" onclick="onNavClick(`#dropout正则化-nav`)" id="dropout正则化-nav">
									dropout正则化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%95%b0%e6%8d%ae%e6%89%a9%e5%ae%b9" onclick="onNavClick(`#数据扩容-nav`)" id="数据扩容-nav">
									数据扩容
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e5%bd%92%e4%b8%80%e5%8c%96%e8%be%93%e5%85%a5" onclick="onNavClick(`#归一化输入-nav`)" id="归一化输入-nav">
									归一化输入
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e5%92%8c%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8" onclick="onNavClick(`#梯度消失和梯度爆炸-nav`)" id="梯度消失和梯度爆炸-nav">
									梯度消失和梯度爆炸
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%a2%af%e5%ba%a6%e6%a3%80%e9%aa%8c" onclick="onNavClick(`#梯度检验-nav`)" id="梯度检验-nav">
									梯度检验
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bd%9c%e4%b8%9a%e5%9b%9b" onclick="onNavClick(`#作业四-nav`)" id="作业四-nav">
									作业四
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
            
        </div>
    </div>
</div>
<transition name="fade">
    <div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div ref="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a ref="navTitle" class="navbar-brand" href="https://caixiongjiang.github.io/">
            🌀Jarson Cai&#39;s Blog
        </a>
        
        <button type="button" class="nav-darkmode-toggle" v-on:click="toggleDarkMode">
            <i class="material-icons" v-if="isDarkMode">
                brightness_4
            </i>
            <i class="material-icons" v-else="isDarkMode">
                brightness_7
            </i>
        </button>
        
    </div>
</nav>
<div class="single-column-header-container" ref="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="https://caixiongjiang.github.io/">
        <div class="single-column-header-title">🌀Jarson Cai&#39;s Blog</div>
        
        <div class="single-column-header-subtitle">头脑是日用品，不是装饰品</div>
        

    </a>
</div>

            <div id="content">
<div ref="streamContainer" class="stream-container">
    <div class="post-list-container post-list-container-shadow">
        <div class="post">
            
            
            
                
            

            <div class="post-head-wrapper"
                
                    
                    
                    style="background-image: url('https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img_title.jpg')"
                    
                
            >
                <div class="post-title">
                    深度学习笔记（4-5节） 
                    
                    <div class="post-subtitle">
                        毕业设计可能会使用深度学习，从暑假开始从头学习
                    </div>
                    
                    <div class="post-meta">
                        
                        <time itemprop="datePublished">
                            2022-07-19 18:18
                        </time>
                        

                        
                            <i class="material-icons" style="">folder</i>
                                <a href="/categories/">[深度学习]</a>
                                &nbsp;
                        

                        
                            <i class="material-icons" style="">label</i>
                            
                                <a href="/tags/deep_learning">Deep_learning</a>
                                &nbsp;
                            
                        
                        
                            <i class="material-icons" style="">schedule</i>
                            

                            
                            

                            
                            56 min
                            
                            40 s.
                        
                    </div>
                </div>
            </div>
            
            <div class="post-body-wrapper">
                
                <div class="post-body" v-pre>
                
                    <h2 id="深度学习4-5节">深度学习（4-5节）</h2>
<h3 id="多层的深层神经网络">多层的深层神经网络</h3>
<h4 id="神经网络的表示">神经网络的表示</h4>
<p>1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4</p>
<p>2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量</p>
<p>3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$</p>
<h4 id="深层网络中的前向传播">深层网络中的前向传播</h4>
<p>神经网络中每层的前向传播过程：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\
a^{[l]}=g^{[l]}(z^{[l]})\
l代表层数
$$
<strong>如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。</strong></p>
<h4 id="核对矩阵中的维数">核对矩阵中的维数</h4>
<p>如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。</p>
<p>通过一个具体的网络来手动计算一下维度：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img7.jpg" alt=""></p>
<p>可以写出该网络的部分参数如下：
$$
n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1
$$
由于前向传播的公式为：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\
a^{[l]}=g^{[l]}(z^{[l]})
$$</p>
<blockquote>
<p>需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。</p>
</blockquote>
<p>1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。</p>
<p>2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$</p>
<p>所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。</p>
<p><em>可以总结出来的是：</em>
$$
W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\
b^{[l]}的维度一定是n^{[l]}\times 1
$$
<em>同理在反向传播时：</em>
$$
dW和W的维度必须保持一致，db必须和b保持一致
$$
因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。</p>
<h4 id="参数vs超参数">参数vs超参数</h4>
<p>参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$</p>
<p>超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。</p>
<h4 id="作业三">作业三</h4>
<p>一个神经网络工作原理的模型如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img8.jpg" alt=""></p>
<p>多层网络模型的前向传播和后向传播过程如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img9.jpg" alt=""></p>
<ul>
<li>实现一个L层神经网络</li>
</ul>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 84
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 85
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 86
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 87
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 88
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 89
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 90
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 91
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 92
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 93
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 94
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 95
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 96
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 97
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 98
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 99
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">100
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">101
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">102
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">103
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">104
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">105
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">106
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">107
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">108
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">109
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">110
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">111
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">112
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">113
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">114
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">115
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">116
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">117
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">118
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">119
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">120
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">121
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">122
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">123
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">124
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">125
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">126
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">127
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">128
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">129
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">130
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">131
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">132
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">133
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">134
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">135
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">136
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">137
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">138
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">139
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">140
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">141
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">142
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">143
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">144
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">145
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">146
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">147
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">148
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">149
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">150
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">151
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">152
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">153
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">154
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">155
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">156
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">157
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">158
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">159
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">160
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">161
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">162
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">163
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">164
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">165
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">166
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">167
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">168
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">169
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">170
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">171
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">172
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">173
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">174
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">175
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">176
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">177
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">178
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">179
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">180
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">181
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">182
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">183
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">184
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">185
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">186
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">187
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">188
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">189
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">190
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">191
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">192
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">193
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">194
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">195
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">196
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">197
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">198
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">199
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">200
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">201
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">202
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">203
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">204
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">205
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">206
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">207
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">208
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">209
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">210
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">211
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">212
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">213
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">214
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">215
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">216
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">217
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">218
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">219
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">220
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">221
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">222
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">223
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">224
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">225
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">226
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">227
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">228
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">229
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">230
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">231
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">232
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">233
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">234
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">235
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">236
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">237
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">238
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">239
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">240
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">241
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">242
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">243
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">244
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">245
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">246
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">247
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">248
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">249
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">250
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">251
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">252
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">dnn_utils_v2</span> <span style="color:#8b008b;font-weight:bold">import</span> sigmoid, sigmoid_backward, relu, relu_backward
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">numpy</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">np</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">initialize_parameters_deep</span>(layer_dims):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    layer_dims -- 包含网络中每一层的维度的Python List
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- Python参数字典 &#34;W1&#34;, &#34;b1&#34;, ..., &#34;WL&#34;, &#34;bL&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    bl -- bias vector of shape (layer_dims[l], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    np.random.seed(<span style="color:#b452cd">3</span>)
</span></span><span style="display:flex;"><span>    parameters = {}
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(layer_dims) <span style="color:#228b22"># 网络层数</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L):
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#34;W&#34;</span> + <span style="color:#658b00">str</span>(i)] = np.random.randn(layer_dims[i], layer_dims[i - <span style="color:#b452cd">1</span>]) * <span style="color:#b452cd">0.01</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#34;b&#34;</span> + <span style="color:#658b00">str</span>(i)] = np.zeros(layer_dims[i], <span style="color:#b452cd">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">assert</span>(parameters[<span style="color:#cd5555">&#34;W&#34;</span> + <span style="color:#658b00">str</span>(i)].shape == (layer_dims[i], layer_dims[i - <span style="color:#b452cd">1</span>]))
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">assert</span>(parameters[<span style="color:#cd5555">&#34;b&#34;</span> + <span style="color:#658b00">str</span>(i)].shape == (layer_dims[i], <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">linear_forward</span>(A, W, b):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    实现前向传播的线性部分
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    A -- 来自上一层的激活结果 (或者为初始输入数据): (前一层的隐藏单元数, 样本数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Z -- 激活函数的输入或称为预激活参数 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- Python参数字典包含&#34;A&#34;, &#34;W&#34; and &#34;b&#34; ; stored for computing the backward pass efficiently
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Z = np.dot(W, A) + b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(Z.shape == (W.shape[<span style="color:#b452cd">0</span>], A.shape[<span style="color:#b452cd">1</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cache = (A, W, b)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> Z, cache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">linear_activation_forward</span>(A_prev, W, b, activation):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    实现 线性——&gt;激活层 的前向传播
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    A_prev -- 来自上层的激活结果 (或为初始输入数据): (前一层的隐藏单元数, 样本数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    activation -- 当前隐藏层使用的激活函数: &#34;sigmoid&#34; or &#34;relu&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    A -- 激活函数的输出,也称为激活后值 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- Python字典包含 &#34;线性缓存&#34; and &#34;激活缓存&#34;;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">             stored for computing the backward pass efficiently
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">if</span> activation == <span style="color:#cd5555">&#34;sigmoid&#34;</span>:
</span></span><span style="display:flex;"><span>        Z, linear_cache = linear_forward(A_prev, W, b)
</span></span><span style="display:flex;"><span>        A, activation_cache = sigmoid(Z)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">elif</span> activation == <span style="color:#cd5555">&#34;relu&#34;</span>:
</span></span><span style="display:flex;"><span>        Z, linear_cache = linear_forward(A_prev, W, b)
</span></span><span style="display:flex;"><span>        A, activation_cache = relu(Z)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(A.shape == (W.shape[<span style="color:#b452cd">0</span>], A.shape[<span style="color:#b452cd">1</span>]))
</span></span><span style="display:flex;"><span>    cache = (linear_cache, activation_cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> A, cache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 为了实现L层神经网络更加方便，需要将前L-1层的激活函数设置为ReLU，最后一层输出层激活函数设置为Sigmoid</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">L_model_forward</span>(X, parameters):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    实现前向传播： the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- 初始数据, numpy array of shape (输入层大小, 样本数量)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- 初始化deep网络的参数输出
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    AL -- 上一层激活后的值
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    caches -- cache的列表:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    caches = []
</span></span><span style="display:flex;"><span>    A = X
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(parameters) // <span style="color:#b452cd">2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># 前L-1层为relu激活函数</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L):
</span></span><span style="display:flex;"><span>        A_prev = A
</span></span><span style="display:flex;"><span>        A, cache = linear_activation_forward(A_prev, parameters[<span style="color:#cd5555">&#34;W&#34;</span> + <span style="color:#658b00">str</span>(i)], parameters[<span style="color:#cd5555">&#34;b&#34;</span> + <span style="color:#658b00">str</span>(i)], <span style="color:#cd5555">&#34;relu&#34;</span>)
</span></span><span style="display:flex;"><span>        caches.append(cache)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># 最后一层为sigmoid激活函数</span>
</span></span><span style="display:flex;"><span>    AL, cache = linear_activation_forward(A, parameters[<span style="color:#cd5555">&#34;W&#34;</span> + <span style="color:#658b00">str</span>(L)], parameters[<span style="color:#cd5555">&#34;b&#34;</span> + <span style="color:#658b00">str</span>(L)], <span style="color:#cd5555">&#34;sigmoid&#34;</span>)
</span></span><span style="display:flex;"><span>    caches.append(cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(AL.shape == (<span style="color:#b452cd">1</span>, X.shape[<span style="color:#b452cd">1</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> AL, caches
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">compute_cost</span>(AL, Y):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    计算代价函数 使用Logistic回归中使用的代价函数
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    AL -- 对应于标签的预测概率向量, shape (1, 样本数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- 正确的样本 (for example: containing 0 if non-cat, 1 if cat), shape (1, 样本数)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cost -- 交叉熵代价
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    m = Y.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(<span style="color:#b452cd">1</span> - AL), (<span style="color:#b452cd">1</span> - Y).T)) / m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cost = np.squeeze(cost) <span style="color:#228b22"># 使得cost的维度是我们想要的（比如将[[17]]变成17）</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(cost.shape == ())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> cost
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">linear_backward</span>(dZ, cache):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    单层实现反向传播的线性部分(l层)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dZ -- 代价函数对于线性输出的梯度 (l层)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- 元组(A_prev, W, b) 来自当前层的前向传播
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A_prev, W, b = cache
</span></span><span style="display:flex;"><span>    m = A_prev.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dW = np.dot(dZ, A_prev.T) / m
</span></span><span style="display:flex;"><span>    db = np.sum(dZ, axis=<span style="color:#b452cd">1</span>, keepdims=<span style="color:#8b008b;font-weight:bold">True</span>) / m 
</span></span><span style="display:flex;"><span>    dA_prev = np.dot(W.T, dZ)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(dA_prev.shape == A_prev.shape)
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(dW.shape == W.shape)
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">assert</span>(db.shape == b.shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> dA_prev, dW, db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">linear_activation_backward</span>(dA, cache, activation):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    实现 线性——&gt;激活 过程的反向传播
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dA -- 当前层l激活后的梯度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- 元组 (linear_cache, activation_cache) 为了有效计算后向传播而存储
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    activation -- 当前层的激活函数, stored as a text string: &#34;sigmoid&#34; or &#34;relu&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度,和A_prev相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    linear_cache, activation_cache = cache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">if</span> activation == <span style="color:#cd5555">&#34;relu&#34;</span>:
</span></span><span style="display:flex;"><span>        dZ = relu_backward(dA, activation)
</span></span><span style="display:flex;"><span>        dA_prev, dW, db = linear_backward(dZ, linear_cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">elif</span> activation == <span style="color:#cd5555">&#34;sigmoid&#34;</span>:
</span></span><span style="display:flex;"><span>        dZ = sigmoid_backward(dA, activation_cache)
</span></span><span style="display:flex;"><span>        dA_prev, dW, db = linear_backward(dZ, linear_cache)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> dA_prev, dW, db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">L_model_backward</span>(AL, Y, caches):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    前L-1层为ReLU激活函数,最后一层为sigmoid函数的后向传播实现
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    AL -- 前向传播输出的概率向量 (L_model_forward())
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- 真实值的向量 (containing 0 if non-cat, 1 if cat)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    caches -- 包含cache的列表:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                every cache of linear_activation_forward() with &#34;relu&#34; (it&#39;s caches[l], for l in range(L-1) i.e l = 0...L-2)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                the cache of linear_activation_forward() with &#34;sigmoid&#34; (it&#39;s caches[L-1])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    grads -- 带有渐变值的字典
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">             grads[&#34;dA&#34; + str(l)] = ... 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">             grads[&#34;dW&#34; + str(l)] = ...
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">             grads[&#34;db&#34; + str(l)] = ... 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    grads = {}
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(caches)
</span></span><span style="display:flex;"><span>    m = AL.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    Y = Y.reshape(AL.shape) <span style="color:#228b22"># 经过这一行转化，Y的维度和AL维度相同</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dAL = -(np.divide(Y, AL) - np.divide(<span style="color:#b452cd">1</span> - Y, <span style="color:#b452cd">1</span> - AL))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    current_cache = caches[L - <span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    grads[<span style="color:#cd5555">&#34;dA&#34;</span> + <span style="color:#658b00">str</span>(L)], grads[<span style="color:#cd5555">&#34;dW&#34;</span> + <span style="color:#658b00">str</span>(L)], grads[<span style="color:#cd5555">&#34;db&#34;</span> + <span style="color:#658b00">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation=<span style="color:#cd5555">&#34;sigmoid&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">reversed</span>(<span style="color:#658b00">range</span>(L-<span style="color:#b452cd">1</span>)):
</span></span><span style="display:flex;"><span>        current_cache = caches[i]
</span></span><span style="display:flex;"><span>        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span style="color:#cd5555">&#34;dA&#34;</span> + <span style="color:#658b00">str</span>(i + <span style="color:#b452cd">2</span>)], current_cache, activation=<span style="color:#cd5555">&#34;relu&#34;</span>)
</span></span><span style="display:flex;"><span>        grads[<span style="color:#cd5555">&#34;dA&#34;</span> + <span style="color:#658b00">str</span>(i + <span style="color:#b452cd">1</span>)] = dA_prev_temp
</span></span><span style="display:flex;"><span>        grads[<span style="color:#cd5555">&#34;dW&#34;</span> + <span style="color:#658b00">str</span>(i + <span style="color:#b452cd">1</span>)] = dW_temp
</span></span><span style="display:flex;"><span>        grads[<span style="color:#cd5555">&#34;db&#34;</span> + <span style="color:#658b00">str</span>(i + <span style="color:#b452cd">1</span>)] = db_temp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> grads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># ------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">update_parameters</span>(parameters, grads, learning_rate):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    使用梯度下降更新参数
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    grads -- python dictionary containing your gradients, output of L_model_backward
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- 更新后的参数字典
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                  parameters[&#34;W&#34; + str(l)] = ... 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                  parameters[&#34;b&#34; + str(l)] = ...
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(parameters) // <span style="color:#b452cd">2</span> <span style="color:#228b22"># 神经网络中的层数</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L + <span style="color:#b452cd">1</span>):
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#34;W&#34;</span> + <span style="color:#658b00">str</span>(i)] -= learning_rate * grads[<span style="color:#cd5555">&#34;dW&#34;</span> + <span style="color:#658b00">str</span>(i)]
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#34;b&#34;</span> + <span style="color:#658b00">str</span>(i)] -= learning_rate * grads[<span style="color:#cd5555">&#34;db&#34;</span> + <span style="color:#658b00">str</span>(i)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="有效运行神经网络">有效运行神经网络</h3>
<p>深度学习网络是一个需要迭代得到结果的模型。它的超参数调整过程：想法-&gt;编码-&gt;实验-&gt;修改想法，需要不断地尝试，才能学习到调参的经验。</p>
<h4 id="训练集和测试集划分">训练集和测试集划分</h4>
<p>我们一般将数据分为三个部分：</p>
<p>1.训练集：为训练模型准备的数据</p>
<p>2.验证集：通过交叉验证集选择最优模型</p>
<p>3.测试集：对模型进行评估</p>
<blockquote>
<p>划分训练测试集最常见的比例是什么？</p>
</blockquote>
<p>如果明确指定验证集，一般训练和测试集的比例为7:3；如果指定验证集，那么一般比例为6:2:2。这一般在数据量在10000条以下都是最好的划分比例。但在大数据时代，如果数据总量比较大，比如是百万条，那么验证集和测试集的比例还需要减少，比如98:1:1，也是合理的。</p>
<p><em>需要特别注意的是：要保证验证集和测试集来自同一分布，这样会使得你的机器学习算法变得更快。</em></p>
<h4 id="偏差和方差">偏差和方差</h4>
<p>根据数据集的分布状况，可以分为以下三种：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img10.jpg" alt=""></p>
<p>如上图，最左边使用Logistic回归，没有大部分正确分类，属于“欠拟合”的状况；最右边使用比较复杂的神经网络，完整地分出了两个类别，属于”过拟合“的情况（因为部分输入数据是不合理的）；中间只有和别数据分类错误，这叫“适度拟合”，是我们比较追求的一种状态。</p>
<p>上图这种只有一个或者两个特征的二维数据中，可以绘制数据，将偏差和方差可视化。但在多维空间数据中，绘制数据和可视化分割边界无法实现。</p>
<blockquote>
<p>我们在多维空间通常通过两个个指标,来研究偏差和方差：训练集误差和测试集误差。为了便于研究，假设人眼分辨的错误率为0，这也被称为基本误差或者最优误差；假设训练集和验证集来自同一分布。如果训练集误差为0.01，测试集误差为0.11，这种情况很有可能是我们过度拟合了训练集，称之为高方差，对应于上图最右边的情况；如果训练集误差为0.15，测试集误差为0.16，这种情况很有可能是我们训练数据的时候欠拟合，称之为高偏差，对应于上图最左边的情况；如果训练集误差为0.15，测试集误差为0.3，偏差和方差都比较高，这种情况是因为你的算法模型并不适合这个任务，需要改变模型；如果训练集误差为0.005，测试集误差为0.01，偏差和方差都很低，是分类效果比较好的情况，对应上图中间的情况。</p>
</blockquote>
<h4 id="模型评估调优的过程">模型评估调优的过程</h4>
<ul>
<li>首先，将进行训练之后的模型用于评估训练集的性能，如果误差高，代表欠拟合。那么你要采取的方法可能是增加训练时间或者增大网络结构或者是选择一个新网络，先将偏差降下来，拟合训练数据。（这在基本误差不同的情况下会有所不同）</li>
<li>如果模型的训练集性能很高后，可以将其用于评估测试集，如果误差高，代表过拟合。那么你要采取的方法最好是采用更多的训练数据，如果无法获得更多数据，可以通过正则化来减少过拟合，降低方差，有时候也会通过替换网络结构来实现。</li>
</ul>
<h4 id="正则化">正则化</h4>
<p>如果你的模型在评估是，由比较高的方差，然而你又不能拿到更多的数据集，那么我们最先想到的办法可能是正则化。</p>
<p>正则化有助于避免数据过度拟合，减少网络误差。</p>
<ul>
<li>Logistic正则化</li>
</ul>
<p>假设我们的目标是找到$w,b$来使得$J(w,b)$达到最小值，且使用逻辑回归的代价函数，那么
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})
$$
我们在该函数中加入正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert_2}^2\
{\Vert w \Vert_2}^2=\sum^{n_x}<em>{j=1}w</em>{j}^2=w^Tw \quad 这被称为L2正则化
$$
同理也会有L正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}{\Vert w \Vert_1}\
{\Vert w \Vert_1}=\sum^{n_x}_{j=1}\lvert w_j \rvert \quad 这被称为L1正则化
$$
通常我们都会使用<code>L2正则化</code>来实现降低方差的效果。</p>
<blockquote>
<p>那么$\lambda$的值我们需要如何确定呢？</p>
</blockquote>
<p>我们通常使用验证集或者交叉验证来配置$\lambda$参数，不过首先要考虑训练集之间的权衡，吧参数$w,b$正常值设为较小的值，避免过拟合，不断调整超参数$\lambda$的值来减小方差。</p>
<p><em>需要特别说明的是在编写代码的时候，python语言中lambda是一个保留字段，所以编程时我们通常使用lambd来代替lambda。</em></p>
<ul>
<li>神经网络正则化</li>
</ul>
<p>$$
J(W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]})=\frac1m\sum^n_{i=1}L(\hat{y}^{(i)},y^{(i)})+ \frac{\lambda}{2m}\sum^L_{l=1}{\Vert W^{[l]} \Vert}^2\
{\Vert W^{[l]} \Vert}^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W_{i,j}^{[l]})^2\quad 该矩阵范数被称为是弗罗贝尼乌斯范数\
W的维度是(n^{[l]},n^{[l-1]})
$$</p>
<p>如果$J(W,b)$发生了改变，那么反向传播的$dW^{[l]}$也要发生变化：</p>
<p>在<code>backprop</code>计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值，这样做的结果是的$W^{[l]}$会比没有正则化之前更小。因此<code>L2范数</code>也被称为<code>权重衰减</code>。</p>
<h4 id="正则化如何预防过拟合">正则化如何预防过拟合</h4>
<p>如果正则化的参数$\lambda$如果设置的过大，$W^{[l]}$会变得更小，就会导致每层上的部分$w$的权重 会接近0，这相当于将部分隐藏单元给去除了，复杂的神经网络会退化称为一 个很深但是又很简单的网络，导致从过拟合直接变成欠拟合的状态。</p>
<p>一个合适的$\lambda$值能够使模型的性能从过拟合到适度拟合。</p>
<blockquote>
<p>$\lambda$越大，得到迭代的$W$就越小，这相当于让部分隐藏单元的影响变小，降低模型的拟合程度，方差减小。</p>
<p>总体来说，正则化的效果其实就是将复杂的网络线性化，如果$\lambda$的取值设置的比较好，能达到适度拟合的效果。</p>
</blockquote>
<h4 id="dropout正则化">dropout正则化</h4>
<p><code>dropout正则化</code>是指如果一个网络存在过拟合的情况下，可以将所有节点（隐藏单元）设置一个删除概率为0.5，那么保留该隐藏单元的概率<code>keep-prob</code>也为0.5，这个值是可以改变的，然后随机消除一些节点并删除该节点进出的连线，得到一个节点更少，规模更小的网络。如下图所示：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img11.jpg" alt=""></p>
<p>实现Dropout的方法很多，最常用的是<code>inverted dropout</code>。以一个深层神经网络的某一个隐藏层为例来解释怎么进行Dropout正则化。首先假设对于第$l$层，其激活函数为$a^{[l]}$，我们设置的保留概率<code>keep_prob</code>等于0.8，这意味着该隐藏层的所有神经元以0.8的概率得到保留。</p>
<p>可以将<code>inverted dropout</code>方法归纳为四步：</p>
<blockquote>
<p>1.根据<code>keep_prob</code>生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，<code>Dl = np.rndom.randn(Al.shape[0], Al.shape[1])</code></p>
<p>2.将$d^{[l]}$转化为0-1矩阵， <code>Dl = Dl &lt; keep_prob</code></p>
<p>3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， <code>Al = np.muiltiply(Al, Dl)</code></p>
<p>4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以<code>keep_prob</code>,<code>Al = Al / keep_prob</code></p>
</blockquote>
<p>需要注意的是，在反向传播的时候，也需要像上面一样进行dropout操作，和前向传播关闭相同的神经元。即对于某一层的$da^{[l]}$，应该进行以下计算：</p>
<blockquote>
<p>1.<code>dAl = dAl * Dl</code></p>
<p>2.<code>dAl = dAl / keep_prob</code></p>
</blockquote>
<p>另一点是，Dropout正则化只在训练阶段实施，在测试阶段只需要利用训练好的参数进行正向预测，而不需要进行神经元的随机失活。</p>
<p>三层网络的前向传播<code>dropout</code>示例代码：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>keep_prob = <span style="color:#b452cd">0.8</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">foward</span>(X):
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># 3层neural network的前向传播</span>
</span></span><span style="display:flex;"><span>    A1 = np.maximum(<span style="color:#b452cd">0</span>, np.dot(W1, X) + b1)  <span style="color:#228b22"># 计算第一层网络的输出</span>
</span></span><span style="display:flex;"><span>    D1 = (np.random.rand(*A1.shape) &lt; keep_prob)  <span style="color:#228b22"># 以keep_prob为标准，判断该层结点哪些可以保留</span>
</span></span><span style="display:flex;"><span>    Z1 = np.multiply(A1, D1)     <span style="color:#228b22">#dropout</span>
</span></span><span style="display:flex;"><span>    Z1 /= keep_prob   <span style="color:#228b22"># 为了期望的一致，除以keep_prob</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A2 = np.maximum(<span style="color:#b452cd">0</span>, np.dot(W2, Z1) + b2)
</span></span><span style="display:flex;"><span>    D2 = (np.random.rand(*A2.shape) &lt; keep_prob)
</span></span><span style="display:flex;"><span>    Z2 = np.multiply(A2, D2) 
</span></span><span style="display:flex;"><span>    Z2 /= keep_prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    out = np.dot(W3, Z2) + b3
</span></span></code></pre></td></tr></table>
</div>
</div><p>那么我们如何在不同的层设置不同的<code>keep_prob</code>，我们可以在不太会发生过拟合现象的地方设置<code>keep_prob</code>为1，在容易发生过拟合的地方将<code>keep_prob</code>设置的低一点。</p>
<p>使用<code>drop_out</code>正则化的缺点是不能定义明确的代价函数，那么我们使用的方法一般是先将<code>drop_out</code>关闭，确保该网络的代价函数是递减的，再打开<code>drop_out</code>进行学习。</p>
<h4 id="数据扩容">数据扩容</h4>
<p>在我们数据不够的情况下，可以在已有的数据的基础上将数据进行稍作改变来增加数据集。举个例子，一张包含猫咪的图片，我们可以将其扩容为几张：</p>
<blockquote>
<p>1.将猫的图片进行水平翻转，得到一张新的图片</p>
<p>2.将猫的图片进行局部放大，并进行裁剪</p>
</blockquote>
<p><em>但是最重要的一点就是经过调整后的图片，必须确保它还是一只猫。</em></p>
<p>如果输入的是一个数字，我们可以将这些数字进行轻微扭曲，旋转，将其加入数据集。</p>
<ul>
<li>Early stopping：一种防止过拟合的方法。</li>
</ul>
<p>使用该方法可以绘制训练集的误差和验证集的误差，可以用来预防过拟合。</p>
<blockquote>
<p>L2正则化和early stopping的对比：</p>
<p>1.L2正则化训练神经网络的时间可能很长。这导致超参数搜索空间更容易分解</p>
<p>2.L2正则化可能许多正则化参数$\lambda$的值，计算代价太高。而early stopping只运行一次梯度下降，你就可以找到W的最大值，中间值，最小值，无需多次迭代。</p>
</blockquote>
<h4 id="归一化输入">归一化输入</h4>
<p>如图，给定输入数据的散点图：</p>
 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img12.jpg" style="zoom:67%;" />
<p>归一化需要两个步骤：</p>
<ul>
<li>1.零均值化</li>
</ul>
<p>$$
u=\frac1m\sum_{i=1}^mX^{(i)}\
x:=X-u
$$</p>
<p>意思是移动训练集，直至它完成零均值化，结果如下：</p>
 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img13.jpg" style="zoom:67%;" />
<ul>
<li>2.归一化方差</li>
</ul>
<p>如上图可以看到，特征$x_1$的方差比特征$x_2$大得多。使用如下公式迭代：
$$
\sigma^2=\frac1m\sum_{i=1}^mx^{(i)}**2\
x/=\sigma^2
$$
结果如下：</p>
 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img14.jpg" style="zoom:67%;" />
<p><em>需要注意的是：我们在训练集和验证集或者测试集上需要设置相同的$u$和$\sigma^2$</em></p>
<p><em>那么为什么要使用归一化呢？</em></p>
<p>归一化会使得不同的特征的起始$W$范围较为接近，使得它们的代价函数有如下图的转变：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img15.jpg" alt=""></p>
<p>这样可以减少迭代，使得梯度下降法使用更大的步长。</p>
<h4 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h4>
<p>在深度神经网络中梯度是不稳定的，可能会变得非常小，也可能会变得非常大，这就是梯度消失和梯度爆炸。</p>
<p>假设$W^{[1]}$比单位矩阵稍微大一点，那么在深度神经网络中，激活函数将会是指数级增长 ；相反，假设$W^{[1]}$比单位矩阵稍微小一些，激活函数会呈现指数级下降的趋势。</p>
<p>那么如何解决如上的问题呢？</p>
<p>一般使用权重初始化来解决：$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{1}{n^{[l-1]}})$，其中$shape$代表该层$W$矩阵的维度，$n^{[l-1]}$代表上一层。如果激活函数使用了<code>ReLU</code>激活函数，$\frac{1}{n^{[l-1]}}$可以变成$\frac{2}{n^{[l-1]}}$效果更好；如果使用<code>tanh</code>激活函数，使用$\frac{1}{n^{[l-1]}}$；有时候也会看到使用$np.sqrt(\frac{1}{n^{[l-1]}+n^{[l]}})$。</p>
<h4 id="梯度检验">梯度检验</h4>
<p>关于梯度的数值逼近，一般使用<code>双边误差</code>公式，即
$$
f^{&rsquo;}(\theta)=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}
$$
我们通常会通过<code>梯度检验</code>来验证backprop过程的正确实施：</p>
<p>首先，我们需要将$W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]}$重新组合成为一个大的向量$\theta$；同样的，在反向传播的过程也需要将$dW^{[1]},db^{[1]},\dots,dW^{[L]},db^{[L]}$重新组合成为一个大的向量$d\theta$。
$$
J(\theta)=J(\theta_1,\theta_2,\dots)
$$
然后对于每个$i$：
$$
{d\theta_{approx}}^{[i]}=\lim_{\epsilon\rightarrow0}\frac{J(\theta_1,\theta_2,\dots,\theta_{i+\epsilon},\dots)-J(\theta_1,\theta_2,\dots,\theta_{i-\epsilon},\dots)}{2\epsilon}\approx d\theta^{[i]}=\frac{\partial J}{\partial \theta_i}
$$
做完计算之后，需要做的就是验证是否：
$$
d\theta_{approx}\approx d\theta
$$
如果上述两个量差值的二范数在$10^{-7}$量级，这代表导数逼近很有可能是正确的；如果在$10^{-5}$，就需要担心是否是正确的；如果在$10^{-3}$，那说明你的梯度下降传播程序出现了bug。</p>
<blockquote>
<p>注意事项：</p>
<p>1.梯度检验是非常耗费时间的，在训练的时候不使用梯度检验，只有在debug的时候使用。</p>
<p>2.如果算法的梯度检验失败，要检查每一项，找出bug。</p>
<p>3.如果代价函数包含了正则项$\frac{\lambda}{2m}{\Vert w \Vert_2}^2$，那么$d\theta_{approx}$也需要多加一个正则项。</p>
<p>4.梯度检验和dropout正则化不能同时使用。所以在梯度检验的时候，需要先关闭dropout正则化。</p>
</blockquote>
<h4 id="作业四">作业四</h4>
<ul>
<li>N维梯度检验：</li>
</ul>
<p>其工作原理如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img16.jpg" alt=""></p>
<p>前向传播和后向传播示例代码：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">numpy</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">np</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">gc_utils</span> <span style="color:#8b008b;font-weight:bold">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22">#-------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">forward_propagation_n</span>(X, Y, parameters):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements the forward propagation (and computes the cost) presented in Figure 3.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- training set for m examples
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- labels for m examples 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, &#34;W2&#34;, &#34;b2&#34;, &#34;W3&#34;, &#34;b3&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W1 -- weight matrix of shape (5, 4)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b1 -- bias vector of shape (5, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W2 -- weight matrix of shape (3, 5)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b2 -- bias vector of shape (3, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W3 -- weight matrix of shape (1, 3)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b3 -- bias vector of shape (1, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cost -- the cost function (logistic cost for one example)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># retrieve parameters</span>
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    W1 = parameters[<span style="color:#cd5555">&#34;W1&#34;</span>]
</span></span><span style="display:flex;"><span>    b1 = parameters[<span style="color:#cd5555">&#34;b1&#34;</span>]
</span></span><span style="display:flex;"><span>    W2 = parameters[<span style="color:#cd5555">&#34;W2&#34;</span>]
</span></span><span style="display:flex;"><span>    b2 = parameters[<span style="color:#cd5555">&#34;b2&#34;</span>]
</span></span><span style="display:flex;"><span>    W3 = parameters[<span style="color:#cd5555">&#34;W3&#34;</span>]
</span></span><span style="display:flex;"><span>    b3 = parameters[<span style="color:#cd5555">&#34;b3&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
</span></span><span style="display:flex;"><span>    Z1 = np.dot(W1, X) + b1
</span></span><span style="display:flex;"><span>    A1 = relu(Z1)
</span></span><span style="display:flex;"><span>    Z2 = np.dot(W2, A1) + b2
</span></span><span style="display:flex;"><span>    A2 = relu(Z2)
</span></span><span style="display:flex;"><span>    Z3 = np.dot(W3, A2) + b3
</span></span><span style="display:flex;"><span>    A3 = sigmoid(Z3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Cost</span>
</span></span><span style="display:flex;"><span>    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span style="color:#b452cd">1</span> - A3), <span style="color:#b452cd">1</span> - Y)
</span></span><span style="display:flex;"><span>    cost = <span style="color:#b452cd">1.</span>/m * np.sum(logprobs)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> cost, cache
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#-------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">backward_propagation_n</span>(X, Y, cache):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implement the backward propagation presented in figure 2.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input datapoint, of shape (input size, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- true &#34;label&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- cache output from forward_propagation_n()
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dZ3 = A3 - Y
</span></span><span style="display:flex;"><span>    dW3 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ3, A2.T)
</span></span><span style="display:flex;"><span>    db3 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ3, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dA2 = np.dot(W3.T, dZ3)
</span></span><span style="display:flex;"><span>    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    dW2 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ2, A1.T) * <span style="color:#b452cd">2</span>
</span></span><span style="display:flex;"><span>    db2 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ2, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dA1 = np.dot(W2.T, dZ2)
</span></span><span style="display:flex;"><span>    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    dW1 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ1, X.T)
</span></span><span style="display:flex;"><span>    db1 = <span style="color:#b452cd">4.</span>/m * np.sum(dZ1, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    gradients = {<span style="color:#cd5555">&#34;dZ3&#34;</span>: dZ3, <span style="color:#cd5555">&#34;dW3&#34;</span>: dW3, <span style="color:#cd5555">&#34;db3&#34;</span>: db3,
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dA2&#34;</span>: dA2, <span style="color:#cd5555">&#34;dZ2&#34;</span>: dZ2, <span style="color:#cd5555">&#34;dW2&#34;</span>: dW2, <span style="color:#cd5555">&#34;db2&#34;</span>: db2,
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dA1&#34;</span>: dA1, <span style="color:#cd5555">&#34;dZ1&#34;</span>: dZ1, <span style="color:#cd5555">&#34;dW1&#34;</span>: dW1, <span style="color:#cd5555">&#34;db1&#34;</span>: db1}
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> gradients
</span></span></code></pre></td></tr></table>
</div>
</div><p>接下去就需要进行梯度检验了！其中涉及的字典转vector的原理图如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img17.jpg" alt=""></p>
<p>梯度检验的代码如下：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">gradient_check_n</span>(parameters, gradients, X, Y, epsilon = <span style="color:#b452cd">1e-7</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, &#34;W2&#34;, &#34;b2&#34;, &#34;W3&#34;, &#34;b3&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    x -- input datapoint, of shape (input size, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    y -- true &#34;label&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    difference -- difference (2) between the approximated gradient and the backward propagation gradient
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Set-up variables</span>
</span></span><span style="display:flex;"><span>    parameters_values, _ = dictionary_to_vector(parameters)
</span></span><span style="display:flex;"><span>    grad = gradients_to_vector(gradients)
</span></span><span style="display:flex;"><span>    num_parameters = parameters_values.shape[<span style="color:#b452cd">0</span>]
</span></span><span style="display:flex;"><span>    J_plus = np.zeros((num_parameters, <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>    J_minus = np.zeros((num_parameters, <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>    gradapprox = np.zeros((num_parameters, <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Compute gradapprox</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(num_parameters):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Compute J_plus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_plus[i]&#34;.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># &#34;_&#34; is used because the function you have to outputs two parameters but we only care about the first one</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (approx. 3 lines)</span>
</span></span><span style="display:flex;"><span>        epsilon = <span style="color:#b452cd">0.01</span>
</span></span><span style="display:flex;"><span>        thetaplus = np.copy(parameters_values)                                      <span style="color:#228b22"># Step 1</span>
</span></span><span style="display:flex;"><span>        thetaplus[i][<span style="color:#b452cd">0</span>] = thetaplus[i][<span style="color:#b452cd">0</span>] + epsilon                                <span style="color:#228b22"># Step 2</span>
</span></span><span style="display:flex;"><span>        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   <span style="color:#228b22"># Step 3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Compute J_minus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_minus[i]&#34;.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (approx. 3 lines)</span>
</span></span><span style="display:flex;"><span>        thetaminus = np.copy(parameters_values)                                     <span style="color:#228b22"># Step 1</span>
</span></span><span style="display:flex;"><span>        thetaminus[i][<span style="color:#b452cd">0</span>] = thetaminus[i][<span style="color:#b452cd">0</span>] - epsilon                               <span style="color:#228b22"># Step 2        </span>
</span></span><span style="display:flex;"><span>        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  <span style="color:#228b22"># Step 3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Compute gradapprox[i]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>        gradapprox[i] = (J_plus[i] - J_minus[i]) / (<span style="color:#b452cd">2</span>*epsilon)
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Compare gradapprox to backward propagation gradients by computing difference.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># np.linalg.norm()的作用是求二范数</span>
</span></span><span style="display:flex;"><span>    numerator = np.linalg.norm(grad)                                           <span style="color:#228b22"># Step 1&#39;</span>
</span></span><span style="display:flex;"><span>    denominator = np.linalg.norm(gradapprox)                                         <span style="color:#228b22"># Step 2&#39;</span>
</span></span><span style="display:flex;"><span>    difference = np.linalg.norm(grad - gradapprox) / (numerator + denominator)                                          <span style="color:#228b22"># Step 3&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">if</span> difference &gt; <span style="color:#b452cd">1e-7</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;</span><span style="color:#cd5555">\033</span><span style="color:#cd5555">[93m&#34;</span> + <span style="color:#cd5555">&#34;There is a mistake in the backward propagation! difference = &#34;</span> + <span style="color:#658b00">str</span>(difference) + <span style="color:#cd5555">&#34;</span><span style="color:#cd5555">\033</span><span style="color:#cd5555">[0m&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;</span><span style="color:#cd5555">\033</span><span style="color:#cd5555">[92m&#34;</span> + <span style="color:#cd5555">&#34;Your backward propagation works perfectly fine! difference = &#34;</span> + <span style="color:#658b00">str</span>(difference) + <span style="color:#cd5555">&#34;</span><span style="color:#cd5555">\033</span><span style="color:#cd5555">[0m&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> difference
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>权重初始化解决梯度消失和梯度爆炸问题</li>
</ul>
<blockquote>
<p>下列代码会通过三种初始化的方式进行对比：</p>
<p>1.将W和b都设置为0向量</p>
<p>2.随机设置W和b</p>
<p>3.在随机设置的基础上进行权重初始化</p>
</blockquote>
<p>代码如下：</p>
<p>加载初始数据：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">numpy</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">np</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">plt</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">sklearn</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">sklearn.datasets</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">init_utils</span> <span style="color:#8b008b;font-weight:bold">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">init_utils</span> <span style="color:#8b008b;font-weight:bold">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>%matplotlib inline
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;figure.figsize&#39;</span>] = (<span style="color:#b452cd">7.0</span>, <span style="color:#b452cd">4.0</span>) <span style="color:#228b22"># set default size of plots</span>
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;image.interpolation&#39;</span>] = <span style="color:#cd5555">&#39;nearest&#39;</span>
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;image.cmap&#39;</span>] = <span style="color:#cd5555">&#39;gray&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># load image dataset: blue/red dots in circles</span>
</span></span><span style="display:flex;"><span>train_X, train_Y, test_X, test_Y = load_dataset()
</span></span></code></pre></td></tr></table>
</div>
</div><p>结果如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img18.jpg" alt=""></p>
<p>初始化模型：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">model</span>(X, Y, learning_rate = <span style="color:#b452cd">0.01</span>, num_iterations = <span style="color:#b452cd">15000</span>, print_cost = <span style="color:#8b008b;font-weight:bold">True</span>, initialization = <span style="color:#cd5555">&#34;he&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input data, of shape (2, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- true &#34;label&#34; vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    learning_rate -- learning rate for gradient descent 
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    num_iterations -- number of iterations to run gradient descent
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    print_cost -- if True, print the cost every 1000 iterations
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    initialization -- flag to choose which initialization to use (&#34;zeros&#34;,&#34;random&#34; or &#34;he&#34;)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- parameters learnt by the model
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    grads = {}
</span></span><span style="display:flex;"><span>    costs = [] <span style="color:#228b22"># to keep track of the loss</span>
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>] <span style="color:#228b22"># number of examples</span>
</span></span><span style="display:flex;"><span>    layers_dims = [X.shape[<span style="color:#b452cd">0</span>], <span style="color:#b452cd">10</span>, <span style="color:#b452cd">5</span>, <span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Initialize parameters dictionary.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">if</span> initialization == <span style="color:#cd5555">&#34;zeros&#34;</span>:
</span></span><span style="display:flex;"><span>        parameters = initialize_parameters_zeros(layers_dims)
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">elif</span> initialization == <span style="color:#cd5555">&#34;random&#34;</span>:
</span></span><span style="display:flex;"><span>        parameters = initialize_parameters_random(layers_dims)
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">elif</span> initialization == <span style="color:#cd5555">&#34;he&#34;</span>:
</span></span><span style="display:flex;"><span>        parameters = initialize_parameters_he(layers_dims)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Loop (gradient descent)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">0</span>, num_iterations):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
</span></span><span style="display:flex;"><span>        a3, cache = forward_propagation(X, parameters)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Loss</span>
</span></span><span style="display:flex;"><span>        cost = compute_loss(a3, Y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Backward propagation.</span>
</span></span><span style="display:flex;"><span>        grads = backward_propagation(X, Y, cache)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Update parameters.</span>
</span></span><span style="display:flex;"><span>        parameters = update_parameters(parameters, grads, learning_rate)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Print the loss every 1000 iterations</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> print_cost <span style="color:#8b008b">and</span> i % <span style="color:#b452cd">1000</span> == <span style="color:#b452cd">0</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#658b00">print</span>(<span style="color:#cd5555">&#34;Cost after iteration </span><span style="color:#cd5555">{}</span><span style="color:#cd5555">: </span><span style="color:#cd5555">{}</span><span style="color:#cd5555">&#34;</span>.format(i, cost))
</span></span><span style="display:flex;"><span>            costs.append(cost)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># plot the loss</span>
</span></span><span style="display:flex;"><span>    plt.plot(costs)
</span></span><span style="display:flex;"><span>    plt.ylabel(<span style="color:#cd5555">&#39;cost&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.xlabel(<span style="color:#cd5555">&#39;iterations (per hundreds)&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.title(<span style="color:#cd5555">&#34;Learning rate =&#34;</span> + <span style="color:#658b00">str</span>(learning_rate))
</span></span><span style="display:flex;"><span>    plt.show()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span></code></pre></td></tr></table>
</div>
</div><p>W为0向量初始化：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">initialize_parameters_zeros</span>(layers_dims):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    layer_dims -- python array (list) containing the size of each layer.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, ..., &#34;WL&#34;, &#34;bL&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b1 -- bias vector of shape (layers_dims[1], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    ...
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    bL -- bias vector of shape (layers_dims[L], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    parameters = {}
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(layers_dims)            <span style="color:#228b22"># number of layers in the network</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L):
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (≈ 2 lines of code)</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;W&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.zeros((layers_dims[i], layers_dims[i - <span style="color:#b452cd">1</span>]))
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;b&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.zeros((layers_dims[i], <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span>parameters = initialize_parameters_zeros([<span style="color:#b452cd">3</span>,<span style="color:#b452cd">2</span>,<span style="color:#b452cd">1</span>])
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y, initialization = <span style="color:#cd5555">&#34;zeros&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the train set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">plot_decision_boundary</span>(model, X, y):
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">#import pdb;pdb.set_trace()</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Set min and max values and give it some padding</span>
</span></span><span style="display:flex;"><span>    x_min, x_max = X[<span style="color:#b452cd">0</span>, :].min() - <span style="color:#b452cd">1</span>, X[<span style="color:#b452cd">0</span>, :].max() + <span style="color:#b452cd">1</span>
</span></span><span style="display:flex;"><span>    y_min, y_max = X[<span style="color:#b452cd">1</span>, :].min() - <span style="color:#b452cd">1</span>, X[<span style="color:#b452cd">1</span>, :].max() + <span style="color:#b452cd">1</span>
</span></span><span style="display:flex;"><span>    h = <span style="color:#b452cd">0.01</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Generate a grid of points with distance h between them</span>
</span></span><span style="display:flex;"><span>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Predict the function value for the whole grid</span>
</span></span><span style="display:flex;"><span>    Z = model(np.c_[xx.ravel(), yy.ravel()])
</span></span><span style="display:flex;"><span>    Z = Z.reshape(xx.shape)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Plot the contour and training examples</span>
</span></span><span style="display:flex;"><span>    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
</span></span><span style="display:flex;"><span>    plt.ylabel(<span style="color:#cd5555">&#39;x2&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.xlabel(<span style="color:#cd5555">&#39;x1&#39;</span>)
</span></span><span style="display:flex;"><span>    y = y.reshape(X[<span style="color:#b452cd">0</span>,:].shape)<span style="color:#228b22">#must reshape,otherwise confliction with dimensions</span>
</span></span><span style="display:flex;"><span>    plt.scatter(X[<span style="color:#b452cd">0</span>, :], X[<span style="color:#b452cd">1</span>, :], c=y, cmap=plt.cm.Spectral)
</span></span><span style="display:flex;"><span>    plt.show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model with Zeros initialization&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>分类结果如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img19.jpg" alt=""></p>
<p>W为随机向量初始化：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">initialize_parameters_random</span>(layers_dims):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    layer_dims -- python array (list) containing the size of each layer.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, ..., &#34;WL&#34;, &#34;bL&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b1 -- bias vector of shape (layers_dims[1], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    ...
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    bL -- bias vector of shape (layers_dims[L], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    np.random.seed(<span style="color:#b452cd">3</span>)               <span style="color:#228b22"># This seed makes sure your &#34;random&#34; numbers will be the as ours</span>
</span></span><span style="display:flex;"><span>    parameters = {}
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(layers_dims)            <span style="color:#228b22"># integer representing the number of layers</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L):
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (≈ 2 lines of code)</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;W&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.random.randn(layers_dims[i], layers_dims[i - <span style="color:#b452cd">1</span>]) * <span style="color:#b452cd">10</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;b&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.zeros((layers_dims[i], <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span>parameters = initialize_parameters_random([<span style="color:#b452cd">3</span>, <span style="color:#b452cd">2</span>, <span style="color:#b452cd">1</span>])
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y, initialization = <span style="color:#cd5555">&#34;random&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the train set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model with large random initialization&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>分类结果如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img20.jpg" alt=""></p>
<p>W为权重初始化后的结果，因为这里使用的ReLU激活函数，W在初始化后需要变为$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{2}{n^{[l-1]}})$：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">initialize_parameters_he</span>(layers_dims):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    layer_dims -- python array (list) containing the size of each layer.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, ..., &#34;WL&#34;, &#34;bL&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b1 -- bias vector of shape (layers_dims[1], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    ...
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    bL -- bias vector of shape (layers_dims[L], 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    np.random.seed(<span style="color:#b452cd">3</span>)
</span></span><span style="display:flex;"><span>    parameters = {}
</span></span><span style="display:flex;"><span>    L = <span style="color:#658b00">len</span>(layers_dims) - <span style="color:#b452cd">1</span> <span style="color:#228b22"># integer representing the number of layers</span>
</span></span><span style="display:flex;"><span>     
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">1</span>, L + <span style="color:#b452cd">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### START CODE HERE ### (≈ 2 lines of code)</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;W&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.random.randn(layers_dims[i],layers_dims[i - <span style="color:#b452cd">1</span>]) * np.sqrt(<span style="color:#b452cd">2.0</span> / layers_dims[i - <span style="color:#b452cd">1</span>])
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#cd5555">&#39;b&#39;</span> + <span style="color:#658b00">str</span>(i)] = np.zeros((layers_dims[i], <span style="color:#b452cd">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span>parameters = initialize_parameters_he([<span style="color:#b452cd">2</span>, <span style="color:#b452cd">4</span>, <span style="color:#b452cd">1</span>])
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y, initialization = <span style="color:#cd5555">&#34;he&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the train set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#228b22">#---------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model with He initialization&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">1.5</span>,<span style="color:#b452cd">1.5</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>分类结果如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img21.jpg" alt=""></p>
<p><strong>可以看到第三种的分类效果最好！</strong></p>
<ul>
<li>L2正则化和dropout正则化的效果问题</li>
</ul>
<p>加载数据：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">numpy</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">np</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">matplotlib.pyplot</span> <span style="color:#8b008b;font-weight:bold">as</span> <span style="color:#008b45;text-decoration:underline">plt</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">reg_utils</span> <span style="color:#8b008b;font-weight:bold">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">reg_utils</span> <span style="color:#8b008b;font-weight:bold">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">sklearn</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">sklearn.datasets</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">import</span> <span style="color:#008b45;text-decoration:underline">scipy.io</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">from</span> <span style="color:#008b45;text-decoration:underline">testCases</span> <span style="color:#8b008b;font-weight:bold">import</span> *
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>%matplotlib inline
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;figure.figsize&#39;</span>] = (<span style="color:#b452cd">7.0</span>, <span style="color:#b452cd">4.0</span>) <span style="color:#228b22"># set default size of plots</span>
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;image.interpolation&#39;</span>] = <span style="color:#cd5555">&#39;nearest&#39;</span>
</span></span><span style="display:flex;"><span>plt.rcParams[<span style="color:#cd5555">&#39;image.cmap&#39;</span>] = <span style="color:#cd5555">&#39;gray&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_X, train_Y, test_X, test_Y = load_2D_dataset()
</span></span></code></pre></td></tr></table>
</div>
</div><p>数据的分布如下图：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img27.jpg" alt=""></p>
<p>使用无正则化模型，进行梯度下降：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#228b22"># 这里其实已经包含了L2正则化和dropout正则化的情况，只默认情况下的参数不使用正则化</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">model</span>(X, Y, learning_rate = <span style="color:#b452cd">0.3</span>, num_iterations = <span style="color:#b452cd">30000</span>, print_cost = <span style="color:#8b008b;font-weight:bold">True</span>, lambd = <span style="color:#b452cd">0</span>, keep_prob = <span style="color:#b452cd">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input data, of shape (input size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- true &#34;label&#34; vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    learning_rate -- learning rate of the optimization
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    num_iterations -- number of iterations of the optimization loop
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    print_cost -- If True, print the cost every 10000 iterations
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    lambd -- regularization hyperparameter, scalar
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    keep_prob - probability of keeping a neuron active during drop-out, scalar.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- parameters learned by the model. They can then be used to predict.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    grads = {}
</span></span><span style="display:flex;"><span>    costs = []                            <span style="color:#228b22"># to keep track of the cost</span>
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>]                        <span style="color:#228b22"># number of examples</span>
</span></span><span style="display:flex;"><span>    layers_dims = [X.shape[<span style="color:#b452cd">0</span>], <span style="color:#b452cd">20</span>, <span style="color:#b452cd">3</span>, <span style="color:#b452cd">1</span>]  <span style="color:#228b22"># 网络结构为三层网络结构，分别有20，3，1个节点 </span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Initialize parameters dictionary.</span>
</span></span><span style="display:flex;"><span>    parameters = initialize_parameters(layers_dims)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># Loop (gradient descent)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">for</span> i <span style="color:#8b008b">in</span> <span style="color:#658b00">range</span>(<span style="color:#b452cd">0</span>, num_iterations):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> keep_prob == <span style="color:#b452cd">1</span>:
</span></span><span style="display:flex;"><span>            a3, cache = forward_propagation(X, parameters)
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">elif</span> keep_prob &lt; <span style="color:#b452cd">1</span>:
</span></span><span style="display:flex;"><span>            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Cost function</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> lambd == <span style="color:#b452cd">0</span>:
</span></span><span style="display:flex;"><span>            cost = compute_cost(a3, Y)
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Backward propagation.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">assert</span>(lambd==<span style="color:#b452cd">0</span> <span style="color:#8b008b">or</span> keep_prob==<span style="color:#b452cd">1</span>)    <span style="color:#228b22"># it is possible to use both L2 regularization and dropout, </span>
</span></span><span style="display:flex;"><span>                                            <span style="color:#228b22"># but this assignment will only explore one at a time</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> lambd == <span style="color:#b452cd">0</span> <span style="color:#8b008b">and</span> keep_prob == <span style="color:#b452cd">1</span>:
</span></span><span style="display:flex;"><span>            grads = backward_propagation(X, Y, cache)
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">elif</span> lambd != <span style="color:#b452cd">0</span>:
</span></span><span style="display:flex;"><span>            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">elif</span> keep_prob &lt; <span style="color:#b452cd">1</span>:
</span></span><span style="display:flex;"><span>            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Update parameters.</span>
</span></span><span style="display:flex;"><span>        parameters = update_parameters(parameters, grads, learning_rate)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#228b22"># Print the loss every 10000 iterations</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> print_cost <span style="color:#8b008b">and</span> i % <span style="color:#b452cd">10000</span> == <span style="color:#b452cd">0</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#658b00">print</span>(<span style="color:#cd5555">&#34;Cost after iteration </span><span style="color:#cd5555">{}</span><span style="color:#cd5555">: </span><span style="color:#cd5555">{}</span><span style="color:#cd5555">&#34;</span>.format(i, cost))
</span></span><span style="display:flex;"><span>        <span style="color:#8b008b;font-weight:bold">if</span> print_cost <span style="color:#8b008b">and</span> i % <span style="color:#b452cd">1000</span> == <span style="color:#b452cd">0</span>:
</span></span><span style="display:flex;"><span>            costs.append(cost)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># plot the cost</span>
</span></span><span style="display:flex;"><span>    plt.plot(costs)
</span></span><span style="display:flex;"><span>    plt.ylabel(<span style="color:#cd5555">&#39;cost&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.xlabel(<span style="color:#cd5555">&#39;iterations (x1,000)&#39;</span>)
</span></span><span style="display:flex;"><span>    plt.title(<span style="color:#cd5555">&#34;Learning rate =&#34;</span> + <span style="color:#658b00">str</span>(learning_rate))
</span></span><span style="display:flex;"><span>    plt.show()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> parameters
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#----------------------------------------------------</span>
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the training set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model without regularization&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.40</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.65</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>训练集准确率为0.948，测试集准确率为0.915。分类图如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img28.jpg" alt=""></p>
<p>使用L2正则化，前向传播公式变化如下：</p>
<p>$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} \tag{1}$$
To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} }<em>\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W</em>{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$</p>
<p>在编程的时候，需要使用<code>np.sum(np.square(W^[l]))</code>，然后将所有项加起来，乘上$\frac{\lambda}{2m}$</p>
<p>后向传播时，在<code>backprop</code>计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值。</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#228b22"># 前向传播</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">compute_cost_with_regularization</span>(A3, Y, parameters, lambd):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implement the cost function with L2 regularization. See formula (2) above.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing parameters of the model
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cost - value of the regularized loss function (formula (2))
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    m = Y.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    W1 = parameters[<span style="color:#cd5555">&#34;W1&#34;</span>]
</span></span><span style="display:flex;"><span>    W2 = parameters[<span style="color:#cd5555">&#34;W2&#34;</span>]
</span></span><span style="display:flex;"><span>    W3 = parameters[<span style="color:#cd5555">&#34;W3&#34;</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    cross_entropy_cost = compute_cost(A3, Y) <span style="color:#228b22"># This gives you the cross-entropy part of the cost</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (<span style="color:#b452cd">2</span>*m)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODER HERE ###</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    cost = cross_entropy_cost + L2_regularization_cost
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> cost
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#----------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 后向传播</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">backward_propagation_with_regularization</span>(X, Y, cache, lambd):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements the backward propagation of our baseline model to which we added an L2 regularization.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input dataset, of shape (input size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- cache output from forward_propagation()
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    lambd -- regularization hyperparameter, scalar
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dZ3 = A3 - Y
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>    dW3 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ3, A2.T) + lambd * W3 / m
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    db3 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ3, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dA2 = np.dot(W3.T, dZ3)
</span></span><span style="display:flex;"><span>    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>    dW2 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ2, A1.T) + lambd * W2 / m
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    db2 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ2, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dA1 = np.dot(W2.T, dZ2)
</span></span><span style="display:flex;"><span>    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 1 line)</span>
</span></span><span style="display:flex;"><span>    dW1 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ1, X.T) + lambd * W1 / m
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    db1 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ1, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    gradients = {<span style="color:#cd5555">&#34;dZ3&#34;</span>: dZ3, <span style="color:#cd5555">&#34;dW3&#34;</span>: dW3, <span style="color:#cd5555">&#34;db3&#34;</span>: db3,<span style="color:#cd5555">&#34;dA2&#34;</span>: dA2,
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dZ2&#34;</span>: dZ2, <span style="color:#cd5555">&#34;dW2&#34;</span>: dW2, <span style="color:#cd5555">&#34;db2&#34;</span>: db2, <span style="color:#cd5555">&#34;dA1&#34;</span>: dA1, 
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dZ1&#34;</span>: dZ1, <span style="color:#cd5555">&#34;dW1&#34;</span>: dW1, <span style="color:#cd5555">&#34;db1&#34;</span>: db1}
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> gradients
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#----------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 使用L2正则化进行梯度下降</span>
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y, lambd = <span style="color:#b452cd">0.7</span>)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the train set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model with L2-regularization&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.40</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.65</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用L2正则化，训练集的准确率为0.938，测试集为0.93，分类图如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img29.jpg" alt=""></p>
<p>使用dropout正则化，在前向传播的时候分为4步：</p>
<blockquote>
<p>1.根据<code>keep_prob</code>生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，<code>Dl = np.rndom.randn(Al.shape[0], Al.shape[1])</code></p>
<p>2.将$d^{[l]}$转化为0-1矩阵， <code>Dl = Dl &lt; keep_prob</code></p>
<p>3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， <code>Al = np.muiltiply(Al, Dl)</code></p>
<p>4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以<code>keep_prob</code>,<code>Al = Al / keep_prob</code></p>
</blockquote>
<p>后向传播也做修改，即对于某一层的$da^{[l]}$，应该进行以下计算：</p>
<blockquote>
<p>1.<code>dAl = dAl * Dl</code></p>
<p>2.<code>dAl = dAl / keep_prob</code></p>
</blockquote>
<p>代码如下：</p>
<div class="highlight"><div style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 84
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 85
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 86
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 87
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 88
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 89
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 90
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 91
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 92
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 93
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 94
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 95
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 96
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 97
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 98
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 99
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">100
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">101
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">102
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">103
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">104
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">105
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">106
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">107
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">108
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">109
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">110
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">111
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">112
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">113
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">114
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">115
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">116
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#228b22"># 前向传播</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">forward_propagation_with_dropout</span>(X, parameters, keep_prob = <span style="color:#b452cd">0.5</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input dataset, of shape (2, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, &#34;W2&#34;, &#34;b2&#34;, &#34;W3&#34;, &#34;b3&#34;:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W1 -- weight matrix of shape (20, 2)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b1 -- bias vector of shape (20, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W2 -- weight matrix of shape (3, 20)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b2 -- bias vector of shape (3, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    W3 -- weight matrix of shape (1, 3)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">                    b3 -- bias vector of shape (1, 1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    keep_prob - probability of keeping a neuron active during drop-out, scalar
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    A3 -- last activation value, output of the forward propagation, of shape (1,1)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- tuple, information stored for computing the backward propagation
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    np.random.seed(<span style="color:#b452cd">1</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># retrieve parameters</span>
</span></span><span style="display:flex;"><span>    W1 = parameters[<span style="color:#cd5555">&#34;W1&#34;</span>]
</span></span><span style="display:flex;"><span>    b1 = parameters[<span style="color:#cd5555">&#34;b1&#34;</span>]
</span></span><span style="display:flex;"><span>    W2 = parameters[<span style="color:#cd5555">&#34;W2&#34;</span>]
</span></span><span style="display:flex;"><span>    b2 = parameters[<span style="color:#cd5555">&#34;b2&#34;</span>]
</span></span><span style="display:flex;"><span>    W3 = parameters[<span style="color:#cd5555">&#34;W3&#34;</span>]
</span></span><span style="display:flex;"><span>    b3 = parameters[<span style="color:#cd5555">&#34;b3&#34;</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#228b22"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
</span></span><span style="display:flex;"><span>    Z1 = np.dot(W1, X) + b1
</span></span><span style="display:flex;"><span>    A1 = relu(Z1)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span>
</span></span><span style="display:flex;"><span>    D1 = np.random.rand(*A1.shape)                    <span style="color:#228b22"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>
</span></span><span style="display:flex;"><span>    D1 = D1 &lt; keep_prob                               <span style="color:#228b22"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>
</span></span><span style="display:flex;"><span>    A1 = np.multiply(A1, D1)                          <span style="color:#228b22"># Step 3: shut down some neurons of A1</span>
</span></span><span style="display:flex;"><span>    A1 = A1 / keep_prob                               <span style="color:#228b22"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    Z2 = np.dot(W2, A1) + b2
</span></span><span style="display:flex;"><span>    A2 = relu(Z2)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (approx. 4 lines)</span>
</span></span><span style="display:flex;"><span>    D2 = np.random.rand(*A2.shape)                    <span style="color:#228b22"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span>
</span></span><span style="display:flex;"><span>    D2 = D2 &lt; keep_prob                               <span style="color:#228b22"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span>
</span></span><span style="display:flex;"><span>    A2 = np.multiply(A2, D2)                          <span style="color:#228b22"># Step 3: shut down some neurons of A2</span>
</span></span><span style="display:flex;"><span>    A2 = A2 / keep_prob                               <span style="color:#228b22"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    Z3 = np.dot(W3, A2) + b3
</span></span><span style="display:flex;"><span>    A3 = sigmoid(Z3)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> A3, cache
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#----------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 后向传播</span>
</span></span><span style="display:flex;"><span><span style="color:#8b008b;font-weight:bold">def</span> <span style="color:#008b45">backward_propagation_with_dropout</span>(X, Y, cache, keep_prob):
</span></span><span style="display:flex;"><span>    <span style="color:#cd5555">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Implements the backward propagation of our baseline model to which we added dropout.
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    X -- input dataset, of shape (2, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    cache -- cache output from forward_propagation_with_dropout()
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    keep_prob - probability of keeping a neuron active during drop-out, scalar
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
</span></span></span><span style="display:flex;"><span><span style="color:#cd5555">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    m = X.shape[<span style="color:#b452cd">1</span>]
</span></span><span style="display:flex;"><span>    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dZ3 = A3 - Y
</span></span><span style="display:flex;"><span>    dW3 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ3, A2.T)
</span></span><span style="display:flex;"><span>    db3 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ3, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    dA2 = np.dot(W3.T, dZ3)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (≈ 2 lines of code)</span>
</span></span><span style="display:flex;"><span>    dA2 = dA2 * D2              <span style="color:#228b22"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span>
</span></span><span style="display:flex;"><span>    dA2 = dA2 / keep_prob       <span style="color:#228b22"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    dW2 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ2, A1.T)
</span></span><span style="display:flex;"><span>    db2 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ2, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dA1 = np.dot(W2.T, dZ2)
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### START CODE HERE ### (≈ 2 lines of code)</span>
</span></span><span style="display:flex;"><span>    dA1 = dA1 * D1              <span style="color:#228b22"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>
</span></span><span style="display:flex;"><span>    dA1 = dA1 / keep_prob       <span style="color:#228b22"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>
</span></span><span style="display:flex;"><span>    <span style="color:#228b22">### END CODE HERE ###</span>
</span></span><span style="display:flex;"><span>    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span style="color:#b452cd">0</span>))
</span></span><span style="display:flex;"><span>    dW1 = <span style="color:#b452cd">1.</span>/m * np.dot(dZ1, X.T)
</span></span><span style="display:flex;"><span>    db1 = <span style="color:#b452cd">1.</span>/m * np.sum(dZ1, axis=<span style="color:#b452cd">1</span>, keepdims = <span style="color:#8b008b;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    gradients = {<span style="color:#cd5555">&#34;dZ3&#34;</span>: dZ3, <span style="color:#cd5555">&#34;dW3&#34;</span>: dW3, <span style="color:#cd5555">&#34;db3&#34;</span>: db3,<span style="color:#cd5555">&#34;dA2&#34;</span>: dA2,
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dZ2&#34;</span>: dZ2, <span style="color:#cd5555">&#34;dW2&#34;</span>: dW2, <span style="color:#cd5555">&#34;db2&#34;</span>: db2, <span style="color:#cd5555">&#34;dA1&#34;</span>: dA1, 
</span></span><span style="display:flex;"><span>                 <span style="color:#cd5555">&#34;dZ1&#34;</span>: dZ1, <span style="color:#cd5555">&#34;dW1&#34;</span>: dW1, <span style="color:#cd5555">&#34;db1&#34;</span>: db1}
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#8b008b;font-weight:bold">return</span> gradients
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#228b22">#-------------------------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 使用dropout正则化来梯度下降</span>
</span></span><span style="display:flex;"><span>parameters = model(train_X, train_Y, keep_prob = <span style="color:#b452cd">0.86</span>, learning_rate = <span style="color:#b452cd">0.3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the train set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_train = predict(train_X, train_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#658b00">print</span> (<span style="color:#cd5555">&#34;On the test set:&#34;</span>)
</span></span><span style="display:flex;"><span>predictions_test = predict(test_X, test_Y, parameters)
</span></span><span style="display:flex;"><span><span style="color:#228b22"># 画图</span>
</span></span><span style="display:flex;"><span>plt.title(<span style="color:#cd5555">&#34;Model with dropout&#34;</span>)
</span></span><span style="display:flex;"><span>axes = plt.gca()
</span></span><span style="display:flex;"><span>axes.set_xlim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.40</span>])
</span></span><span style="display:flex;"><span>axes.set_ylim([-<span style="color:#b452cd">0.75</span>,<span style="color:#b452cd">0.65</span>])
</span></span><span style="display:flex;"><span>plot_decision_boundary(<span style="color:#8b008b;font-weight:bold">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>dropout正则化在训练集的预测准确率为0.929，测试集的准确率为0.95。分类图如下：</p>
<p><img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img30.jpg" alt=""></p>
<p><strong>因为有部分公式可能因为博客插件不支持的原因，完整的笔记请看:</strong>
<a href="https://github.com/caixiongjiang/deep-learning-computer-vision">https://github.com/caixiongjiang/deep-learning-computer-vision</a></p>

                    
                    <HR width="100%" id="EOF">
		            <p style="color:#777;">最后修改于 2022-07-20</p>
                    
                    
                    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://image-1252109614.cos.ap-beijing.myqcloud.com/img/20210508215939.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
                </div>
            </div>
            
            
            <nav class="post-pagination">

                
                <a class="newer-posts" href="https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/">
			下回<br>深度学习笔记（6-7节） 
                </a>
                
                
                
                <a class="older-posts" href="https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/">
			上回<br>深度学习笔记（1-3节） 
                </a>
                
            </nav>
            <div class="post-comment-wrapper">
                








<div id="tcomment"></div>



            </div>
        </div>
    </div>
</div>

            </div><div id="single-column-footer">
魔改自 <a href="https://github.com/riba2534/hugo-blog">Riba2534</a> by <a href="https://caixiongjiang.github.io">Jarson Cai</a>
<br>

&copy;
	
	2024 🌀Jarson Cai&#39;s Blog
	</div>
            </div>
    <script>
let app;

app = new Vue({
    el: '#app',
    data: {
        scrollY: 0,
        navOpacity: 0,
        isDrawerOpen: false,
        mounted: false,
        isDarkMode: false
    },
    methods: {
            sgn(t, x) {
                let k = 1. / (1. - 2 * t);
                if (x <= t) return 0;
                else if (x >= 1 - t) return 1;
                else {
                    return k * (x - t);
                }
            },
            handleScroll() {
                this.scrollY = window.scrollY;
                this.navOpacity = this.sgn(.0, Math.min(1, Math.max(0, window.scrollY / (this.pageHeadHeight() - this.navBarHeight() * 0.8))));
                const {navBar, navBackground, navTitle, extraContainer, streamContainer} = this.$refs;

                if (this.navOpacity >= 1) {
                    navBackground.style.opacity = 1;
                    navTitle.style.opacity = 1;
                } else {
                    navBackground.style.opacity = 0;
                    navTitle.style.opacity = 0;
                }
            },
            handleResize() {
                const {navBar, navBackground, navTitle, extraContainer, streamContainer} = this.$refs;
                extraContainer.style.left = (streamContainer.offsetWidth - extraContainer.offsetWidth) + 'px';
            },
            navBarHeight() {
                return this.$refs.navBar.offsetHeight;
            },
            pageHeadHeight() {
                return this.$refs.pageHead.offsetHeight;
            },
            toggleDrawer() {
                this.isDrawerOpen = !this.isDrawerOpen;
                document.getElementsByTagName('html')[0].style.overflow = this.isDrawerOpen ? 'hidden' : 'unset';
            },
            closeDrawer() {
                this.isDrawerOpen = false;
                document.getElementsByTagName('html')[0].style.overflow = this.isDrawerOpen ? 'hidden' : 'unset';
            },
            toggleDarkMode() {
                this.isDarkMode = !this.isDarkMode;
                if (this.isDarkMode==true){
                    document.cookie = "night=1;path=/";
                    document.body.classList.add("night");
                } else {
                    document.cookie = "night=0;path=/";
                    document.body.classList.remove("night");
                }
            },
            debounce(func, wait, options) {
                let lastArgs,
                    lastThis,
                    maxWait,
                    result,
                    timerId,
                    lastCallTime

                let lastInvokeTime = 0
                let leading = false
                let maxing = false
                let trailing = true

                
                const useRAF = (!wait && wait !== 0 && typeof root.requestAnimationFrame === 'function')

                if (typeof func !== 'function') {
                    throw new TypeError('Expected a function')
                }
                function isObject(value) {
                    const type = typeof value
                    return value != null && (type === 'object' || type === 'function')
                }

                wait = +wait || 0
                if (isObject(options)) {
                    leading = !!options.leading
                    maxing = 'maxWait' in options
                    maxWait = maxing ? Math.max(+options.maxWait || 0, wait) : maxWait
                    trailing = 'trailing' in options ? !!options.trailing : trailing
                }

                function invokeFunc(time) {
                    const args = lastArgs
                    const thisArg = lastThis

                    lastArgs = lastThis = undefined
                    lastInvokeTime = time
                    result = func.apply(thisArg, args)
                    return result
                }

                function startTimer(pendingFunc, wait) {
                    if (useRAF) {
                    root.cancelAnimationFrame(timerId)
                    return root.requestAnimationFrame(pendingFunc)
                    }
                    return setTimeout(pendingFunc, wait)
                }

                function cancelTimer(id) {
                    if (useRAF) {
                    return root.cancelAnimationFrame(id)
                    }
                    clearTimeout(id)
                }

                function leadingEdge(time) {
                    
                    lastInvokeTime = time
                    
                    timerId = startTimer(timerExpired, wait)
                    
                    return leading ? invokeFunc(time) : result
                }

                function remainingWait(time) {
                    const timeSinceLastCall = time - lastCallTime
                    const timeSinceLastInvoke = time - lastInvokeTime
                    const timeWaiting = wait - timeSinceLastCall

                    return maxing
                    ? Math.min(timeWaiting, maxWait - timeSinceLastInvoke)
                    : timeWaiting
                }

                function shouldInvoke(time) {
                    const timeSinceLastCall = time - lastCallTime
                    const timeSinceLastInvoke = time - lastInvokeTime

                    
                    
                    
                    return (lastCallTime === undefined || (timeSinceLastCall >= wait) ||
                    (timeSinceLastCall < 0) || (maxing && timeSinceLastInvoke >= maxWait))
                }

                function timerExpired() {
                    const time = Date.now()
                    if (shouldInvoke(time)) {
                    return trailingEdge(time)
                    }
                    
                    timerId = startTimer(timerExpired, remainingWait(time))
                }

                function trailingEdge(time) {
                    timerId = undefined

                    
                    
                    if (trailing && lastArgs) {
                    return invokeFunc(time)
                    }
                    lastArgs = lastThis = undefined
                    return result
                }

                function cancel() {
                    if (timerId !== undefined) {
                    cancelTimer(timerId)
                    }
                    lastInvokeTime = 0
                    lastArgs = lastCallTime = lastThis = timerId = undefined
                }

                function flush() {
                    return timerId === undefined ? result : trailingEdge(Date.now())
                }

                function pending() {
                    return timerId !== undefined
                }

                function debounced(...args) {
                    const time = Date.now()
                    const isInvoking = shouldInvoke(time)

                    lastArgs = args
                    lastThis = this
                    lastCallTime = time

                    if (isInvoking) {
                    if (timerId === undefined) {
                        return leadingEdge(lastCallTime)
                    }
                    if (maxing) {
                        
                        timerId = startTimer(timerExpired, wait)
                        return invokeFunc(lastCallTime)
                    }
                    }
                    if (timerId === undefined) {
                    timerId = startTimer(timerExpired, wait)
                    }
                    return result
                }
                debounced.cancel = cancel
                debounced.flush = flush
                debounced.pending = pending
                return debounced
                }

    },
    created() {
        window.addEventListener('scroll', this.handleScroll);
        window.addEventListener('resize', this.handleResize);
        window._nonDesktop = function () {
            let check = false;
            (function (a) {
                if (/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino|android|ipad|playbook|silk/i.test(a) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(a.substr(0, 4))) check = true;
            })(navigator.userAgent || navigator.vendor || window.opera);
            return check;
        };
        
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1");
        if (night==""){
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                
            }
        }else{
            
            if (night=="1") {
                this.toggleDarkMode();
            }
        }
    },
    mounted() {
        this.handleScroll();
        this.handleResize();
        this.mounted = true;

        
        
        
            twikoo.init({
                envId: "https://twikoo-rho-olive.vercel.app/",
                el: '#tcomment',
                region:  null ,
            });
        

        document.querySelectorAll("table").forEach(function(elem){
            elem.classList.add("table-striped");
            elem.classList.add("table");
            elem.classList.add("table-responsive");
            elem.classList.add("table-hover");
        })

        
        spy();
        window.addEventListener('scroll', this.debounce(spy, 250, { 'maxWait': 250 }), false);
        
        
    },
    destroyed() {
        window.removeEventListener('scroll', this.handleScroll);
        window.removeEventListener('resize', this.handleResize);
    }
});



</script>
    </body>
</html>
