<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI部署 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/ai%E9%83%A8%E7%BD%B2/</link>
    <description>Recent content in AI部署 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 06 Sep 2023 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/ai%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>算能（Sophon）sdk部署：以SE5为例</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%9B%BD%E4%BA%A7sdk%E9%83%A8%E7%BD%B2/%E7%AE%97%E8%83%BDsdk%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Wed, 06 Sep 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%9B%BD%E4%BA%A7sdk%E9%83%A8%E7%BD%B2/%E7%AE%97%E8%83%BDsdk%E9%83%A8%E7%BD%B2/</guid>
      <description>Sophon（算能）：SE5 SDK部署 算能官方提供的开发者文档：https://doc.sophgo.com/sdk-docs/v23.05.01/docs_latest_release/docs/SophonSDK_doc/zh/html/index.html
设备信息 目前拿到的盒子型号为SE5，主要参数如下：
4GB A53（cpu）专用 4GB TPU（张量处理器）专用(BM) 4GB VPU（编解码）专用 盒子与加速卡不同，其走的Soc模式，其Host Memory代表芯片主控上的内存，而Device Memory则是代表划分给TPU/VPU的内存。
算力信息
AI算力：INT8为17.6 TOPS，FP32为2.2 TFLOPs。
支持的视频解码能力：H.264 &amp;amp; H.265: 1080P @960fps
支持的视频解码分辨率：8192 * 8192 / 8K / 4K / 1080P / 720P / D1 / CIF
支持的视频编码能力： H.264 &amp;amp; H.265: 1080P @50fps
视频编码分辨率：4K / 1080P / 720P / D1 / CIF
图片解码能力：JPEG:480张/秒 @1080P
环境配置（Soc） tpu-nntc环境供用户在x86主机上进行模型的编译量化，提供了libsophon环境供用户进行应用的开发部署。PCIe用户可以基于tpu-nntc和libsophon完成模型的编译量化与应用的开发部署； SoC用户可以在x86主机上基于tpu-nntc和libsophon完成模型的编译量化与程序的交叉编译，部署时将编译好的程序拷贝至SoC平台（SE微服务器/SM模组）中执行。
开发环境配置 因为最后运行的环境为soc平台，以及需要完成对摄像头的拉流、模型推理。开发平台需要的开发环境为交叉编译环境。需要先在x86_64架构上的主机配置交叉编译环境。
环境要求：Linux系统（Ubuntu）、x86_64架构、能连接外网（互联网）
下载交叉编译工具链： 1 $ apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu 验证工具链： 1 2 $ which aarch64-linux-gnu-g++ # 终端输出 /usr/bin/aarch64-linux-gnu-g++ 配置pipeline交叉编译需要的libsophon，sophon-opencv，sophon-ffmpeg: 具体参考https://github.</description>
    </item>
    
    <item>
      <title>从零制作昇腾开发环境Docker镜像</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83docker%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Mon, 21 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83docker%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</guid>
      <description>昇腾开发环境Docker镜像制作 昇腾计算服务器信息 登录服务器后，查看计算卡信息：
1 npu-smi info 显示如下信息： 可以发现服务器npu信息可以正常获取，说明服务器的npu卡驱动和固件已经安装完毕，不需要我们继续安装！
查看服务器的架构：
1 uname -m 发现为x86_64架构。
查看上述npu为8张昇腾910B，每张卡有16G的显存。
下载nnrt和Toolkit 要在计算平台上进行开发，需要四要素：
驱动+固件 nnrt toolkit MindStudio 由于我们的服务器已经下好了驱动和固件，所以我们已经不需要驱动和固件了，而且在服务器上只能使用CLI界面进行操作，所以不需要MindStudio软件了！
前往昇腾社区的相应下载界面:https://www.hiascend.com/zh/software/cann/community
从零制作Docker镜像 制作基础镜像 检查Docker是否有效： 1 docker images 拉取一个Ubuntu的Docker镜像： 1 docker pull ubuntu:18.04 通过镜像启动一个容器并进入容器： 1 docker run -it ubuntu:18.04 /bin/bash 容器内安装常用工具 容器内安装vim工具： 1 2 $ apt-get update $ apt-get install vim 更换Ubuntu的apt源： 1 2 3 4 $ cd /etc/apt $ cp sources.list sources.list.bak # 制作一个备份 $ &amp;gt;sources.list # 清空内容 $ vim sources.</description>
    </item>
    
    <item>
      <title>ATC模型转换工具</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/atc%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 09 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/atc%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7/</guid>
      <description>ATC模型转换工具 ATC模型转换工具聚焦于开源框架的网络模型（Caffe，TensorFlow，Pytorch）通过昇腾张量编译器ATC（Ascend Tensor Compiler）将其转换成昇腾AI处理器支持的离线模型。模型转换的过程中可以实现算子调度的优化、权重数据重排、内存使用优化等（自动），可以脱离设备完成模型的预处理。
ATC模型转换-快速入门 下图是整个ATC工具的功能架构：
以上支持Caffe，TensorFlow，ONNX，MindSpore四个框架转化为离线模型(.om)。
Caffe需要的文件：model.protoxt（caffe网络模型结构）、model.caffemodel（caffe网络模型权重） TensorFlow需要的文件：model.pb（包含TensorFlow模型的网络结构和权重） ONNX需要的文件（代表Pytorch框架）：model.pt、model.pth &amp;ndash;&amp;gt; model.onnx（包含ONNX模型的计算图和权重） MindSpore需要的文件（华为自家的框架）：model.air 需要注意的是MindSpore训练出来的模型文件有两种：.air和.mindir，其中.air经过转化之后可以用于离线推理，.mindir经过转化之后可以用于在线推理。
离线推理和在线推理的区别
在线推理是在AI框架内执行推理的场景，这通常只能用于验证结果。不利用在设备上进行迁移，因为它需要依赖厚重的AI框架，也没有对模型的推理速度进行优化。而离线模型不依赖于AI框架（需要推理框架，但通常在设备上会集成），可以直接在设备上进行推理。
在线推理类似于pytorch模型的推理，离线推理类似于NVIDIA的TensorRT的推理。
使用MindStudio进行模型转换
模型转化的过程已经在开发环境部署中介绍过了，不赘述。模型转换成功之后会在命令行提示的目录下生成.om离线模型文件和ModelConvert.txt模型转化日志文件。
ATC模型转换-高手进阶 如何在命令行窗口使用ATC工具？ 如何在模型转换的过程中加入一点动态输入形状信息？ 如何在模型转换过程中把一些预处理动作加入到模型中？ 命令行(CLI)调用ATC
在命令行窗口调用ATC工具，首先需要配置一些环境变量。这些环境变量在安装CANN-toolkit的时候已经展示过了，是第三组环境变量，需要把这些环境变量配置到~/.bashrc中，再source一下。
使用ATC工具的注意事项：
支持原始框架类型为Caffe、TensorFlow、MindSpore、ONNX的模型转换。
当原始框架为Caffe、MindSpore、ONNX时，输入数据类型为FP32，FP16（通过设置入参--input_fp16_nodes实现，MindSpore框架不支持该参数）、UINT8（通过配置数据预处理实现）。
当原始框架为TensorFlow时，输入数据类型为FP16、FP32、UINT8、INT32、BOOL（原始框架类型为TensorFlow时，不支持输入输出数据类型为INT64，需要用户自行将INT64修改为INT32类型）。
当原始框架为Caffe时，模型文件（.prototxt）和权重文件（.caffemodel）的输出名字、输出类型必须保持一致（名称包括大小写也要保持一致）。
当原始框架为TensorFlow时，只支持FrozenGraphDef（.pb）格式。
不支持动态shape的输入，模型转换时需要指定固定数值。
对于Caffe框架网络模型：输入数据最大支持四维，转维算子（reshape、expanddim等）不能输出五维。
模型中的所有层算子除了const算子外，输入和输出需要满足dim != 0。
只支持算子规格参考中的算子，并满足算子限制条件。
ATC模型转换命令举例：
CAFFE模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.prototxt&amp;#34; --weight=&amp;#34;/home/module/resnet50.caffemodel&amp;#34; --framework=0 --output=&amp;#34;/home/module/out/caffe_resnet50&amp;#34; --soc_version=Ascend310 TensorFlow模型转换: 1 $ atc --model=&amp;#34;/home/module/resnet_v1_50.pb&amp;#34; --framework=3 --output=&amp;#34;home/module/out/tf_resnet_v1_50&amp;#34; --soc_version=${soc_version} --input_shape=&amp;#34;input:1,224,224,3&amp;#34; ONNX模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.onnx&amp;#34; --framework=5 --output=&amp;#34;/home/module/out/onnx_resnet50&amp;#34; --soc_version=${soc_version} MindSpore模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.air&amp;#34; --framework=1 --output=&amp;#34;/home/module/out/ResNet50_mindspore&amp;#34; --soc_version=${soc_version} 其中output参数代表的是输出离线模型的文件夹位置， soc_version代表要跑在哪一个昇腾AI处理器上，&amp;ldquo;&amp;ldquo;里面的input代表的是输入节点的名字。</description>
    </item>
    
    <item>
      <title>昇腾计算开发环境配置</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Tue, 08 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid>
      <description>华为部署生态 CANN：华为昇腾的异构计算架构
Ascend：基于达芬奇架构的计算卡，昇腾310，昇腾910等。
MindSpore：基于昇腾架构研发的计算框架，类似于nvidia的TensorRT。
AscendCL：基于昇腾架构的计算语言，类似于nvidia的CUDA语言。
华为昇腾开发环境 首先有了硬件之后，需要先装驱动。驱动装完之后往上需要装用于推理的软件包，叫nnrt(Nerual Network RunTime)。这样我们就已经装完了运行的环境。为了我们开发方便，我们还需要在其之上装一个Toolkit。这些和nvidia都是非常像的，驱动 -&amp;gt; nvcc -&amp;gt; toolkit。
完成上述步骤，就可以在命令行编译运行我们的程序了。如果为了方便开发，还可以再安装一个MindStudio的软件。
驱动安装 首先关注一下华为昇腾社区的官网：https://www.hiascend.com
点击产品中的固件和驱动，选择加速卡的产品系列，由于此次公司的计算卡是昇腾910处理器，这里选择Atlas 300T Pro训练卡（型号：9000）。选择.run格式的驱动包，自行选择ARM和X86_64架构的包。需要注意的是这里选择的CANN版本与后面要安装的nnrt版本有关联。
CANN架构包下载 点击产品中的CANN，选择社区版下载，选择与驱动安装时相同的架构的选项，选择nnrt和toolkit的包进行下载。
nnrt是推理包，nnae是面向训练的包。
MindStudio下载 MindStudio提供您在AI开发所需的一站式开发环境，支持模型开发、算子开发以及应用开发三个主流程中的开发任务。 依靠模型可视化、算力测试、IDE本地仿真调试等功能，MindStudio能够帮助您在一个工具上就能高效便捷地完成AI应用开发 MindStudio采用了插件化扩展机制，开发者可以通过开发插件来扩展已有功能。
选择产品中的全流程开发工具链MindStudio，选择对应操作系统的软件包进行下载。
开发者文档 点击右上角的文档选项，进入昇腾文档，下滑，找到如下页面：
点击CANN软件安装，选择安装开发环境，按照文档进行对应系统的依赖。
然后按照安装驱动（包含重启）-&amp;gt;安装nnrt（包含配置环境变量）-&amp;gt;安装toolkit-&amp;gt;安装MindStudio：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ ./你的驱动安装包(.run包) --full # 重启之后进行nnrt安装 $ ./你的nnrt安装包(.run包) --install # 在.bashrc中配置一下环境变量（按照提示来），并source一下.bashrc $ ./你的toolkit安装包(.run包) --install # 这里会提示很多的环境变量配置（都是命令行开发所必须的） # 可以使用env.txt保存一下 $ vim env.txt # 复制并保存 # 环境变量分组：第一组是推理离线应用所需要的环境变量；第二组是推理在线应用所需要的环境变量；第三组是模型转换所需要的环境变量 $ tar zxvf .</description>
    </item>
    
  </channel>
</rss>
