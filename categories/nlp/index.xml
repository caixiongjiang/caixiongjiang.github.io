<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/nlp/</link>
    <description>Recent content in NLP on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 31 Aug 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GraphRAG技术解读</title>
      <link>https://caixiongjiang.github.io/blog/2024/rag/graphrag/</link>
      <pubDate>Sat, 31 Aug 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/rag/graphrag/</guid>
      <description>GraphRAG：用知识图谱增强RAG RAG通过外挂知识库，适合LLM访问私人或者特定领域的数据并解决幻觉问题。
RAG的基本方法集成了向量数据库和LLM，其中向量数据库负责检索用户查询的上下文，LLM根据检索的上下文内容生成答案。这种方法在很多情况下效果好，速率快，但遇到复杂的问题就很难完成，譬如LLM的推理需要涉及不同的信息之间进行关联。
举一个例子，常规的RAG通常会按照下面的步骤来回答问题：
问题：识别这个人：确定谁打败了Allectus？ 系统： 1、检索男人的儿子：查找此人家庭的信息，特别是他的儿子 2、LLM通过检索到的上下文找到儿子：识别儿子的名字 这里的挑战通常出现在检索环节，因为常规RAG基于语义相似性检索文本，而不能直接回答知识库中可能没有明确提及具体细节的复杂查询。这种限制使得很难找到所需的确切信息，通常需要昂贵且不切实际的解决方案，例如手动创建Q&amp;amp;A对以进行频繁的查询。
为了应对这些挑战，微软研究公司推出了GraphRAG，这是一种全新的方法，通过知识图增强RAG检索和生成。在接下来的章节中，我们将解释GraphRAG引擎如何工作，以及如何使用Milvus矢量数据库运行它。
GraphRAG的工作原理 与常规RAG不同的是，GraphRAG通过结合知识图谱（KG）来增强RAG。知识图谱在上一篇文章中已经讲过，它是根据实体之间的关系存储和索引相关或者无关数据的数据结构。
GraphRAG Pipeline通常由两个基本过程组成：索引和查询
索引（Indexing） GraphRAG的索引过程包括四个关键步骤：
1、文本单元分割（Chunking）：整个输入语料库被划分为多个文本单元（文本块）。这些块是最小的可分析单位，可以是段落、句子或其他逻辑单位。通过将长文档分割成更小的块，我们可以提取和保存有关此输入数据的更多详细信息。
2、实体、关系和声明提取：GraphRAG使用LLM来识别和提取所有实体（人员、地点、组织等名称）、它们之间的关系以及每个文本单元文本中表达的关键声明。我们将使用这个提取的信息来构建一个初始知识图谱。
3、分层聚类：GraphRAG使用Leiden技术在初始知识图谱上执行分层聚类。Leiden是一种社区检索算法。可以有效发现图表中的社区结构。每个集群中的实体被分配给不同的社区，以便进行更深入的分析。
注意：社区是图中的一组节点，这些节点彼此紧密相连，但与网络中其他密集组的连接稀疏。
社区摘要生成：GraphRAG使用自下而上的方法为每个社区及其成员生成摘要。这些摘要包括社区内的主要实体、他们的关系和关键主张。此步骤概述了整个数据集，并为后续查询提供了有用的上下文信息。
查询（Querying） GraphRAG有两种不同的查询工作流程，专为不同的查询量身定做。
Global Search：通过利用社区摘要，将整个知识库相关的整体问题推理。
Local Search：通过向它们的邻居和相关概念筛出特定实体
Global Search Global Search工作流程包含以下阶段：
1、用户查询和对话历史记录：系统将用户查询和对话历史记录作为初始输入。
2、社区报告批处理：系统使用LLM从社区层次结构的指定级别生成的节点报告作为上下文数据。这些社区报告被洗牌并区分为多个批次。
3、评级中间回复（Rated Intermediate Responses, RIR）： 每批社区报告都会被进一步划分为预定义大小的文本块。每个文本块用于生成一个中间回复。回复包含一个信息片段列表，称为点。每个点都有一个表示其重要性的数字分数。这些生成的中间回复就是额定中间回复（额定中间回复 1、回复 2&amp;hellip;&amp;hellip;回复 N）。
4、排序和筛选：系统对这些中间回复进行排序和筛选，选出最重要的点。被选中的重要内容构成汇总的中间回复。
5、最终响应：聚合的中间响应被用作生成最终响应的上下文。
global search阶段的使用的map_system_prompt提示词模版：https://github.com/microsoft/graphrag/blob/main//graphrag/query/structured_search/global_search/map_system_prompt.py DeepSeekV2 翻译的中文版本：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # Copyright (c) 2024 Microsoft Corporation.</description>
    </item>
    
    <item>
      <title>高级RAG检索策略之知识图谱</title>
      <link>https://caixiongjiang.github.io/blog/2024/rag/%E9%AB%98%E7%BA%A7rag-kg/</link>
      <pubDate>Thu, 29 Aug 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/rag/%E9%AB%98%E7%BA%A7rag-kg/</guid>
      <description>高级RAG检索策略之知识图谱 在文档的属性分明，不同的文档之间没有很强的关联性，或者每个文档的长度都不大的情况下，基于向量数据库的普通RAG也能很好地工作。但如果使用大型的私有知识库，普通的RAG检索的效果往往没有那么理想。知识图谱的核心在于通过三元组形式（实体-关系-实体）来描述事物之间的关联，这种结构化的数据表示方法不仅能够捕捉数据的语义含义，还能便于理解和分析。
整体流程 知识图谱在RAG中的使用流程如图所示：
首先和普通RAG的流程相同的是，它们首先都需要进行文本分块。后续知识图谱RAG会将分块后的文档进行实体和关系的提取，提取出实体和关系后保存到图数据库中。
检索的时候会将问题的实体提取出来再对图数据库进行检索，检索结果是一个庞大的实体关系网络，最后将检索到的实体和关系以及目标文本结合问题提交给大模型进行答案生成。
当然知识图谱RAG也可以图检索、向量检索和关键词检索等，这样可以综合利用多个检索的链路进行判断，提高检索的准确性和效率。
解决的场景 在 RAG 中使用知识图谱主要解决在大型文档库上问答和理解困难的问题，特别是那些普通RAG方法难以处理的全局性问题。
普通RAG在回答针对整个文档库的全局性问题时表现不佳，例如问题：请告诉我所有关于XXX的事情，这个问题涉及到的上下文可能分布在整个大型文档库中，普通RAG的向量检索方法很难得到这种分散、细粒度的文档信息，向量检索经常使用top-k算法来获取最相近的上下文文档，这种方式很容易遗漏关联的文档块，从而导致信息检索不完整。
知识图谱RAG和普通RAG的区别 知识图谱RAG使用图结构来表示和存储信息，捕捉实体间的复杂关系，而普通RAG通常使用向量化的文本数据 知识图谱RAG通过图遍历和子图检索来获取相关信息，普通RAG主要依赖向量相似度搜索 知识图谱RAG能更好地理解实体间的关系和层次结构，提供更丰富的上下文，普通RAG在处理复杂关系时能力有限 数据入库 知识图谱和普通RAG的数据入库流程不同，普通RAG在进行文档分块后，通常使用embedding模型将文档块进行向量化，将向量和文档保存到向量数据库。与普通RAG不同，知识图谱RAG在入库过程中会将文档块进行实体和关系的提取，提取出实体和关系后保存到图数据库。
实体提取的传统方法是基于预定义的规则和词典、统计机器学习或者深度学习等技术，但进入到LLM时代后，实体提取更多的是使用LLM来进行，因为LLM能够更好地理解文本的语义，实现也更加简单。
LLamaIndex中的KnowledgeGraphIndex类中的实体提取提示词如下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 DEFAULT_KG_TRIPLET_EXTRACT_TMPL = ( &amp;#34;Some text is provided below. Given the text, extract up to &amp;#34; &amp;#34;{max_knowledge_triplets} &amp;#34; &amp;#34;knowledge triplets in the form of (subject, predicate, object).</description>
    </item>
    
    <item>
      <title>Advanced-RAG: RAG进阶使用技巧</title>
      <link>https://caixiongjiang.github.io/blog/2024/rag/advanced_rag/</link>
      <pubDate>Sun, 25 Aug 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/rag/advanced_rag/</guid>
      <description>Naive RAG 前言：目前下述的代码都基于Langchain 0.1的版本进行，目前Langchain已经更新到0.2，还有在构建RAG应用的时候还是不要过分依赖框架，降低灵活性，这有时候会让你的工程开发陷入被动！
首先先介绍一下最简单的RAG的流程，其主要的流程如下图所示：
将知识库文本拆分为块，然后使用一些Transformer Encoder模型将这些块嵌入向量中，将所有这些向量放入索引中，最后为LLM创建一个提示，告诉模型根据我们在搜索步骤中发现的上下文，回答用户的查询。
在运行时，我们使用相同的编码器模型矢量化用户的查询，然后针对索引执行此查询矢量的搜索，找到top-k结果，从我们的数据库中检索相应的文本块，并将其作为上下文输入LLM提示符。
示例的RAG提示词模版如下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 英文提示词 prompt_template = &amp;#34;&amp;#34;&amp;#34;Give the answer to the user query delimited by triple backticks ```{query}```\ using the information given in context delimited by triple backticks ```{context}```.\ If there is no relevant information in the provided context, try to answer yourself, but tell user that you did not have any relevant context to base your answer on.</description>
    </item>
    
    <item>
      <title>Document-AI: 使用模型工具处理非结构化、复杂的各类文档</title>
      <link>https://caixiongjiang.github.io/blog/2024/rag/document_ai/</link>
      <pubDate>Tue, 06 Aug 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/rag/document_ai/</guid>
      <description>Document-AI 随着RAG的爆火，目前市面上出现了许多做文档解析的工具，它们相比传统的文档解析，增加了许多非结构化数据的读取和识别。现在我将会介绍几款目前市面上比较火的工具。
Datalab开源工具 Datalab目前开源了三款免费使用的工具，不过12个月内超过500w美元收入的组织进行商业使用时需要收费。旗下有三款文档解析工具，分别是Surya、Texify、Marker。
Datalab门户：https://www.datalab.to
Surya Surya是一个文档OCR工具包，它可以胜任：
90多种语言的OCR，与云服务相比，具有良好的基准 任何语言的行级文本检测 布局分析（表、图像、标题等检测） 阅读顺序检测 官方Github:https://github.com/VikParuchuri/surya
下载
首先下载好Python3.9+和Pytorch环境，然后再安装surya-ocr。
1 2 3 4 5 conda create -n parser python=3.10 conda activate parser # 我这里使用的 MacOS 系统 pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pip install surya-ocr OCR（文本识别）
命令行执行： 1 surya_ocr DATA_PATH --images --langs hi,en 常用参数说明：
DATA_PATH可以是图片，pdf，或者是包含图片和pdf的文件夹 --langs用于指定OCR的语言，可以通过逗号指定多种语言，但不建议同时超过4种。这里使用语言名称或双字母ISO代码来指定。语言相关的ISO代码查询：https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes --lang_file可以为不同的pdf/图像分别使用不同的语言，可以通过该参数自行指定语言。格式为JSON dict，键为文件名，值为列表，如{&amp;quot;file1.pdf&amp;quot;: [&amp;quot;en&amp;quot;, &amp;quot;hi&amp;quot;], &amp;quot;file2.pdf&amp;quot;: [&amp;quot;en&amp;quot;]}。 --images参数将保存页面的图像和检测到的文本行（可选） --results_dir参数指定要保存结果的目录 --max参数指定要处理的最大页数 --start_page参数指定要开始处理的页码 结果results.json文件的格式说明，其中key是没有扩展名（.pdf）的输入文件名。每个value将会是一个字典列表，输入文档每页一个，每页字典都包含：
text_lines - 每行检测到的文本和边界框 text - 行中的文本 confidence - 检测到的文本中的模型置信度（0～1） polygon -（x1，y1），（x2，y2），（x3，y3），（x4，y4）格式的文本行的多边形。这些点从左上角按顺时针顺序排列。 bbox - 文本行（x1，y1，x2，y2）格式的轴对齐矩形。（x1，y1）是左上角，（x2，y2）是右下角。 language - 该页面指定的语言 page - 文件中的页码 image_bbox - （X1，y1，x2，y2）格式的图像的bbox。（x1，y1）是左上角，（x2，y2）是右下角。所有行bbox都将包含在这个bbox中。 结果示例：</description>
    </item>
    
  </channel>
</rss>
