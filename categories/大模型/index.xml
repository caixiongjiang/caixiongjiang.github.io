<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大模型 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
    <description>Recent content in 大模型 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 18 Jul 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLaMA-Factory：大模型的训练工厂</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llama-factory%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 18 Jul 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llama-factory%E5%85%A5%E9%97%A8/</guid>
      <description>LLaMA-Factory：大模型的训练工厂 Github官网：https://github.com/hiyouga/LLaMA-Factory
LLaMA-Factory集成了几乎市面上所有的开源大模型架构的训练，也集成了Full、Freeze、LoRA、QLoRA等主流训练方法，以及各种数据精度（16、8、4、2）的训练。
训练框架部署 如果你使用conda管理python环境，则使用如下命令： 1 2 3 4 5 conda create -n LLaMA-Factory python=3.10 conda activate LLaMA-Factory git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git cd LLaMA-Factory pip install -e &amp;#34;.[torch,metrics]&amp;#34; 如果你使用Docker部署，可以直接执行说明书指出的Build without Docker Compose: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 docker build -f ./docker/docker-cuda/Dockerfile \ --build-arg INSTALL_BNB=false \ --build-arg INSTALL_VLLM=false \ --build-arg INSTALL_DEEPSPEED=false \ --build-arg INSTALL_FLASHATTN=false \ --build-arg PIP_INDEX=https://pypi.</description>
    </item>
    
    <item>
      <title>Xinference：为LLM app赋能</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/xinference/</link>
      <pubDate>Thu, 11 Jul 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/xinference/</guid>
      <description>Xinference：为LLM app赋能 Xinference Github官网：https://github.com/xorbitsai/inference Xinference 官方中文文档：https://inference.readthedocs.io/zh-cn/latest/index.html Xinference 集成了 LLM、embedding、image、audio、rerank模型。关于LLM，则集成了Transformer、vLLM、Llama.cpp、SGLang引擎。该框架主要是制作了一个UI界面来自主控制集成好的模型，为应用开发提供底层能力。
环境安装 本地环境安装：可根据不同的引擎来安装不同的版本，具体看https://inference.readthedocs.io/zh-cn/latest/getting_started/installation.html Docker镜像：由于Docker管理起来比较方便，我这里就使用了Docker的部署方式。 官方提供了方便下载的镜像： 1 docker pull registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:&amp;lt;tag&amp;gt; 我选择了v0.13.0的版本。
Xinference启动 使用Docker启动的方式比较简单，使用官方提供的命令：
1 docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:v&amp;lt;your_version&amp;gt; xinference-local -H 0.0.0.0 --log-level debug 由于公司的网下载大模型比较慢，所以通常我们会选择在本地下好的模型进行加载，所以需要对命令做一点改造
现在本地新建文件夹，并把需要的模型传入。
1 mkdir xinference_hub 开始启动镜像：
1 2 # 这里使用了我实际的镜像名，并将需要的模型映射进了镜像内，并在后台运行 docker run -v /data/caixj/Interests/xinference_hub:/model_hub -e XINFERENCE_HOME=/model_hub -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 -itd --name xinference --gpus all caixj/xinference:v0.13.0 xinference-local -H 0.0.0.0 --log-level debug 模型启用 按照上述镜像启动后，容器已经在后台运行，此时可以在浏览器输入相应https://&amp;lt;ip&amp;gt;:9998进行访问了。
在本地开发环境，可以使用界面来操作配置模型，管理模型，启动模型等。
在框架内本身集成了很多现有的模型，如下图所示，你可以直接点击启动，框架会自动去下载模型。 但很多时候网络不好的时候，我们则需要将自己映射进去的模型给注册了并使用。 以Qwen2-7B-Instruct模型为例: 主要还是填写一些基本信息，以及模型的格式，模型的架构等，注意名字要加前缀，防止更系统中自带的模型名字不同而冲突。</description>
    </item>
    
    <item>
      <title>LLM启动大全</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llm%E9%83%A8%E7%BD%B2%E5%A4%A7%E5%85%A8/</link>
      <pubDate>Mon, 06 May 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llm%E9%83%A8%E7%BD%B2%E5%A4%A7%E5%85%A8/</guid>
      <description>LLM启动大全 大模型发展至今，已经诞生了各种各样的部署框架以及使用方式，他们通常适用于不同的场景。现在通过自己了解的知识对大模型的各种启动方式做一个总结。
模型下载 首先启动大模型，首先需要下载一个模型，模型的下载首选地址为HF官网模型库：https://huggingface.co/models
假设你的网络不能进入上述网址，可以进入魔搭社区下载，大部分模型都会同步到这里：https://www.modelscope.cn/models
我们这里使用Qwen1.5系列的模型做示例，具体的模型型号为Qwen1.5-4B-Chat。
模型启动 原生Transformers库启动 使用最原始的Transformer框架启动，需要写一小部分代码，官方已经为你写好了Demo，地址在https://www.modelscope.cn/models/qwen/Qwen1.5-4B-Chat/summary。
为了通用性，我对代码做了略微的修改：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer from threading import Thread import os from .</description>
    </item>
    
    <item>
      <title>vLLM如何提高并发？</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/vllm%E5%B9%B6%E5%8F%91/</link>
      <pubDate>Mon, 22 Apr 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/vllm%E5%B9%B6%E5%8F%91/</guid>
      <description>vLLM并发加速 论文：Efficient Memory Management for Large Language Model Serving with PagedAttention
链接：https://dl.acm.org/doi/pdf/10.1145/3600006.3613165
随着LLM模型能力的逐步提升，相关的业务应用也在井喷式增长。作为一个商业服务，必须承受高用户量的并发，vLLM的出现很好解决了这个需求。
LLM服务吞吐量的挑战 大模型服务有以下几个特点：
高吞吐量的服务需要批量处理足够多的请求。 kv缓存巨大，并且会动态增长和收缩。 KV cache
LLM权重的特点如下，大部分参数在服务期间保持静态，近30%的内存用于存储请求的动态状态，这些状态在模型结构中属于注意力机制中的key和value组成的张量组，通常被称为kv cache。
在标准的自注意力计算中，对于序列中的每个元素，模型都需要计算其与序列中所有其他元素的注意力权重，并根据这些权重聚合信息。这个过程涉及大量的计算，尤其是当序列很长时，计算量会呈平方级增长。KV缓存是一种优化方法，它通过缓存已经计算过的键（Key, K）和值（Value, V）对，来减少重复计算。具体来说，当模型处理序列中的下一个元素时，它只需要计算新的查询（Query, Q）与之前缓存的键值对之间的注意力权重，而不需要重新计算整个序列的键值对。这样可以显著减少计算量，提高处理长序列时的效率。
LLM的原理 LLM的生成原理为根据上文生成下文，学习大量数据的生成规律。在生成每一个token时，需要不断对前文（先前的序列）做注意力。对于每个请求，都会重复这个昂贵的过程，直到模型输出终止令牌。这种顺序生成过程使工作负载被内存绑定，没有充分利用GPU的计算能力，并限制了服务吞吐量。
现有深度学习框架的限制 大多数深度学习框架要求张量存储在连续的内存中。而KV 缓存随时间动态增长的特性使其在两个方面效率低下：
预先分配了具有请求最大长度的连续内存块（例如2048个令牌）。这可能会导致严重的内部碎片化，因为请求的实际长度可能比其最大长度短得多。此外，即使实际长度是先验的，预分配仍然效率低下：由于整个块在请求的生命周期内被保留，其他较短的请求无法利用当前未使用的块的任何部分。 现有的LLM使用的均为并行采样等高级算法，请求通常由多个序列组成,而连续存储决定了其不能共享权重。 LLM的批处理挑战 虽然批次足够大的时候，可以提高LLM的计算利用率（请求共享模型权重，能摊销）。 但是LLM的批次请求受两个点限制：
请求长度不一会产生等待导致的排队延迟， 对长度不一的序列使用填充策略平衡长度会造成很大的内存和计算浪费。 vLLM的作者沿用了先前提出的蜂窝批处理和迭代级调度。这些技术在迭代级别工作。每次迭代后，已完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单次迭代后处理新请求，而不是等待整个批次完成。此外，使用特殊的GPU内核，这些技术无需填充输入和输出。通过减少排队延迟和填充效率低下，细粒度的配料机制显著增加了LLM服务的吞吐量。
LLM中的内存挑战 从GPU制造的成本来说，计算能力的增长时很快的，但GPU的内存的增长却很缓慢。对于大模型的巨大参数量，内存已经成为主要的瓶颈。
缓存共享的挑战： 大模型的输入输出缓存主要分为两个部分，输入请求的提示词KV缓存（在作者的实验中占用12%的内存），它时可以共享的，可以减少内存使用量。另一部分是自动回归模型阶段生成的KV缓存，由于不同的样本结果对其上下文和位置的依赖，该阶段的KV缓存应保持不共享的状态。KV缓存共享的程度取决于特定的解码算法。
输入输出内存的调度： 调度未知的输入和输出长度。对LLM服务的请求在输入和输出长度上表现出可变性。这需要内存管理系统来适应广泛的提示长度。此外，随着请求的输出长度在解码时增加，其KV缓存所需的内存也会扩展，并可能耗尽传入请求的可用内存或现有提示的持续生成。系统需要做出调度决策，例如从GPU内存中删除或交换一些请求的KV缓存。
现有系统中的内存管理 现有深度学习框架（Pytorch等）的特点：
张量存储在连续内存中，一个请求的KV缓存被存储为跨越不同位置的连续张量。 LLM的输出长度不可预测，无论请求的实际输入或最终输出长度如何，他们根据请求的最大可能序列长度静态地为请求分配一块内存。 vLLM框架 作者开发了一个LLM的服务引擎vLLM应对一般深度学习框架推理LLM出现的挑战。vLLM采用集中式调度器来协调分布式GPU线程执行，KV缓存管理器通过PagedAttention启用，以分页方式有效管理KV缓存。
PageAttention PagedAttention允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV缓存划分为KV块，每个块包含固定数量令牌的密钥和值向量，将其表示为KV块大小（B）。
如下图：键（k）和值（v）向量分布在三个块上，这三个块在物理内存上不毗连。这中管理策略使得内存碎片较小，可以分配更大的需要连续内存的大对象，防止出现总内存足够，而因为内存碎片无法分配大的连续内存的情况。
总之，PagedAttention算法允许KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。
KV缓存管理器 </description>
    </item>
    
    <item>
      <title>NLP &amp; LLM入门</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</link>
      <pubDate>Thu, 18 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</guid>
      <description>NLP &amp;amp; LLM入门 对于一个LLM菜鸡来说，从头理解LLM主要架构和任务是很有意义的。
LLM发展 双向RNN中的注意力 论文：Neural Machine Translation by Jointly Learning to Align and Translate
链接：https://arxiv.org/pdf/1409.0473.pdf
该论文引入了循环神经网络（RNN）的注意力机制。传统的 RNN 在处理较长序列时可能会遇到梯度消失或梯度爆炸等问题，导致远程位置的信息难以传递。注意力机制能够通过给予不同位置的输入不同的权重，使模型更好地捕捉到远程位置的信息，从而提高模型处理远程序列的能力。后续Transformer网络的开发也是为了提高网络的远程序列建模能力。
Transformer 论文：Attention Is All You Need
链接：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
无论是做NLP还是做CV，这篇论文大家应该都很熟悉。Transformer是一个编码器解码器结构，其引入了位置编码，使得模型能够直接看到全局的信息，更有利于序列的长距离建模。其主要的结构Transformer Block主要由一个多头自注意力和一个前向传播网络（FFN）组成，内部的归一化使用的是Layer Normal，这可以摆脱训练批次对模型训练性能的影响。
Post-LN vs Pre-LN Transformer 论文：On Layer Normalization in the Transformer Architecture
链接：https://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf
该论文提出Transformer结构中的Layer Normal结构的位置应该放置在多头注意力和FFN之前。该论文表明了Pre-LN的结构比先前的Post-LN效果更佳，解决了梯度问题，许多架构在实践中采用了这一点，但表示它有可能导致崩溃。关于使用Post-LN和Pre-LN的争论目前还在，可以关注后续的发展。
NLP中的训练范式 论文：Universal Language Model Fine-tuning for Text Classification
链接：https://arxiv.org/pdf/1801.06146.pdf
预训练微调的范式最早是在CV界广泛应用。早期NLP中的预训练微调的应用不广泛，大部分研究都是从头开始训练的，少数进行的预训练微调的效果并不好，比随机初始化的结果还差，或者需要很多的数据集作为预训练的语料库。该论文根据CV模型和NLP模型不同的架构，设计了三层完全相同的LSTM网络拼接的预训练网络。
上图所示即为该论文提出的语言模型的预训练微调的范式ULMFit，其主要分为三个阶段：
在大量文本语料库上训练语言模型 在特定任务数据上微调此预训练的语言模型，使其能适应文本的特定风格和词汇。 对特定任务数据的分类器进行微调，逐步解冻层，避免灾难性遗忘。 这个训练范式（在大型语料库上训练语言模型，然后在下游任务上进行微调）是后续基于Transformer的模型和基础模型（如BERT、GPT-2/3/4、RoBERTa等）中使用的中心方法。
然而，在使用Transformer架构时，逐渐解冻在实践中通常不会例行完成，所有层通常都同时进行微调。
BERT 论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</description>
    </item>
    
    <item>
      <title>LangChain:LLM应用框架</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sun, 14 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</guid>
      <description>LangChain：LLM应用框架 下面是一个简单场景（LangChain结合本地知识库对用户的提问进行回答）的整体流程：
本地知识库处理 首先需要加载本地知识库文件，将其转化为知识向量库。
文本嵌入模型：它本身用于将高维的稀疏数据（文本）转化为低维稠密数据。主流的嵌入模型有Word2Vec、GloVe、BERT、ERNIE、text2vec。 向量索引库：它在不同领域中用于存储和高效查询向量数据，主要应用于图像、音频、视频、自然语言等领域的相似性搜索任务。主流的库有faiss、pgvector、Milvus、pinecone、weaviate、LanceDB、Chroma等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from langchain.document_loaders import DirectoryLoader, unstructured from langchain.</description>
    </item>
    
  </channel>
</rss>
