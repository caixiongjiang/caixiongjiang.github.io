<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大模型 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
    <description>Recent content in 大模型 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 18 Jan 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NLP &amp; LLM入门</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</link>
      <pubDate>Thu, 18 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</guid>
      <description>NLP &amp;amp; LLM入门 对于一个LLM菜鸡来说，从头理解LLM主要架构和任务是很有意义的。
LLM发展 双向RNN中的注意力 论文：Neural Machine Translation by Jointly Learning to Align and Translate
链接：https://arxiv.org/pdf/1409.0473.pdf
该论文引入了循环神经网络（RNN）的注意力机制。传统的 RNN 在处理较长序列时可能会遇到梯度消失或梯度爆炸等问题，导致远程位置的信息难以传递。注意力机制能够通过给予不同位置的输入不同的权重，使模型更好地捕捉到远程位置的信息，从而提高模型处理远程序列的能力。后续Transformer网络的开发也是为了提高网络的远程序列建模能力。
Transformer 论文：Attention Is All You Need
链接：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
无论是做NLP还是做CV，这篇论文大家应该都很熟悉。Transformer是一个编码器解码器结构，其引入了位置编码，使得模型能够直接看到全局的信息，更有利于序列的长距离建模。其主要的结构Transformer Block主要由一个多头自注意力和一个前向传播网络（FFN）组成，内部的归一化使用的是Layer Normal，这可以摆脱训练批次对模型训练性能的影响。
Post-LN vs Pre-LN Transformer 论文：On Layer Normalization in the Transformer Architecture
链接：https://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf
该论文提出Transformer结构中的Layer Normal结构的位置应该放置在多头注意力和FFN之前。该论文表明了Pre-LN的结构比先前的Post-LN效果更佳，解决了梯度问题，许多架构在实践中采用了这一点，但表示它有可能导致崩溃。关于使用Post-LN和Pre-LN的争论目前还在，可以关注后续的发展。
NLP中的训练范式 论文：Universal Language Model Fine-tuning for Text Classification
链接：https://arxiv.org/pdf/1801.06146.pdf
预训练微调的范式最早是在CV界广泛应用。早期NLP中的预训练微调的应用不广泛，大部分研究都是从头开始训练的，少数进行的预训练微调的效果并不好，比随机初始化的结果还差，或者需要很多的数据集作为预训练的语料库。该论文根据CV模型和NLP模型不同的架构，设计了三层完全相同的LSTM网络拼接的预训练网络。
上图所示即为该论文提出的语言模型的预训练微调的范式ULMFit，其主要分为三个阶段：
在大量文本语料库上训练语言模型 在特定任务数据上微调此预训练的语言模型，使其能适应文本的特定风格和词汇。 对特定任务数据的分类器进行微调，逐步解冻层，避免灾难性遗忘。 这个训练范式（在大型语料库上训练语言模型，然后在下游任务上进行微调）是后续基于Transformer的模型和基础模型（如BERT、GPT-2/3/4、RoBERTa等）中使用的中心方法。
然而，在使用Transformer架构时，逐渐解冻在实践中通常不会例行完成，所有层通常都同时进行微调。
BERT 论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</description>
    </item>
    
    <item>
      <title>LangChain:LLM应用框架</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sun, 14 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</guid>
      <description>LangChain：LLM应用框架 下面是一个简单场景（LangChain结合本地知识库对用户的提问进行回答）的整体流程：
本地知识库处理 首先需要加载本地知识库文件，将其转化为知识向量库。
文本嵌入模型：它本身用于将高维的稀疏数据（文本）转化为低维稠密数据。主流的嵌入模型有Word2Vec、GloVe、BERT、ERNIE、text2vec。 向量索引库：它在不同领域中用于存储和高效查询向量数据，主要应用于图像、音频、视频、自然语言等领域的相似性搜索任务。主流的库有faiss、pgvector、Milvus、pinecone、weaviate、LanceDB、Chroma等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from langchain.document_loaders import DirectoryLoader, unstructured from langchain.</description>
    </item>
    
  </channel>
</rss>
