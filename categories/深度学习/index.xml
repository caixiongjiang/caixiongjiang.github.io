<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 24 Oct 2023 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型高效微调技术（PEFT）</title>
      <link>https://caixiongjiang.github.io/blog/2023/llm/mmdeploy%E9%83%A8%E7%BD%B2%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Tue, 24 Oct 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/llm/mmdeploy%E9%83%A8%E7%BD%B2%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/</guid>
      <description>大模型高效微调技术（PEFT） Adapter Tuning 技术原理 论文：Parameter-Efficient Transfer Learning for NLP 论文链接：http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf
该方法设计了一个Adapter结构，嵌入Transformer结构中。针对一个Transformer Block，增加两个Adapter结构，增加都放在残差结构之前。训练时，固定住原来的预训练模型参数不变，只对Adapter结构和Layer Normal层进行微调。Adapter层是一个类似于Squeeze-and-Excitation层思想的结构，首先使用较高维度的特征投影到较低维度的特征，中间通过一个非线性层，再将低维特征重新映射回原来的高维特征。其参数量主要低维特征的维度决定。
方法效果 对$BERT_{LARGE}$在各个任务上进行全量微调和Adapter-Tuning的结果如下，在不同的任务上，低维特征的维度m不同时效果不同。将m固定在64时，性能会略微下降。
Adapter通过引入额外的0.5%~5%参数可以媲美全量微调的效果，缺点是引入的参数在Transformer Block内部，在面对不同的任务时，需要保存不同的权重副本。
Adapter Fusion 技术原理 方法效果 Adapter Drop 技术原理 方法效果 BitFit 技术原理 论文：BitFit: Simple Parameter-efﬁcient Fine-tuning for Transformer-based Masked Language-models
论文链接：https://arxiv.org/pdf/2106.10199.pdf
Bitfit是一种稀疏微调的方法，它冻结了大部分transformer-encoder参数，只更新bias参数跟特定任务的分类层参数。涉及到的bias参数有attention模块中计算query,key,value跟合并多个attention结果时涉及到的bias，MLP层中的bias，Layernormalization层的bias参数。方法的技术原理非常简单，且需要微调的参数量极小。
方法效果 使用$BERT_{LARGE}$进行全量微调，以及一些对比的高效微调方法，在不同的任务数据集上进行了实验。可以看到仅微调bias的Bitfit方法也能达到略低于3.6%参数的Adapter和使用0.5%参数的Diff-Prune方法的水平，而需要微调的参数量只有0.08%。
而且通过消融实验也可以看出，bias参数的变化并不是在所有结构的部分都比较大。从结果来看，计算query和FFN层的bias参数变化最为明显，只更新这一部分参数也能达到较为不错的效果。
Bitfit只引入了0.03%~0.09%的参数，达到了较好的微调效果，其参数量比其他model tuning的方法少了将近10倍，缺点是更新bias参数依旧需要根据不同的任务保存不同模型的副本。
Prefix Tuning 技术原理 论文：Preﬁx-Tuning: Optimizing Continuous Prompts for Generation
论文链接：https://arxiv.org/pdf/2101.00190.pdf
Prefix Tuning是一种优化前缀提示的微调方法。与前文对模型参数的微调不同，前缀微调的方法对预训练的模型没有任何改动，其目标是通过微调输入的向量参数来改变模型的生成行为。前缀被添加到输入文本的开头，并且通过调整前缀的内容和长度，可以引导模型生成符合用户意图的输出。但是直接更新前缀表示通常会对模型产生较大的负面影响，所以该方法优化了输入向量参数，即前缀的表示，以控制模型的生成结果。
正如下图表示，对于原有的Transformer预训练模型不需要进行改动，而是针对不同的任务训练不同的前缀提示。
为了实现这个目标，文中使用了多层感知机（MLP）来将前缀与模型输入进行连接。通过训练MLP的参数，可以将前缀映射到与模型输入相同的维度，并且学习前缀和输入之间的复杂映射关系。这样还解决了前缀的长度可能与模型预期的输入长度不匹配的问题，通过使用MLP可以将前缀映射到与模型输入相同的维度。
Prefix Tuning与构造Prompt比较相似，构造Prompt是人为的，叫不稳定，不同的Prompt可能会产生极大的差异。而Prefix可以视为隐藏式的Prompt，且该Prompt是可以训练的！
方法效果 使用$GPT-2_{MEDIUM}$和$GPT-2_{LARGE}$作为基础模型进行全量微调和一些常用的高效微调方法，在不同任务上都进行了实验。可以看到在很多任务上的微调效果都很不错，引入额外的参数为0.1%，该部分参数不在模型内部。
通过消融实验证明，在部分实验上仅仅对Embedding层添加前缀表现力不够，性能将会远不如Full Fine-Tuning，因此在每个Layer都增加了一个可训练的提示前缀，尽量逼近全量微调的效果，参数量上涨为2%。
![](/Users/caixiongjiang/Library/Application Support/typora-user-images/image-20231026175645046.png)
论文还对不同的前缀提示长度做了消融研究，发现提示的长度越长，MLP进行转化的效果越好，当然这种增长并不是线性的，随着前缀变长，增长的幅度会越来越小。
Prompt Tuning P-Tuning P-Tuning v2 LoRA AdaLoRA QLoRA </description>
    </item>
    
    <item>
      <title>MMDeploy部署分割模型</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmdeploy%E9%83%A8%E7%BD%B2%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 27 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmdeploy%E9%83%A8%E7%BD%B2%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B/</guid>
      <description>MMDeploy教程 一图看懂MMDeploy的作用： 如果模型的训练是使用mmcv系列工具生成的，那么使用MMDeploy是最好的！
MMDeploy安装 安装MMDeploy 1 2 # pip install mmdeploy --upgrade pip install mmdeploy==1.2.0 下载MMDeploy源码 1 git clone https://github.com/open-mmlab/mmdeploy.git 验证安装配置成功 1 2 3 4 $ python3 $ &amp;gt;&amp;gt;&amp;gt;import mmdeploy $ &amp;gt;&amp;gt;&amp;gt;print(&amp;#39;MMDeploy 版本&amp;#39;, mmdeploy.__version__) # MMDeploy 版本 1.2.0 成功输出上述信息则安装成功。
MMDeploy-模型转换 在线模型转换工具 官方提供了一个在线模型转换工具：https://platform.openmmlab.com/deploee
点击模型转化，再点击新建转化任务之后，会进入这样一个画面： 然后将需要的pth文件和config的python文件上传就可以开始转化了！
在线模型测试工具 官方提供了一个在线模型测试工具：https://platform.openmmlab.com/deploee/task-profile-list
点击模型测试，再点击新建测试任务之后，会进入这样一个画面： 使用Python API进行模型转换 进入主目录： 1 cd mmdeploy 下载ONNX包： 1 2 3 $ pip install onnxruntime $ import onnxruntime as ort $ print(&amp;#39;ONNXRuntime 版本&amp;#39;, ort.__version__) Pytorch模型转ONNX模型： 1 2 3 4 5 6 7 python tools/deploy.</description>
    </item>
    
    <item>
      <title>MMSegmentation使用</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmsegmentation%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 16 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmsegmentation%E4%BD%BF%E7%94%A8/</guid>
      <description>MMSegmentation教程 分割模型预测 语义分割模型预测-命令行模式（CLI） 进入mmsegmentaion的目录： 1 cd mmsegmentation 准备素材（模型配置文件，模型权重文件，素材图片） 1 2 3 4 $ mkdir config $ mkdir checkpoints $ mkdir test_imgs $ mkdir test_results 进行预测 1 2 3 4 5 6 7 8 # 以PSPNet为例 python demo/image_demo.py \ data/street_uk.jpeg \ configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py \ https://download.openmmlab.com/mmsegmentation/v0.5/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth \ --out-file outputs/B1_uk_pspnet.jpg \ --device cuda:0 \ --opacity 0.5 语义分割模型预测-Python API 对于批量图片的预测，或者需要对图像进行后处理，我推荐使用Python API的方式。
进入mmsegmentation目录： 1 cd mmsegmentation 编写脚本： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 import torch import numpy as np import matplotlib.</description>
    </item>
    
    <item>
      <title>MMSegmentation初探</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmsegmentation%E5%88%9D%E6%8E%A2/</link>
      <pubDate>Tue, 15 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/mmsegmentation%E5%88%9D%E6%8E%A2/</guid>
      <description>MMSegmentation环境配置 安装Pytorch 这部分比较常规，就不细讲了，一般来说就是安装虚拟环境再安装pytorch的cuda训练包，这里要求的环境是Pytorch 1.10.0 + CUDA 11.3。
使用MIM安装MMCV mmcv是MMsegmentation的基础，也是商汤框架所有算法库的母框架，这里我们使用MIM来安装MMCV：
1 2 3 $ pip install -U openmim $ mim install mmengine $ mim install &amp;#39;mmcv==2.0.0&amp;#39; 安装其他工具包 这些工具包都是图像处理常用的包，可以自行选择。
1 pip install opencv-python pillow matplotlib seaborn tqdm &amp;#39;MMdet&amp;gt;=3.1.0&amp;#39; -i https://pypi.tuna.tsinghua.edu.cn/simple 这里需要下载MMdet是因为后面可能部分语义分割算法依赖于MMdet
安装MMSegmentation 这里推荐使用源码安装的方式，这样也可以参考部分源码获取信息：
下载源码： 1 git clone https://github.com/open-mmlab/mmsegmentation.git -b v1.1.1 -b代表该仓库的分支下载！
进入主目录： 1 cd mmsegmentation 源码安装： 1 pip install -v -e . 检查安装是否成功 检查Pytorch: 1 2 3 4 import torch, torchvision print(&amp;#34;Pytorch 版本&amp;#34;, torch.</description>
    </item>
    
    <item>
      <title>Jetson Nano算法部署实战</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/jetson-nano%E7%AE%97%E6%B3%95%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98/</link>
      <pubDate>Wed, 31 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/jetson-nano%E7%AE%97%E6%B3%95%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98/</guid>
      <description>Jetson Nano算法部署实战 硬件和环境准备 Jetson Nano B01开发板 CSI摄像头模块 Wifi模块 我选择的是亚博智能的wife进阶套餐开发套件，TF卡自带镜像，自带一些所需的Package:
deepstream-app version: 6.0.1 DeepStream SDK: 6.0.1 JetPack: 4.6 Python: 3.6.9 CUDA Driver version: 10.2 CUDA Runtime version: 10.2 TensorRT version: 8.2 cuDNN version: 8.2 ONNXRuntime-gpu: 1.6.0(自行下载) ONNXRuntime-gpu的下载：Jetson Nano为arm64架构，ONNXRuntime-gpu不能直接通过pip下载，需要手动编译。好在官方已经帮我们完成了，需要根据Jetpack版本和Python版本进行选择！ 下载地址
下载完成之后，打开Terminal,进入下载地方的地址，使用pip安装：
1 $ pip3 install https://nvidia.box.com/shared/static/49fzcqa1g4oblwxr3ikmuvhuaprqyxb7.whl 连接工具 我们需要在Jetson Nano内部写代码，需要使用较为方便的编辑器，我这里选择的是vscode远程连接Jetson Nano。
vscode远程配置连接：
首先在vscode中添加扩展Remote - SSH 启动Jetson Nano，并连接wifi，打开Terminal输入ifconfig，将最下方的ip地址记下。 在PC端的Remote - SSH连接到刚刚的IP地址，并输入Jetson Nano账户和密码（亚博智能的为 账户：Jetson 密码：yahboom）需要注意的是PC和Jetson Nano必须连接到同一个wifi。 在vscode端为jetson nano内部配置Python扩展。 文件传输工具，因为我使用的是MAC端，所以我使用Transmit工具，连接方式和vscode连接是一样的。 算法准备 关于算法你需要的文件就只有一个ONNX文件，因为ONNX既包含了模型的权值参数也包含了计算图。我在这里使用的算法是我开发的脐橙缺陷检测分割算法FastSegFormer-P。
ONNX文件导出（需要pth文件）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #--*-- coding:utf-8 --*-- import torch from models.</description>
    </item>
    
    <item>
      <title>NVIDIA官方教程：第一节</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B_1/</link>
      <pubDate>Wed, 17 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B_1/</guid>
      <description>TensorRT简介 TensorRT是用于高效实现已经训练好的深度学习模型的推理过程的SDK。 TensorRT内含推理优化器和运行时环境。 TensorRT使Deep Learning模型能以更高的吞吐量和更低的延迟运行。 包含C++和python的API，完全等价可以混用。 一些reference： TensorRT文档:https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html C++ API文档:https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.html python API文档:https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html TensorRT下载:https://developer.nvidia.com/nvidia-tensorrt-download 该教程配套代码:https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/cookbook
TensorRT基本特性 TensorRT的基本流程 示例代码
基本流程：
构建期： 建立Buider（构建引擎器） 创建Network（计算图内容） 生成SerializedNetwork（网络的TRT内部表示） 运行期： 建立Engine和Context Buffer相关准备（Host端 + Device端 + 拷贝操作） 执行推理（Execute） TensorRT工作流 使用框架自带的TRT接口(TF-TRT、Torch-TensorRT) 简单灵活、部署仍然在原框架中，无需书写插件。 使用Parser(TF/Torch/&amp;hellip; -&amp;gt; ONNX -&amp;gt; TensorRT) 流程成熟，ONNX通用性好，方便网络调整，兼顾性能效率 使用TensorRT原生API搭建网络 性能最优，精细网络控制，兼容性最好 使用TensorRT API搭建 下面是一个API完整搭建一个MNIST手写识别模型的示例： 示例代码
由于我没有学过Tensorflow，我也不会使用该框架去实现，未来应该只选择Parser的方式实现，这里只做了解。
基本流程：
1.Tensorflow中创建并训练一个网络 2.提取网络权重，保存为para.npz 3.TensorRT中重建该网络并加载para.npz的权重 4.生成推理引擎 5.用引擎做实际推理 用一张图来表示TensorRT使用的通用流程： 其中黄色部分文字是API创建方式特有的步骤，Parse将用onnx来代替。
构建阶段介绍 Logger日志记录器 1 2 3 # 可选参数：VERBOSE，INFO，WARNING，ERROR，INTERNAL_ERROR， # 产生不同等级的日志 ，由详细到简略。 logger = trt.Logger(trt.Logger.VERBOSE) Builder引擎构建器 1 2 3 4 # 常用成员：Builder.</description>
    </item>
    
    <item>
      <title>服务器深度学习环境</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Wed, 17 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</guid>
      <description>深度学习环境搭建 租用云GPU服务 这里我使用的是AutoDL，主要是价格比较便宜。
在租用GPU时，要注意所需项目的环境，特别是Pytorch和CUDA版本。服务器可以选择你需要的环境，如果没有你想要的环境，可以只选择Miniconda环境，选择一个使用的Unbantu和Python版本，然后自行下载需要的环境。
vscode连接服务器 打开vscode，下载Remote - SSH插件。 打开最左边的Remote - SSH模块，点击+，添加远程ssh登录指令，并输入AutoDL实例页面的登录指令，并按回车，再次按回车写入配置文件。 点击右下角的connect，然后输入AutoDL示例页面提供的密码，回车，成功连接。 在vscode的插件商店中选择Python,并为服务器端下载插件。 在vscode中按下command+shift+p，输入python，选择Python: Select Interpreter，选择服务器中自动配置好的base环境。如果在配置实例时只选择了Miniconda，可以重新建立一个虚拟环境，安装Pytorch大礼包。 配置服务器免密登录 这里我使用的是Mac操作系统进行配置，Windows具体流程相似。
操作流程：
在本机创建新的ssh key: 1 2 $ cd ~/.ssh $ ssh-keygen -t rsa -C &amp;#34;ssh key的注释&amp;#34; 此时在.ssh目录下就会生成两个文件id_rsa和id_rsa.pub，分别是私钥和公钥。
复制公钥的内容到服务器端/root/.ssh目录下的authorized_keys（有时也是authorized_keys2）: 1 2 $ cat ~/.ssh/id_rsa.pub $ # 然后选中复制，并粘贴到服务器端的目标位置 配置完成之后，本地进入服务器端就不需要一直输ssh密码了。 上传文件 如果我们的文件可以在Github中找到，直接使用Terminal进行克隆： 1 $ git clone 你的repo的https地址 如果文件在本机，可以直接将文件拖拽到vscode目录下，也可以通过实例进入JupyterLab上传文件。 谷歌Colab白嫖GPU资源 首先登录Colab官网，点击新建笔记本，点击代码执行程序，点击下拉列表里的更改运行时类型，选择GPU，保存。然后就可以开心地白嫖了！
修改Linux服务器文件权限问题 将文件设置为可读写执行权限： 1 $ chmod 777 file 给文件所有者增加写权限： 1 $ chmod u+w file 给文件所有者和同组用户赋予读写权限，其他用户只有读权限： 1 $ chmod 664 file 递归修改目录及其子目录中的文件权限： 1 $ chmod -R 755 directory 显示修改后的权限信息： 1 $ chmod -v 755 file 请注意，修改文件或目录的权限需要有足够的权限进行操作。只有文件或目录的所有者或超级用户(root)才能更改权限。</description>
    </item>
    
    <item>
      <title>TensorRT动态Batch和动态宽高的实现</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/%E5%8A%A8%E6%80%81batch%E5%92%8C%E5%AE%BD%E9%AB%98/</link>
      <pubDate>Tue, 16 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/%E5%8A%A8%E6%80%81batch%E5%92%8C%E5%AE%BD%E9%AB%98/</guid>
      <description>TensorRT之动态Batch和动态宽高 动态Batch 该特性的需求主要源于TensorRT编译时对batch的处理，若静态batch则意味着无论你有多少图，都按照固定大小batch推理。耗时是固定的。
实现动态Batch的注意点：
1.onnx导出模型是，注意view操作不能固定batch维度数值，通常写-1。 2.onnx导出模型是，通常可以指定dynamic_axes（通常用于指定动态维度），实际上不指定也没关系。
动态宽高 该特性需求来自onnx导出时指定的宽高是固定的，TensorRT编译时也需要固定大小引擎，若你想得到另外一个不同大小的TensorRT引擎（一个eng模型只能支持一个输入分辨率）时，就需要动态宽高的存在。而直接使用TensorRT的动态宽高（一个eng模型能支持不同输入分辨率的推理）会带来不必要的复杂度，所以使用中间方案：在编译时修改onnx输入实现相对动态（一个onnx模型，修改参数可以得到不同输入分辨率大小的eng模型），避免重回Pytorch再做导出。
实现动态宽高的注意点：
1.不建议使用dynamic_axes指定Batch以外的维度为动态，这样带来的复杂度太高，并且存在有的layer不支持。 2.如果onnx文件已经导出，但是输入的shape固定了，此时希望修改onnx的输入shape： 步骤一：使用TRT::compile函数的inputsDimsSetup参数重新定义输入的shape。 步骤二：使用TRT::set_layer_hook_reshape钩子动态修改reshape的参数实现适配。
动态Batch demo：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 int max_batch_size = 5; /** 模型编译，onnx到trtmodel **/ TRT::compile( TRT::Model::FP32, max_batch_size, //最大batch size &amp;#34;model_name.onnx&amp;#34;, &amp;#34;model_name.fp32.trtmodel&amp;#34; ); /** 加载编译好的引擎 **/ auto infer = TRT::load_infer(&amp;#34;model_name.fp32.trtmodel&amp;#34;); /** 设置输入的值 **/ /** 修改input的0维度为1，最大可以是5 **/ infer-&amp;gt;input(0)-&amp;gt;resize_single_dim(0, 2); infer-&amp;gt;input(0)-&amp;gt;set_to(1.0f); /** 引擎进行推理 **/ infer-&amp;gt;forward(); /** 取出引擎的输出并打印 **/ auto out = infer-&amp;gt;output(0); INFO(&amp;#34;out.</description>
    </item>
    
    <item>
      <title>TensorRT部署方案介绍</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</link>
      <pubDate>Thu, 11 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</guid>
      <description>TensorRT部署方案介绍 为每个模型写硬代码（c++） 仓库地址:https://github.com/wang-xinyu/tensorrtx
看图可知，也就是通过c++源代码直接调用TensorRT API，对每个不同的网络模型进行重写，再调用TensorRT Builder生成TensorRT Engine。
原理：
1.使用作者定义的gen_wts.py的存储权重。 2.使用C++硬代码调用TensorRT C++API构建模型，加载gen_wts.py产生的权重组成完整模型。
优点：
1.可以控制每个layer的细节和权重，直接面对TensorRT API。 2.这种方案不存在算子问题，如果存在不支持的算子可以自行增加插件。
缺点：
1.新模型需要对每个layer重写C++代码。 2.过于灵活，需要控制的细节多，技能要求很高。 3.部署时无法查看网络结构进行分析和排查。
为每个算子写Converter 仓库地址:https://github.com/NVIDIA-AI-IOT/torch2trt
原理：
1.作者为每一个算子（比如ReLU，Conv等），为每一个操醉的forward反射到自定义函数 2.通过反射torch的forward操作获取模块的权重，调用Python API接口实现模型
优点：
1.直接集成了Python、Pytorch，可以实现Pytorch模型到TensorRT模型的无缝转换。
缺点：
1.提供了Python的方案并没有提供c++的方案。 2.新的算子需要自己实现converter，需要维护新的算子库 3.直接用Pytorch赚到tensorRT存储的模型是TensorRT模型，如果跨设备必须在设备上安装pytoch，灵活度差，不利于部署。 4.部署时无法查看网络结构进行分析和排查。
基于ONNX路线提供C++和Python的接口 仓库地址:https://github.com/shouxieai/tensorRT_Pro
原理：
通过Pytorch官方和NVIDIA官方对torch-&amp;gt;onnx和onnx-&amp;gt;TRT算子库的支持。
优点：
1.集成工业级推理方案，支持TensorRT从模型导出到应用到项目中的全部工作 2.案例有YoloV5、YoloX、AlphaPose、RetinaFace、DeepSORT等，每个应用均为高性能工业级。 3.具有简单的模型导出方法和onnx问题的解决方案 4.具有简单的模型推理接口，封装tensorRT细节。支持插件。 5.依赖onnx，有两大官方进行维护。
缺点：
onnx存在各种兼容性问题。
如何正确导出onnx（避坑指南） 1.对于任何用到shape、size返回值的参数时，例如tensor.view(tensor.size(0), -1)，避免直接使用tensor.size的返回值，而是加上int转换，tensor.view(int(tensor.size(0)), -1)。（这里的tensor值的是一个具体的张量） 2.对于nn.Upsample或者nn.fucntional.interpolate函数，使用scale_factor指定倍率，而不是使用size参数指定大小。 3.对于reshape、view操作时，-1请指定到batch维度，其他维度计算出来即可。 4.torch.onnx.export指定dynamic_axes参数，并只指定batch维度，不指定其他维度。我们只需要动态batch，相对动态的宽高有其他方案。 这些做法的必要性体现在简化过程的复杂度，去掉gather、shape类的节点。
Example：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch import torch.</description>
    </item>
    
    <item>
      <title>推理框架ONNX Runtime</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/onnx%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Wed, 10 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/onnx%E9%83%A8%E7%BD%B2/</guid>
      <description>ONNX ONNX简介 目前我们熟知的Pytorch，Tensorflow和PaddlePaddle等深度学习框架是专门用于深度学习网络的框架。模型训练好之后会导出模型的权值文件，使用Pytorch导出 的文件一般以.pt或者.pth结尾的文件，他们可以在Pytorch框架上进行推理。根据训练和部署分离的原则，如果采用Pytorch框架进行训练，如何使用其他的框架进行 推理。这就需要使用万金油文件格式onnx。
两张图感受onnx的作用
可以看到使用了onnx中间格式后，极大地降低了部署的难度。
ONNX权值文件导出 在Pytorch训练完一个模型后，可以通过onnx将.pth和.pt文件转化为onnx格式。
首先需要先下载对应的Package:Pytorch，ONNX，ONNX Runtime：
1 2 3 4 5 6 # 安装Pytorch !pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 # 安装ONNX !pip install onnx -i https://pypi.tuna.tsinghua.edu.cn/simple # 安装ONNX Runtime(cpu) !pip install onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple 准备好训练完成的模型权值文件，进行ONNX导出，这里使用分割模型进行演示：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch import onnx # 定义要使用的设备 device = torch.</description>
    </item>
    
    <item>
      <title>分割网络模型轻量化技术</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E8%BD%BB%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Mon, 20 Mar 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E8%BD%BB%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/</guid>
      <description>分割模型量化技术 本文主要介绍的内容如下：
提高网络推理效率的基本技术 轻量化实时分割网络常用的架构 知识蒸馏 文中涉及的部分代码全部使用Pytorch框架实现。
本文的大部分内容来自于综述文章（On Efficient Real-Time Semantic Segmentation: A Survey）
提高网络推理效率的基本技术 本节将从以下几个方面来介绍：
采样技术 高效卷积技术 残差连接/跳过连接 轻量化骨干网络 采样技术 采样技术是减少推理延迟最常用的手段，采样分为上采样和下采样。
下采样可以用来降低图像的分辨率，在大型网络中广泛使用，来增加深层卷积核的接受场。通常在网络早期对图像进行下采样可以显著减少网络的推理延迟，在深层网络进行下采样也可以更好地提取高分辨率的细节。
常用的下采样方式有两种，一是使用最大池化层，二是使用步进卷积：
最大池化将图像分为若干个池化子区域，在每个区域中取最大的像素值。 步进卷积则通过调整步幅大小来调整图片的大小： 根据输入图像的大小$W\times W$，卷积核的大小$F\times F$，步长$S$，填充的数量$P$来计算输出图像的大小 $$W_{out} = \lvert\frac{W - F + 2P}{S}\rvert+1$$
1 2 3 4 5 6 import torch.nn as nn # 最大池化层(以下采样2倍为例) maxpooling = nn.MaxPool2d(kernel_size=2) # 步进卷积（以3*3卷积下采样2倍为例） conv_downsample = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0) 上采样的主要目的是为了重建输入分辨率的图像，上采样的方法主要有三种：
最近邻插值 双线性插值 转置卷积 从上到下计算代价越来越贵，采样效果也越来越好。
1 2 3 4 5 6 7 8 9 10 import torch import torch.</description>
    </item>
    
    <item>
      <title>VOC分割数据集制作</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/voc%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Thu, 02 Mar 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/voc%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%B6%E4%BD%9C/</guid>
      <description>VOC格式数据集制作 Labelme安装 一般来说，Labelme标图软件包比较通用，可以直接安装在anaconda的base环境，在Terminal或者anaconda prompt中输入：
1 2 3 4 5 6 7 8 # 注意前面三项安装为labelme的依赖项。 conda install -c conda-forge pyside2 conda install pyqt pip install pyqt5 pip install labelme 如果安装不流畅，或者因为网络原因下载不下来可以换源：
pip换源：加-i https://pypi.tuna.tsinghua.edu.cn/simple, 这里加的清华源 conda焕源： 1 2 3 4 5 6 7 # 加的都是中科大的镜像源 conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/ conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/ conda config --add channels https://mirrors.</description>
    </item>
    
    <item>
      <title>有限资源下的预训练方案</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%9C%89%E9%99%90%E8%B5%84%E6%BA%90%E4%B8%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88/</link>
      <pubDate>Thu, 13 Oct 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%9C%89%E9%99%90%E8%B5%84%E6%BA%90%E4%B8%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88/</guid>
      <description>其实有一个月没更新了！因为之前做的实验有了一丢丢成果，所以花了半个月时间写了一篇论文，试投了sci 4区的期刊，祝自己好运吧！
然后这是我新研究的方向就是swin transformer系列分割模型，先来做个最基本的工作！
有限资源下的预训练方案 最近在使用swin_Transformer的骨干网络模型做图像分割，但该骨干网络又十分依赖于预训练。
预训练的两种方案 第一种就是直接拿swin_Transformer的图像分类网络在ImageNet_1k和ImageNet_22k的数据集直接的预训练权重，直接载入你的模型和目标数据进行训练。（一般需要先锁权重让剩余的网络参数迭代一下，再解锁进行迭代）
第二种就是拿你组建好的图像分割模型在分割数据集（例如VOC2012BST，或者更大的ADE20k数据集）上进行预训练，然后将得到的权重加载到你的目标数据集上进行迭代。
swin_Transfomer的训练方案 首先先下载swin_Transformer的预训练权重，包括两个input_image_size(224$\times$224,384$\times$384)，4个model_size(Swin_T,Swin_S,Swin_B,Swin_L)都有在ImageNet1k和ImageNet22k的预训练权重。
预训练权重地址(在release的v1.0.0和v1.0.8):https://github.com/SwinTransformer/storage/releases/
首先构建分割网络，swin_TransUnet
Swin_transformer.py(对源码进行微调，加了3个输出特征图):
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 &amp;#34;&amp;#34;&amp;#34; Swin Transformer A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows` - https://arxiv.</description>
    </item>
    
    <item>
      <title>充分利用你的显存，智能调节学习率</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%E4%BD%A0%E7%9A%84%E6%98%BE%E5%AD%98/</link>
      <pubDate>Fri, 09 Sep 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%85%85%E5%88%86%E5%88%A9%E7%94%A8%E4%BD%A0%E7%9A%84%E6%98%BE%E5%AD%98/</guid>
      <description>一些新手比较容易踩的坑 batch_size 大小设置 先讲batch_size的功效：
增大batch_size的大小，可以加快训练速度 增大batch_size的大小，可以使得你使用的batch normalization更稳定，训练效果最好。 副作用：
batch_size越大，需要的显存就越大 相信所有人都经历过cuda out of memory报错，让人很心烦！那原因真的一定是batch_size过大吗？
第一种情况，是在验证的时候，没有加with torch.no_grad()，在验证的时候是不需要梯度反向传播的！ 第二种情况是确实模型太大，超显存了。 相信小伙伴都遇到过第三种情况：你设batch_size为2还报out of memory，任务管理器里明明显示还有很多显存，那就需要注意你的num_worker数量了。 如果你的num_worker比较大的话，cpu多线程读取图片的速度是快于你的GPU速度的，这时候会增加你的显存。这和你的GPU以及CPU的性能都是有关的，需要合理设置！ 学习率设置 学习率设置太大，容易让模型训练不动 batch_size调大之后，学习率是需要相应调大的 调参的时候通常使用大的学习率先开始训练，如果模型收敛不了再调更小 训练的时候一般采用学习率衰减的方法防止过拟合，一般使用step步进或者是模拟余弦退火算法来控制学习率大小。 </description>
    </item>
    
    <item>
      <title>图像分割之迁移学习</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B9%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 02 Sep 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B9%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</guid>
      <description>Unet，Attention_Unet网络修改之替换主干网络 对于算力有限的机器来说，从零开始训练实际效果并不好。使用迁移学习的预训练权重对于缺少参数调优的机器的炼丹人是非常重要的！
为了使用预训练权重，就需要将特征提取网络替换成知名的主干网络，比如VGG，Resnet，mobilenet等
Unet的修改——特征提取网络替换成VGG Unet论文地址：https://arxiv.org/abs/1505.04597
为了使替换主干网络后，保持维度匹配，需要对网络结构进行修改。所以我重构了网络结构图：
代码如下：
vgg.py:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import torch.</description>
    </item>
    
    <item>
      <title>图像分割的边角料</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E8%BE%B9%E8%A7%92%E6%96%99%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sun, 21 Aug 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E8%BE%B9%E8%A7%92%E6%96%99%E7%9F%A5%E8%AF%86/</guid>
      <description>图像分割中的上采样方法 反最大池化的方法 在下采样中，我们通常采用最大池化的方法来进行。那么对应在上采样中，反最大池化的方法其实就是记住最大池化时得到像素在原图中的位置，将其他位置填充为0。如图所示：
最早的SegNet所使用的上采样就是这种方式！
转置卷积 第二种方法就是转置卷积,这种方法和前面的反最大池化方法的最大区别就是转置卷积的参数是可以用于学习训练的，它不是一个固定的策略！
这个图可能看的不是很准确，想看动图的可以访问PyTorch官方给出的动图
以3$\times$3的卷积核将2$\times$2变为4$\times$4的图像为例（没有填充，没有步幅）：
1.第一步就是要将2$\times$2的图像使用零padding成为一个6$\times$6（4+2*1得到）的图像
2.对该填充后的图像做3$\times$3的卷积，得到输出图像
在深度学习的论文中，出现反卷积/跨步卷积/上卷积其实指的就是这种转置卷积。
放一张一维的图用于理解：
指标计算 基本指标 在图像分割中的基本指标通常包括Global Acc、mean Acc、MIOU,``等指标。
Global Acc(Pixel Acc) = $\frac{\sum_in_{ii}}{t_i}$
mean Acc = $\frac{1}{n_{cls}}\sum_i\frac{n_{ii}}{t_i}$
MIOU = $\frac{1}{n_{cls}}\sum_i\frac{n_{ii}}{t_i+\sum_jn_{ji}-n_{ii}}$
$n_{ij}$:类别i被预测成为类别j的像素个数
$n_{cls}$:目标类别个数（包含背景）
$t_i=\sum_jn_{ij}$:目标类别i的总像素个数（真实标签）
通过混淆矩阵理解指标 直接看下面三个图就可以理解了：
实操 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 &amp;#39;&amp;#39;&amp;#39; 混淆矩阵 Recall、Precision、MIOU计算 &amp;#39;&amp;#39;&amp;#39; import numpy as np from sklearn.</description>
    </item>
    
    <item>
      <title>图像分割入门</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</link>
      <pubDate>Wed, 10 Aug 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</guid>
      <description>图像分割 图像分割的分类 语义分割：可以将图片中不同类别的事物用不同的颜色分割出来，同一类的食物使用相同的颜色填充。 实例分割：在语义分割的基础上将同一类别的不同个体分割出来，使用不同的颜色填充。 语义分割如何做？ 语义分割是对图像中的每一个像素做语义判断，对逐个像素进行分类。
损失函数（c分类）： $$ loss=-\sum_{i=1}^c(w_cy\log(\hat{y}))\quad y代表真实值，\hat{y}代表预测值\ w_c=\frac{N-N_c}{N}\quad N代表总的像素个数，N_c代表类别为c的像素个数 $$
Focal loss： Focal loss用来解决难易不同的样本。在损失函数的基础上加入难易程度的奖励，识别准确率越高越容易，权重越低；识别准确率越低越难，权重越高。 $$ loss=-\sum_{i=1}^c(1-\hat{y})^{\gamma}y\log(\hat{y})\quad \gamma值一般取2 $$ 再将之前的样本比例权值$w_c$加上之后，损失函数为 $$ loss=-\sum_{i=1}^cw_c(1-\hat{y})^{\gamma}y\log(\hat{y})\quad $$
IOU计算 在多分类任务时：iou_dog = 正确预测为狗的数量/（真实的狗的数量 + 预测为狗的数量 - 正确预测为狗的数量） 在语义分割时：假设真实的图片标签为A，预测的图片为B，那么计算的公式：A和B的交集/A和B的并集。在多类别时，这个指标会变成MIOU：计算所有类别的平均值。 U-net系列 U-net U-net是一个典型的编码器解码器结构，编码器阶段进行特征提取和下采样（卷积和池化），解码器阶段进行上采样（插值）和特征拼接。模型结构如下：
缺点：进行特征拼接的时候，浅层特征离得比较远，上层拼接效果不佳。
U-net++ 首先看一下网络结构：
特点：在U-net的基础上，把能拼凑的特征全部都用上了。拼接的方式如上图，所有相邻的特征进行拼接。中间连线的部分是因为这部分的图像大小和输出图像大小一致，通过$1\times1$卷积将他们都作为预测的损失函数（也就是经常说的多输出）。
U-net++的优点：
可以更容易剪枝：比如我做到L4层的效果就很好，就可以将不需要的L5的上采样过程给去掉。
U-net+++ 直接看结构，也是在特征融合上下功夫：
DeepLab系列 感受野 假设一个$7\times7$的图片，通过2次$3\times3$的卷积操作，得到一个像素，那么这个像素的感受野就是$7\times7$的。那么感受野的概念其实就类似于当前计算出像素的值和前面原始图像的49个像素都有关。一个像素的感受野越大，那么它提取处的特征信息可能更加准确。
空洞卷积 为了增大感受野，我们通常的做法会采用下采样（也就是池化层），但是池化层有一个缺点是它会丢失一部分信息。
空洞卷积的提出为增大感受野提供了一种新的方法，空洞卷积的做法如下图：
空洞卷积的优势：
1.图像分割任务中需要较大的感受野来完成更好的任务
2.在Pytorch代码中之需要改变一个参数就可以了
3.扩大感受野时，参数的多少不不变的，计算代价没有变化
SPP层的作用 为了使卷积网络模型满足不同分辨率的要求，我们通常会使用spatial pyramid pooling layer达到这种效果。
在卷积网络中，卷积层通常不会对输入图像的分辨率由要求，但是在全连接层，会限制你图像输入的大小。SPP-Layer 的结构如下图：
不管输入的图像大小是多少，该层都把你分别16等分、4等分、分成1份，分别进行池化，再进行特征拼接。无论你的输入大小是多大，得到的结果都是一样的，这样就保障了我们的输出特征是固定的。
ASPP特征融合策略 ASPP其实是在SPP的策略基础上加入了不同倍率的空洞卷积。
其主要结构如下：
DeepLabV3+ 先看网络结构：
可以看到在特征融合层ASPP之前，有一个深度卷积网络（DCNN），这里一般选用市场上效果比较好的特征提取网络。整个网络是一种编码器和解码器的结构，解码器通过DCNN中提取的浅层特征（局部特征）和经过上采样的融合特征（考虑全局的特征）做拼接，再经过一次卷积核一次上采样（插值法）得到输出结果。注意：特征融合层采用不同倍率的空洞卷积来完成。</description>
    </item>
    
    <item>
      <title>深度学习笔记（10-11节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-11%E8%8A%82/</link>
      <pubDate>Fri, 29 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010-11%E8%8A%82/</guid>
      <description>深度学习（10-11节） 卷积操作 计算机视觉的例子：
图片分类 目标检测 神经网络实现图片转化迁移 但因为图片的像素高时，每张图片都很是一个很大的维度，如何处理大量的高像素图片呢？这可能就要用到卷积神经网络了。
边缘检测示例 我们通过边缘检测的例子来看卷积操作是如何进行的。
在边缘检测的例子里，卷积核从左到右每列为{1, 1, 1},{0, 0, 0},{-1, -1, -1}。
如图所示，左边是一个6$\times$6的矩阵，中间是一个3$\times$3的卷积核，”*“代表的是卷积操作，但在Python中一般代表乘积的操作，需要注意区分。将卷积核与左边颜色加深的矩阵一一对应相乘后再相加，得到右边绿色的数值。让卷积核在模板上进行移动卷积，可以得到一个4$\times$4的矩阵。假设原图像的维度为m$\times$n，卷积核的大小为a$\times$a，那么得到的矩阵大小为(m-a+1)$\times$(n-a+1)。
对应编程的函数：
python：conv_forward
tensorflow：tf.nn.conv2d
PyTorch：torch.nn.Conv2d
再举个更加明显的例子，如下图：
可以看到很明显的就是，如果左边的图片没有变化，与卷积核进行卷积就会得到0，如果选取的区域图片有变化，那么得到的结果就有正值。上述的过程看一看成如下示意图：
将最左边的变化的部分在最右边得到的结果显现出来。
更多的边缘检测内容 在上节例子中的卷积核可以为我们检测由亮到暗的一个过渡的过程，如果是由暗到亮，那么输出的就是负值。
如果我们把上节检测垂直边缘的卷积核旋转一下，从第一行到第三行分别是{1, 1, 1},{0, 0, 0},{-1, -1, -1}，这就变成了一个水平的边缘检测。
列举几个3$\times$3滤波器：
Sobel滤波器：第一列到第三列分别为：{1, 2, 1},{0, 0, 0},{-1, -2, -1}。 Scharr滤波器：第一列到第三列分别为：{3, 10, 3},{0, 0, 0},{-3, -10, -3}。 Padding 在进行卷积的过程中，会发现很容易将图像最边缘的部分给忽略掉。一般的做法是在进行卷积之前，会在周围再填充一层像素，这样m$\times$n的矩阵就变成了(m+2)$\times$(n+2)的矩阵，这样在每层神经网络里进行卷积，不会损失掉边缘的特征。
填充一层就代表padding=1，填充两层代表padding=2&amp;hellip;
Valid卷积，其卷积过程如下： $$ n\times n\quad*\quad f\times f\xrightarrow{no\quad padding}(n-f+1)\times (n-f+1) $$
Same卷积，顾名思义就是卷积前后的大小是相同的，其卷积过程如下： $$ n\times n\quad*\quad f\times f\xrightarrow[p=\frac{f-1}{2}]{padding=p}n\times n\ f=2p+1,所以我们的f只取奇数 $$
上述过程中所有填充的像素一般使用0来填充！</description>
    </item>
    
    <item>
      <title>PyTorch入门</title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/pytorch%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 28 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/pytorch%E5%85%A5%E9%97%A8/</guid>
      <description>PyTorch学习笔记 PyTorch安装及环境配置 conda环境安装 这里我们使用的anaconda，目前它也已经适配了apple芯片。所以我在我的M1 Pro的电脑上将miniforge3的方案替换成了anaconda3。官网：https://www.anaconda.com/
Windows端无需多说，苹果端如果是apple芯片，选择M1版本，如果是intel芯片，选择普通版本。
下载完成之后安装，需要配置一下环境变量（MacOS）。
首先进入根目录，并配置环境变量：
1 2 &amp;gt; cd ~ &amp;gt; vim ~/.zshrc 在配置文件中加入一行：
1 2 3 export PATH=&amp;#34;/Users/caixiongjiang/opt/anaconda3:$PATH&amp;#34; # 其中caixiongjiang是你的用户名 # vim中按i进行编辑模式，按Ese推出编辑模式，在普通模式下输入“:wq”表示保存并退出 启动环境配置，并配置国内镜像源：
1 2 3 4 5 6 7 8 &amp;gt; source ~/.zshrc &amp;gt; conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ &amp;gt; conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ &amp;gt; conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ &amp;gt; conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/ &amp;gt; conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/ &amp;gt; conda config --add channels https://mirrors.</description>
    </item>
    
    <item>
      <title>深度学习笔记（8-9节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-9%E8%8A%82/</link>
      <pubDate>Mon, 25 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-9%E8%8A%82/</guid>
      <description>深度学习（8-9节） 网络模型的优化 ML策略 当你采用一个分类器，得到的准确率为90%，如何提高它的准确率呢？以下有这么多的方法可以使用：
收集更多的数据 提高训练集的多样性（反例集） 尝试使用优化算法 使用规模更大或者更小的网络 使用正则化 修改网络结构 修改激活函数 改变隐藏层单元的数目 &amp;hellip; 修改的方法有很多，但是需要选择一个正确的并不容易。
正交化 如果将神经网络比作一个可调节的电视机，调整电视机的各种按钮来改变电视机的布局，色彩，亮度就如同在神经网络中调整各种超参数来查看神经网络的效果。
对于一个电视来说，我们需要慢慢调整（一个开关一个开关调整）才能将电视机调好，那么这种一个超参数调整，只能改变其神经网络的某个性质的形式就叫做正交化。
单一数字评估指标 理解一下两个概念，Precision和Recall。
Precison：中文为查准率，预测为真的模型中，有多少样本是真的，它的占比值。对于一个猫分类器来说就是，模型预测为猫的类型样本中，有多少占比为真正的猫。
Recall:中文为召回率，对于所有样本标签为真的情况下，有多少占比是你的模型正确预测出来的。对于一个猫分类器来说就是，在所有标签为真猫的样本中，有多少占比是你的模型正确预测的。
对于一个分类器来说，两个指标都同等重要，所以我们需要找到一个结合查准率和召回率的指标，也就是所谓的F1分数，公式如下： $$ F1=\frac{2}{\frac1p+\frac1R}\ 其中p代表查准率，R代表召回率，计算方式在数学上称为调和平均数 $$ 这样算出来的F1分数来判断网络的性能更加合理，称这种形式为单一数字评估指标。
再举一个例子，如下图：
算法在不同地区的错误率都不同，无法统一进行比较，那我们就将其整合为一个值，就是做平均之后再做比较。
满足和优化指标 对于某些多个指标的情况，我们想建立单一实数评估指标是非常困难的。比如，准确率和运行时间，想要合并为一个成本指标是很困难的，因为我们并不知道两个指标哪个更重要，或者说两个指标之间的关联性。所以我们可以采用满足和优化指标，在该例中就是在满足运行时间在多少毫秒之内，优化准确率，这样就可以找到最优的那个算法。
划分训练/开发/测试集 划分dev/test sets： 假设你要开发一个猫分类器，需要在不同区域进行运营：美国，英国，中国，印度，澳大利亚等。你如何设立开发集和测试集呢？
如果你在8个区域中随机选取4个区域作为开发集，4个区域作为测试集，效果可能是非常糟糕的。我们选取的原则是让开发集和测试集尽量来自同一分布。所以我们的做法一般是将所有数据随机洗牌，放入开发集和测试集，这样开发集和测试集都有来自8个区域的数据。
开发集和测试的大小： 在机器学习中，会有一条七三分的比例划分训练集和测试集，如果有开发集，则会按照6:2:2的比例来划分训练集，开发集，测试集。但在如今的数据量在百万级别的情况下，这样做会更加合理：98%作为训练集，1%开发集，%1测试集。
训练集，开发集和测试集的目的： 开发集是为了指定产品的优化目标，训练集决定了你向优化目标迭代的速度，测试集是为了评估投产系统的性能。
何时改变开发_测试集和指标 假设一个猫分类器，使用了算法A和算法B，它们都分别采用分类错误率来衡量算法的性能。算法A的错误率为3%，算法B的错误率为5%，但是算法A在错误分类的图片里，将色情图片分为了猫的图片，这是用户不能接受的。所以在这种情况下，即使你的算法在开发集上的指标更好，依然被认为是不好的产品。当你的指标错误地预测算法，就是你需要修改开发_测试集和指标的时候。
那么如何修改指标？
我们可能会对目标错误率设置一个$W$来赋予不同的图片以不同的权重： $$ W^{(i)}=\begin{cases} 1,&amp;amp;如果x^{(i)}不是色情图片\ 10,&amp;amp;如果x^{(i)}是色情图片 \end{cases} $$ 需要注意的是在目标函数上加上$W$，也需要修改归一化常数
贝叶斯误差和可避免偏差 假设一个两个不同的模型，在训练集和开发集上的错误率如下：
一个问题的错误率是有一个贝叶斯误差（理论最小误差），在大多数时候人类识别的错误率是接近贝叶斯误差的。
问题1的错误率 问题2 训练集 8% 8% 开发集 10% 10% 人类错误率相近 1% 7.5% 在训练集和人类错误率之间的差值被称为可避免偏差，训练集和开发集之间的误差被认为是需要更加优化方差。问题1，训练集与人类错误率相差较远，是训练欠拟合，可以加大训练网络或者迭代时间等来缩小；问题2中的训练集8%和测试集10%，略微过拟合，可以用正则化来降低拟合程度。
那么如果要把人类错误率作为贝叶斯错误的替代，该如何定义人类错误率呢？
假设一个医学图像分类的例子：
普通的人类在该任务上达到了3%的错误率。
普通的医生能达到1%的错误率</description>
    </item>
    
    <item>
      <title>深度学习笔记（6-7节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</link>
      <pubDate>Fri, 22 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</guid>
      <description>深度学习（6-7节） 优化算法 Mini-batch梯度下降法 假设我们的样本数量为500w个，那么在进行梯度下降之前，我们需要先将500w个数据整合成一个大的向量$X$。Mini-batch的做法为将500w个样本按照每个子集为1000个样本等分。每个子集标记为$X^{\left{ 1\right} }X^{\left{ 2\right} },\dots,X^{\left{ 5000\right} }$。相应的，除了需要拆分$X$，也需要拆分标签$Y$，拆分的方法和$X$相同。
Mini-batch的原理是将同时原本对所有样本和标签同时进行梯度下降转变为同时只对一个子集进行梯度下降处理，处理5000次。需要注意代价函数也要改变，因为每次训练的样本个数改变了。
当你的训练集大小很大的时候，mini-batch梯度下降法比batch梯度下降法运行地更快。
batch梯度下降法和Mini-batch梯度下降法的代价随迭代的图像如下：
右边的图像出现波动的原因是：每次实现梯度下降的样本集不同，可能$X^{\left{ 1\right} }$和$Y^{\left{ 1\right} }$需要花费的代价更大，而$X^{\left{ 2\right} }$和$Y^{\left{ 2\right} }$花费的代价更少，从而形成一个噪声的现象。
那么mini-bash的大小如何决定呢？
先看两种极端情况：
如果子集的大小为m，那么mini-bash梯度下降就变成了batch梯度下降；
如果子集的大小为1，那么mini-bash梯度下降就变成了随机梯度下降法，每个样本都是一个子集；
batch梯度下降每次下降的噪声会小一点，幅度会大一点（这里的噪声是指梯度下降的方向偏离目标）；而随机梯度下降大部分时间会向着全局最小值逼近，但有时候会远离最小值（刚好该样本是一个&amp;rsquo;&amp;lsquo;坏&amp;rsquo;&amp;lsquo;样本），随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。
batch梯度下降在训练数据很大的时候，单次训练迭代时间过长，如果训练数据量较小的情况下效果较好；而随机梯度下降单次迭代很快，但却无法使用向量化技术对运算进行加速。我们的目的就是选择一个不大不小的size，使得我们的学习速率达到最快（梯度下降）。
最优的情况就是，单次选取的size大小的数据分布比较符合整体数据的分布，这样使得学习速率和运行效率都比较高。
指数加权平均 指数加权平均也称指数加权移动平均，通过它可以来计算局部的平均值，来描述数值的变化趋势，下面通过一个温度的例子来详细介绍一下。
上图是温度随时间变化的图像，我们通过温度的局部平均值（移动平均值）来描述温度的变化趋势，计算公式如下： $$ v_t=\beta v_{t-1}+(1-\beta)\theta_{t}\ v_0=0\ v_1=0.9v_0+0.1\theta_1\ v_2=0.9v_1+0.1\theta_2\ \theta 代表当天的温度，v代表局部平均值 $$ 当$\beta$为0.9时，可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
当$\beta$变得越小，移动平均值的波动越大。
通过上面的公式往下推到，可以得到$v_{100}$的表达式： $$ v_{100}=0.1\times\theta_{100}+0.1\times0.9\times\theta_{99}+\dots+0.1\times0.9^{99}\times\theta_1\ =0.1\times\sum_{i=1}^{100}0.9^{100-i}\times\theta_i $$ 当$\epsilon=1-\beta$时，$(1-\beta)^{\frac{1}{\epsilon}}\approx\frac{1}{e}\approx\frac{1}{1-\beta}$，所以可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
简单来说，普通的加权求平均值的方法每一项的权重是$\frac{1}{n}$，指数加权平均每一项的权重是指数递减的。
指数加权平均的偏差修正 由于我们初始设置的$v_0$为0，这样会使前面几个$v_1,v_2\dots$的值与实际值相比偏小，我们通常会采取以下的办法来修正偏差： $$ v_t=\frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t} $$ 这样修正的效果为随着t的增加，分母越来越接近1。相当于时间越短，修正的幅度越大，所以这个公式主要是为了修正早期的偏差。
动量梯度下降法 我们将上面所说的指数加权平均的做法应用于神经网络的反向传播过程，如下： $$ V_{dW}=\beta V_{dW}+(1-\beta)dW\ V_{db}=\beta V_{db}+(1-\beta)db\ W:=W-\alpha V_{dW},b:=b-\alpha V_{db} $$ 这样做可以减缓梯度下降的幅度，因为梯度下降不一定朝着最快的方向前进。如下图所示：
原本为蓝色的梯度下降会变成红色，纵轴摆动的方向变小了且上下摆动的幅度均值大概为0。这样一来，即使我增加学习率或者步长也不会出现紫色线这种偏离函数的情况。</description>
    </item>
    
    <item>
      <title>深度学习笔记（4-5节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</link>
      <pubDate>Tue, 19 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</guid>
      <description>深度学习（4-5节） 多层的深层神经网络 神经网络的表示 1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4
2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量
3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$
深层网络中的前向传播 神经网络中每层的前向传播过程： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]})\ l代表层数 $$ 如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。
核对矩阵中的维数 如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。
通过一个具体的网络来手动计算一下维度：
可以写出该网络的部分参数如下： $$ n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1 $$ 由于前向传播的公式为： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]}) $$
需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。
1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。
2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$
所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。
可以总结出来的是： $$ W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\ b^{[l]}的维度一定是n^{[l]}\times 1 $$ 同理在反向传播时： $$ dW和W的维度必须保持一致，db必须和b保持一致 $$ 因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。
参数vs超参数 参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$
超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。
作业三 一个神经网络工作原理的模型如下：
多层网络模型的前向传播和后向传播过程如下：
实现一个L层神经网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward import numpy as np # ------------------------------------------ def initialize_parameters_deep(layer_dims): &amp;#34;&amp;#34;&amp;#34; Arguments: layer_dims -- 包含网络中每一层的维度的Python List Returns: parameters -- Python参数字典 &amp;#34;W1&amp;#34;, &amp;#34;b1&amp;#34;, .</description>
    </item>
    
    <item>
      <title>深度学习笔记（1-3节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</link>
      <pubDate>Tue, 12 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</guid>
      <description>深度学习（1-3节） 深度学习介绍 线性整流函数（ReLU函数） 通常意义下，线性整流函数指代数学中的斜坡函数，即 $$ f(x)=max(0,x) $$ 而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出 $$ max(0,W^Tx+b) $$ 到下一层神经元或作为整个神经网络的输出。
神经网络介绍 神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。
监督学习 监督学习也称为带标签的学习方式。监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。
结构化数据vs非结构化数据 结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。
深度学习的准确率 取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。
Sigmoid函数 sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：
优点：平滑、易于求导。
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。
Sigmoid函数的公式如下： $$ S(x)=\frac{1}{1+e^{-x}} $$ 函数图形如下：
深度学习基础 为了方便学习：
1.使用$(x,y)$来表示一个单独的样本
2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in {0,1}$代表标签$y$值为0或1
3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
4.$m=m_{train}+m_{test}$
5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&amp;amp;|&amp;amp;&amp;amp;|\ x^{\left( 1\right) }&amp;amp;x^{\left( 2\right) }&amp;amp;\cdots &amp;amp;x^{\left( m\right) }\ |&amp;amp;|&amp;amp;&amp;amp;|\end{matrix} \right] $，$m$是训练集样本的个数。
6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right) }&amp;amp;y^{\left( 2\right) }&amp;amp;\cdots &amp;amp;y^{\left( m\right) }\end{matrix} \right] $,$Y\in\R^{1\times m}$
Logistic回归 Logistic回归通常用于二元分类问题。
它通常的做法是将sigmoid函数作用于线性回归： $$ \hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}} $$ 这会使得$\hat{y}$的范围在0~1之间</description>
    </item>
    
  </channel>
</rss>
