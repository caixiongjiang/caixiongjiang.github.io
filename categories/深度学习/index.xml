<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 25 Jul 2022 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习笔记（8-9节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-9%E8%8A%82/</link>
      <pubDate>Mon, 25 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08-9%E8%8A%82/</guid>
      <description>深度学习（8-9节） 网络模型的优化 ML策略 当你采用一个分类器，得到的准确率为90%，如何提高它的准确率呢？以下有这么多的方法可以使用：
收集更多的数据 提高训练集的多样性（反例集） 尝试使用优化算法 使用规模更大或者更小的网络 使用正则化 修改网络结构 修改激活函数 改变隐藏层单元的数目 &amp;hellip; 修改的方法有很多，但是需要选择一个正确的并不容易。
正交化 如果将神经网络比作一个可调节的电视机，调整电视机的各种按钮来改变电视机的布局，色彩，亮度就如同在神经网络中调整各种超参数来查看神经网络的效果。
对于一个电视来说，我们需要慢慢调整（一个开关一个开关调整）才能将电视机调好，那么这种一个超参数调整，只能改变其神经网络的某个性质的形式就叫做正交化。
单一数字评估指标 理解一下两个概念，Precision和Recall。
Precison：中文为查准率，预测为真的模型中，有多少样本是真的，它的占比值。对于一个猫分类器来说就是，模型预测为猫的类型样本中，有多少占比为真正的猫。
Recall:中文为召回率，对于所有样本标签为真的情况下，有多少占比是你的模型正确预测出来的。对于一个猫分类器来说就是，在所有标签为真猫的样本中，有多少占比是你的模型正确预测的。
对于一个分类器来说，两个指标都同等重要，所以我们需要找到一个结合查准率和召回率的指标，也就是所谓的F1分数，公式如下： $$ F1=\frac{2}{\frac1p+\frac1R}\ 其中p代表查准率，R代表召回率，计算方式在数学上称为调和平均数 $$ 这样算出来的F1分数来判断网络的性能更加合理，称这种形式为单一数字评估指标。
再举一个例子，如下图：
算法在不同地区的错误率都不同，无法统一进行比较，那我们就将其整合为一个值，就是做平均之后再做比较。
满足和优化指标 对于某些多个指标的情况，我们想建立单一实数评估指标是非常困难的。比如，准确率和运行时间，想要合并为一个成本指标是很困难的，因为我们并不知道两个指标哪个更重要，或者说两个指标之间的关联性。所以我们可以采用满足和优化指标，在该例中就是在满足运行时间在多少毫秒之内，优化准确率，这样就可以找到最优的那个算法。
划分训练/开发/测试集 划分dev/test sets： 假设你要开发一个猫分类器，需要在不同区域进行运营：美国，英国，中国，印度，澳大利亚等。你如何设立开发集和测试集呢？
如果你在8个区域中随机选取4个区域作为开发集，4个区域作为测试集，效果可能是非常糟糕的。我们选取的原则是让开发集和测试集尽量来自同一分布。所以我们的做法一般是将所有数据随机洗牌，放入开发集和测试集，这样开发集和测试集都有来自8个区域的数据。
开发集和测试的大小： 在机器学习中，会有一条七三分的比例划分训练集和测试集，如果有开发集，则会按照6:2:2的比例来划分训练集，开发集，测试集。但在如今的数据量在百万级别的情况下，这样做会更加合理：98%作为训练集，1%开发集，%1测试集。
训练集，开发集和测试集的目的： 开发集是为了指定产品的优化目标，训练集决定了你向优化目标迭代的速度，测试集是为了评估投产系统的性能。
何时改变开发_测试集和指标 假设一个猫分类器，使用了算法A和算法B，它们都分别采用分类错误率来衡量算法的性能。算法A的错误率为3%，算法B的错误率为5%，但是算法A在错误分类的图片里，将色情图片分为了猫的图片，这是用户不能接受的。所以在这种情况下，即使你的算法在开发集上的指标更好，依然被认为是不好的产品。当你的指标错误地预测算法，就是你需要修改开发_测试集和指标的时候。
那么如何修改指标？
我们可能会对目标错误率设置一个$W$来赋予不同的图片以不同的权重： $$ W^{(i)}=\begin{cases} 1,&amp;amp;如果x^{(i)}不是色情图片\ 10,&amp;amp;如果x^{(i)}是色情图片 \end{cases} $$ 需要注意的是在目标函数上加上$W$，也需要修改归一化常数
贝叶斯误差和可避免偏差 假设一个两个不同的模型，在训练集和开发集上的错误率如下：
一个问题的错误率是有一个贝叶斯误差（理论最小误差），在大多数时候人类识别的错误率是接近贝叶斯误差的。
问题1的错误率 问题2 训练集 8% 8% 开发集 10% 10% 人类错误率相近 1% 7.5% 在训练集和人类错误率之间的差值被称为可避免偏差，训练集和开发集之间的误差被认为是需要更加优化方差。问题1，训练集与人类错误率相差较远，是训练欠拟合，可以加大训练网络或者迭代时间等来缩小；问题2中的训练集8%和测试集10%，略微过拟合，可以用正则化来降低拟合程度。
那么如果要把人类错误率作为贝叶斯错误的替代，该如何定义人类错误率呢？
假设一个医学图像分类的例子：
普通的人类在该任务上达到了3%的错误率。
普通的医生能达到1%的错误率</description>
    </item>
    
    <item>
      <title>深度学习笔记（6-7节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</link>
      <pubDate>Fri, 22 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</guid>
      <description>深度学习（6-7节） 优化算法 Mini-batch梯度下降法 假设我们的样本数量为500w个，那么在进行梯度下降之前，我们需要先将500w个数据整合成一个大的向量$X$。Mini-batch的做法为将500w个样本按照每个子集为1000个样本等分。每个子集标记为$X^{\left{ 1\right} }X^{\left{ 2\right} },\dots,X^{\left{ 5000\right} }$。相应的，除了需要拆分$X$，也需要拆分标签$Y$，拆分的方法和$X$相同。
Mini-batch的原理是将同时原本对所有样本和标签同时进行梯度下降转变为同时只对一个子集进行梯度下降处理，处理5000次。需要注意代价函数也要改变，因为每次训练的样本个数改变了。
当你的训练集大小很大的时候，mini-batch梯度下降法比batch梯度下降法运行地更快。
batch梯度下降法和Mini-batch梯度下降法的代价随迭代的图像如下：
右边的图像出现波动的原因是：每次实现梯度下降的样本集不同，可能$X^{\left{ 1\right} }$和$Y^{\left{ 1\right} }$需要花费的代价更大，而$X^{\left{ 2\right} }$和$Y^{\left{ 2\right} }$花费的代价更少，从而形成一个噪声的现象。
那么mini-bash的大小如何决定呢？
先看两种极端情况：
如果子集的大小为m，那么mini-bash梯度下降就变成了batch梯度下降；
如果子集的大小为1，那么mini-bash梯度下降就变成了随机梯度下降法，每个样本都是一个子集；
batch梯度下降每次下降的噪声会小一点，幅度会大一点（这里的噪声是指梯度下降的方向偏离目标）；而随机梯度下降大部分时间会向着全局最小值逼近，但有时候会远离最小值（刚好该样本是一个&amp;rsquo;&amp;lsquo;坏&amp;rsquo;&amp;lsquo;样本），随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。
batch梯度下降在训练数据很大的时候，单次训练迭代时间过长，如果训练数据量较小的情况下效果较好；而随机梯度下降单次迭代很快，但却无法使用向量化技术对运算进行加速。我们的目的就是选择一个不大不小的size，使得我们的学习速率达到最快（梯度下降）。
最优的情况就是，单次选取的size大小的数据分布比较符合整体数据的分布，这样使得学习速率和运行效率都比较高。
指数加权平均 指数加权平均也称指数加权移动平均，通过它可以来计算局部的平均值，来描述数值的变化趋势，下面通过一个温度的例子来详细介绍一下。
上图是温度随时间变化的图像，我们通过温度的局部平均值（移动平均值）来描述温度的变化趋势，计算公式如下： $$ v_t=\beta v_{t-1}+(1-\beta)\theta_{t}\ v_0=0\ v_1=0.9v_0+0.1\theta_1\ v_2=0.9v_1+0.1\theta_2\ \theta 代表当天的温度，v代表局部平均值 $$ 当$\beta$为0.9时，可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
当$\beta$变得越小，移动平均值的波动越大。
通过上面的公式往下推到，可以得到$v_{100}$的表达式： $$ v_{100}=0.1\times\theta_{100}+0.1\times0.9\times\theta_{99}+\dots+0.1\times0.9^{99}\times\theta_1\ =0.1\times\sum_{i=1}^{100}0.9^{100-i}\times\theta_i $$ 当$\epsilon=1-\beta$时，$(1-\beta)^{\frac{1}{\epsilon}}\approx\frac{1}{e}\approx\frac{1}{1-\beta}$，所以可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
简单来说，普通的加权求平均值的方法每一项的权重是$\frac{1}{n}$，指数加权平均每一项的权重是指数递减的。
指数加权平均的偏差修正 由于我们初始设置的$v_0$为0，这样会使前面几个$v_1,v_2\dots$的值与实际值相比偏小，我们通常会采取以下的办法来修正偏差： $$ v_t=\frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t} $$ 这样修正的效果为随着t的增加，分母越来越接近1。相当于时间越短，修正的幅度越大，所以这个公式主要是为了修正早期的偏差。
动量梯度下降法 我们将上面所说的指数加权平均的做法应用于神经网络的反向传播过程，如下： $$ V_{dW}=\beta V_{dW}+(1-\beta)dW\ V_{db}=\beta V_{db}+(1-\beta)db\ W:=W-\alpha V_{dW},b:=b-\alpha V_{db} $$ 这样做可以减缓梯度下降的幅度，因为梯度下降不一定朝着最快的方向前进。如下图所示：
原本为蓝色的梯度下降会变成红色，纵轴摆动的方向变小了且上下摆动的幅度均值大概为0。这样一来，即使我增加学习率或者步长也不会出现紫色线这种偏离函数的情况。</description>
    </item>
    
    <item>
      <title>深度学习笔记（4-5节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</link>
      <pubDate>Tue, 19 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</guid>
      <description>深度学习（4-5节） 多层的深层神经网络 神经网络的表示 1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4
2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量
3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$
深层网络中的前向传播 神经网络中每层的前向传播过程： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]})\ l代表层数 $$ 如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。
核对矩阵中的维数 如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。
通过一个具体的网络来手动计算一下维度：
可以写出该网络的部分参数如下： $$ n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1 $$ 由于前向传播的公式为： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]}) $$
需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。
1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。
2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$
所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。
可以总结出来的是： $$ W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\ b^{[l]}的维度一定是n^{[l]}\times 1 $$ 同理在反向传播时： $$ dW和W的维度必须保持一致，db必须和b保持一致 $$ 因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。
参数vs超参数 参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$
超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。
作业三 一个神经网络工作原理的模型如下：
多层网络模型的前向传播和后向传播过程如下：
实现一个L层神经网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward import numpy as np # ------------------------------------------ def initialize_parameters_deep(layer_dims): &amp;#34;&amp;#34;&amp;#34; Arguments: layer_dims -- 包含网络中每一层的维度的Python List Returns: parameters -- Python参数字典 &amp;#34;W1&amp;#34;, &amp;#34;b1&amp;#34;, .</description>
    </item>
    
    <item>
      <title>深度学习笔记（1-3节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</link>
      <pubDate>Tue, 12 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</guid>
      <description>深度学习（1-3节） 深度学习介绍 线性整流函数（ReLU函数） 通常意义下，线性整流函数指代数学中的斜坡函数，即 $$ f(x)=max(0,x) $$ 而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出 $$ max(0,W^Tx+b) $$ 到下一层神经元或作为整个神经网络的输出。
神经网络介绍 神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。
监督学习 监督学习也称为带标签的学习方式。监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。
结构化数据vs非结构化数据 结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。
深度学习的准确率 取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。
Sigmoid函数 sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：
优点：平滑、易于求导。
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。
Sigmoid函数的公式如下： $$ S(x)=\frac{1}{1+e^{-x}} $$ 函数图形如下：
深度学习基础 为了方便学习：
1.使用$(x,y)$来表示一个单独的样本
2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in {0,1}$代表标签$y$值为0或1
3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
4.$m=m_{train}+m_{test}$
5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&amp;amp;|&amp;amp;&amp;amp;|\ x^{\left( 1\right) }&amp;amp;x^{\left( 2\right) }&amp;amp;\cdots &amp;amp;x^{\left( m\right) }\ |&amp;amp;|&amp;amp;&amp;amp;|\end{matrix} \right] $，$m$是训练集样本的个数。
6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right) }&amp;amp;y^{\left( 2\right) }&amp;amp;\cdots &amp;amp;y^{\left( m\right) }\end{matrix} \right] $,$Y\in\R^{1\times m}$
Logistic回归 Logistic回归通常用于二元分类问题。
它通常的做法是将sigmoid函数作用于线性回归： $$ \hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}} $$ 这会使得$\hat{y}$的范围在0~1之间</description>
    </item>
    
  </channel>
</rss>
