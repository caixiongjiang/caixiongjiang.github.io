<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习&amp;计算机视觉 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
    <description>Recent content in 深度学习&amp;计算机视觉 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 12 Jul 2022 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习笔记（1-3节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</link>
      <pubDate>Tue, 12 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</guid>
      <description>深度学习（1-3节） 深度学习介绍 线性整流函数（ReLU函数） 通常意义下，线性整流函数指代数学中的斜坡函数，即 $$ f(x)=max(0,x) $$ 而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出 $$ max(0,W^Tx+b) $$ 到下一层神经元或作为整个神经网络的输出。
神经网络介绍 神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。
监督学习 监督学习也称为带标签的学习方式。监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。
结构化数据vs非结构化数据 结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。
深度学习的准确率 取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。
Sigmoid函数 sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：
优点：平滑、易于求导。
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。
Sigmoid函数的公式如下： $$ S(x)=\frac{1}{1+e^{-x}} $$ 函数图形如下：
深度学习基础 为了方便学习：
1.使用$(x,y)$来表示一个单独的样本
2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in {0,1}$代表标签$y$值为0或1
3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
4.$m=m_{train}+m_{test}$
5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&amp;amp;|&amp;amp;&amp;amp;|\ x^{\left( 1\right) }&amp;amp;x^{\left( 2\right) }&amp;amp;\cdots &amp;amp;x^{\left( m\right) }\ |&amp;amp;|&amp;amp;&amp;amp;|\end{matrix} \right] $，$m$是训练集样本的个数。
6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right) }&amp;amp;y^{\left( 2\right) }&amp;amp;\cdots &amp;amp;y^{\left( m\right) }\end{matrix} \right] $,$Y\in\R^{1\times m}$
Logistic回归 Logistic回归通常用于二元分类问题。
它通常的做法是将sigmoid函数作用于线性回归： $$ \hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}} $$ 这会使得$\hat{y}$的范围在0~1之间</description>
    </item>
    
  </channel>
</rss>
