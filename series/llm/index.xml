<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>《LLM》 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/llm/</link>
    <description>Recent content in 《LLM》 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 14 Jan 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LangChain:LLM应用框架</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sun, 14 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</guid>
      <description>LangChain：LLM应用框架 下面是一个简单场景（LangChain结合本地知识库对用户的提问进行回答）的整体流程：
本地知识库处理 首先需要加载本地知识库文件，将其转化为知识向量库。
文本嵌入模型：它本身用于将高维的稀疏数据（文本）转化为低维稠密数据。主流的嵌入模型有Word2Vec、GloVe、BERT、ERNIE、text2vec。 向量索引库：它在不同领域中用于存储和高效查询向量数据，主要应用于图像、音频、视频、自然语言等领域的相似性搜索任务。主流的库有faiss、pgvector、Milvus、pinecone、weaviate、LanceDB、Chroma等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from langchain.document_loaders import DirectoryLoader, unstructured from langchain.</description>
    </item>
    
    <item>
      <title>大模型高效微调技术（PEFT）</title>
      <link>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Tue, 24 Oct 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</guid>
      <description>大模型高效微调技术（PEFT） Adapter Tuning 技术原理 论文：Parameter-Efficient Transfer Learning for NLP 论文链接：http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf
该方法设计了一个Adapter结构，嵌入Transformer结构中。针对一个Transformer Block，增加两个Adapter结构，增加都放在残差结构之前。训练时，固定住原来的预训练模型参数不变，只对Adapter结构和Layer Normal层进行微调。Adapter层是一个类似于Squeeze-and-Excitation层思想的结构，首先使用较高维度的特征投影到较低维度的特征，中间通过一个非线性层，再将低维特征重新映射回原来的高维特征。其参数量主要低维特征的维度决定。
方法效果 对$BERT_{LARGE}$在各个任务上进行全量微调和Adapter-Tuning的结果如下，在不同的任务上，低维特征的维度m不同时效果不同。将m固定在64时，性能会略微下降。
Adapter通过引入额外的0.5%~5%参数可以媲美全量微调的效果，缺点是引入的参数在Transformer Block内部，在面对不同的任务时，需要保存不同的权重副本。
Adapter Fusion 技术原理 论文：AdapterFusion: Non-Destructive Task Composition for Transfer Learning
论文链接：https://arxiv.org/pdf/2005.00247.pdf
整合多个任务的知识，传统的两个方法是按一定顺序微调（Sequential fine-tuning）或者多任务学习（multi-task learning）。前者的问题是灾难性遗忘，后者的问题是不同的任务会相互影响，也难以平衡数据集差距很大的任务，对于新添加任务，需要进行完整的联合训练，这对于大成本的任务是不可取的。
作者在Adapter Tuning的启发下，考虑将多个任务的Adapter参数结合起来。作者提出了Adapter Fusion，这是一种两阶段学习方法，可以同时结合多个任务的知识。第一阶段知识提取，学习适配器的特定参数，这些参数封装了特定任务的信息；第二阶段知识组合，将所有的任务信息进行组合；按照这两步走，可以学习到多个任务中的表示，并且是非破坏性的。
Adapter Fusion根据了Adapter Tuning的优点和局限性，提出了两步训练的方法。
第一步：该步骤有两种方法，第一种是对每个任务进行单独微调，各个任务之间互不干扰；第二种方法是对所有任务进行多任务学习，联合优化。 第二步：为了避免特定任务接入的灾难性遗忘的问题。Adapter Fusion联合了第一个阶段的N个Adapter信息，新引入AdapterFusion结构的参数，目标函数也是学习针对特定任务m的AdapterFusion的参数。 AdapterFusion的结构：
AdapterFusion具体结构就是一个Attention，它的参数包括query，key, value的矩阵参数，在transformer的每一层都存在，它的query是transformer每个子模块的输出结果，它的key跟value则是N个任务的adapter的输出。通过AdapterFusion，模型可以为不同的任务对应的adapter分配不同的权重，聚合N个任务的信息，从而为特定任务输出更合适的结果。
方法效果 ST-A：单任务Adapter Tuning
MT-A：多任务Adapter Tuning 联合调优
F.w/ST-A：第一阶段使用ST-A，第二阶段使用AdapterFusion
F.w/MT-A：第一阶段使用MT-A，第二阶段使用AdapterFusion
可以看到，在第一阶段的只微调分类头的部分，效果并不好，ST-A微调大多数任务都能达到全量微调的水准，而MT-A在进行联合微调的时候发生了明显的任务不平衡的问题，这说明MT-A虽然可以学习到一个通用的表征，但是由于不同任务的差异性，很难保证在所有任务上都取得最优的效果。F.w/ST-A是最有效的方法，在多个数据集上的平均效果达到了最佳。而F.w/MT-A在于第一阶段其实已经联合了多个任务的信息了，所以AdapterFusion的作用没有那么明显，同时MT-A这种多任务联合训练的方式需要投入较多的成本，并不算一种高效的参数更新方式。
Adapter Drop 技术原理 论文：AdapterDrop: On the Efﬁciency of Adapters in Transformers
论文链接：https://arxiv.org/pdf/2010.11918.pdf
作者通过对Adapter的计算效率进行分析，发现与全量微调相比，Adapter在训练时快60%，但是在推理时慢4%-6%。
基于此，作者提出了AdapterDrop方法缓解该问题。
Adapter Drop 在不影响任务性能的情况下，对Adapter层进行动态删除，尽可能减少模型的参数量，提高模型在训练和特别是推理时的效率。
实验表明，从较低的 Transformer 层中删除Adapter可以显着提高多任务设置中的推理速度。 例如，将前五个Transformer层中的Adapter丢弃，在对 8 个任务进行推理时，速度提高了 39%。并且即使有多个丢弃层，AdapterDrop 也能保持良好的结果。</description>
    </item>
    
  </channel>
</rss>
