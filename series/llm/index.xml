<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>《LLM》 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/llm/</link>
    <description>Recent content in 《LLM》 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 18 Jan 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NLP &amp; LLM入门</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</link>
      <pubDate>Thu, 18 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</guid>
      <description>NLP &amp;amp; LLM入门 对于一个LLM菜鸡来说，从头理解LLM主要架构和任务是很有意义的。
LLM发展 双向RNN中的注意力 论文：Neural Machine Translation by Jointly Learning to Align and Translate
链接：https://arxiv.org/pdf/1409.0473.pdf
该论文引入了循环神经网络（RNN）的注意力机制。传统的 RNN 在处理较长序列时可能会遇到梯度消失或梯度爆炸等问题，导致远程位置的信息难以传递。注意力机制能够通过给予不同位置的输入不同的权重，使模型更好地捕捉到远程位置的信息，从而提高模型处理远程序列的能力。后续Transformer网络的开发也是为了提高网络的远程序列建模能力。
Transformer 论文：Attention Is All You Need
链接：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
无论是做NLP还是做CV，这篇论文大家应该都很熟悉。Transformer是一个编码器解码器结构，其引入了位置编码，使得模型能够直接看到全局的信息，更有利于序列的长距离建模。其主要的结构Transformer Block主要由一个多头自注意力和一个前向传播网络（FFN）组成，内部的归一化使用的是Layer Normal，这可以摆脱训练批次对模型训练性能的影响。
Post-LN vs Pre-LN Transformer 论文：On Layer Normalization in the Transformer Architecture
链接：https://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf
该论文提出Transformer结构中的Layer Normal结构的位置应该放置在多头注意力和FFN之前。该论文表明了Pre-LN的结构比先前的Post-LN效果更佳，解决了梯度问题，许多架构在实践中采用了这一点，但表示它有可能导致崩溃。关于使用Post-LN和Pre-LN的争论目前还在，可以关注后续的发展。
NLP中的训练范式 论文：Universal Language Model Fine-tuning for Text Classification
链接：https://arxiv.org/pdf/1801.06146.pdf
预训练微调的范式最早是在CV界广泛应用。早期NLP中的预训练微调的应用不广泛，大部分研究都是从头开始训练的，少数进行的预训练微调的效果并不好，比随机初始化的结果还差，或者需要很多的数据集作为预训练的语料库。该论文根据CV模型和NLP模型不同的架构，设计了三层完全相同的LSTM网络拼接的预训练网络。
上图所示即为该论文提出的语言模型的预训练微调的范式ULMFit，其主要分为三个阶段：
在大量文本语料库上训练语言模型 在特定任务数据上微调此预训练的语言模型，使其能适应文本的特定风格和词汇。 对特定任务数据的分类器进行微调，逐步解冻层，避免灾难性遗忘。 这个训练范式（在大型语料库上训练语言模型，然后在下游任务上进行微调）是后续基于Transformer的模型和基础模型（如BERT、GPT-2/3/4、RoBERTa等）中使用的中心方法。
然而，在使用Transformer架构时，逐渐解冻在实践中通常不会例行完成，所有层通常都同时进行微调。
BERT 论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</description>
    </item>
    
    <item>
      <title>LangChain:LLM应用框架</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sun, 14 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</guid>
      <description>LangChain：LLM应用框架 下面是一个简单场景（LangChain结合本地知识库对用户的提问进行回答）的整体流程：
本地知识库处理 首先需要加载本地知识库文件，将其转化为知识向量库。
文本嵌入模型：它本身用于将高维的稀疏数据（文本）转化为低维稠密数据。主流的嵌入模型有Word2Vec、GloVe、BERT、ERNIE、text2vec。 向量索引库：它在不同领域中用于存储和高效查询向量数据，主要应用于图像、音频、视频、自然语言等领域的相似性搜索任务。主流的库有faiss、pgvector、Milvus、pinecone、weaviate、LanceDB、Chroma等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from langchain.document_loaders import DirectoryLoader, unstructured from langchain.</description>
    </item>
    
    <item>
      <title>大模型高效微调技术（PEFT）</title>
      <link>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Tue, 24 Oct 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</guid>
      <description>大模型高效微调技术（PEFT） Adapter Tuning 技术原理 论文：Parameter-Efficient Transfer Learning for NLP 论文链接：http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf
该方法设计了一个Adapter结构，嵌入Transformer结构中。针对一个Transformer Block，增加两个Adapter结构，增加都放在残差结构之前。训练时，固定住原来的预训练模型参数不变，只对Adapter结构和Layer Normal层进行微调。Adapter层是一个类似于Squeeze-and-Excitation层思想的结构，首先使用较高维度的特征投影到较低维度的特征，中间通过一个非线性层，再将低维特征重新映射回原来的高维特征。其参数量主要低维特征的维度决定。
方法效果 对$BERT_{LARGE}$在各个任务上进行全量微调和Adapter-Tuning的结果如下，在不同的任务上，低维特征的维度m不同时效果不同。将m固定在64时，性能会略微下降。
Adapter通过引入额外的0.5%~5%参数可以媲美全量微调的效果，缺点是引入的参数在Transformer Block内部，在面对不同的任务时，需要保存不同的权重副本。
Adapter Fusion 技术原理 论文：AdapterFusion: Non-Destructive Task Composition for Transfer Learning
论文链接：https://arxiv.org/pdf/2005.00247.pdf
整合多个任务的知识，传统的两个方法是按一定顺序微调（Sequential fine-tuning）或者多任务学习（multi-task learning）。前者的问题是灾难性遗忘，后者的问题是不同的任务会相互影响，也难以平衡数据集差距很大的任务，对于新添加任务，需要进行完整的联合训练，这对于大成本的任务是不可取的。
作者在Adapter Tuning的启发下，考虑将多个任务的Adapter参数结合起来。作者提出了Adapter Fusion，这是一种两阶段学习方法，可以同时结合多个任务的知识。第一阶段知识提取，学习适配器的特定参数，这些参数封装了特定任务的信息；第二阶段知识组合，将所有的任务信息进行组合；按照这两步走，可以学习到多个任务中的表示，并且是非破坏性的。
Adapter Fusion根据了Adapter Tuning的优点和局限性，提出了两步训练的方法。
第一步：该步骤有两种方法，第一种是对每个任务进行单独微调，各个任务之间互不干扰；第二种方法是对所有任务进行多任务学习，联合优化。 第二步：为了避免特定任务接入的灾难性遗忘的问题。Adapter Fusion联合了第一个阶段的N个Adapter信息，新引入AdapterFusion结构的参数，目标函数也是学习针对特定任务m的AdapterFusion的参数。 AdapterFusion的结构：
AdapterFusion具体结构就是一个Attention，它的参数包括query，key, value的矩阵参数，在transformer的每一层都存在，它的query是transformer每个子模块的输出结果，它的key跟value则是N个任务的adapter的输出。通过AdapterFusion，模型可以为不同的任务对应的adapter分配不同的权重，聚合N个任务的信息，从而为特定任务输出更合适的结果。
方法效果 ST-A：单任务Adapter Tuning
MT-A：多任务Adapter Tuning 联合调优
F.w/ST-A：第一阶段使用ST-A，第二阶段使用AdapterFusion
F.w/MT-A：第一阶段使用MT-A，第二阶段使用AdapterFusion
可以看到，在第一阶段的只微调分类头的部分，效果并不好，ST-A微调大多数任务都能达到全量微调的水准，而MT-A在进行联合微调的时候发生了明显的任务不平衡的问题，这说明MT-A虽然可以学习到一个通用的表征，但是由于不同任务的差异性，很难保证在所有任务上都取得最优的效果。F.w/ST-A是最有效的方法，在多个数据集上的平均效果达到了最佳。而F.w/MT-A在于第一阶段其实已经联合了多个任务的信息了，所以AdapterFusion的作用没有那么明显，同时MT-A这种多任务联合训练的方式需要投入较多的成本，并不算一种高效的参数更新方式。
Adapter Drop 技术原理 论文：AdapterDrop: On the Efﬁciency of Adapters in Transformers
论文链接：https://arxiv.org/pdf/2010.11918.pdf
作者通过对Adapter的计算效率进行分析，发现与全量微调相比，Adapter在训练时快60%，但是在推理时慢4%-6%。
基于此，作者提出了AdapterDrop方法缓解该问题。
Adapter Drop 在不影响任务性能的情况下，对Adapter层进行动态删除，尽可能减少模型的参数量，提高模型在训练和特别是推理时的效率。
实验表明，从较低的 Transformer 层中删除Adapter可以显着提高多任务设置中的推理速度。 例如，将前五个Transformer层中的Adapter丢弃，在对 8 个任务进行推理时，速度提高了 39%。并且即使有多个丢弃层，AdapterDrop 也能保持良好的结果。</description>
    </item>
    
  </channel>
</rss>
