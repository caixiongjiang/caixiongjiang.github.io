<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>《LLM》 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/llm/</link>
    <description>Recent content in 《LLM》 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 06 May 2024 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM启动大全</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llm%E9%83%A8%E7%BD%B2%E5%A4%A7%E5%85%A8/</link>
      <pubDate>Mon, 06 May 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/llm%E9%83%A8%E7%BD%B2%E5%A4%A7%E5%85%A8/</guid>
      <description>LLM启动大全 大模型发展至今，已经诞生了各种各样的部署框架以及使用方式，他们通常适用于不同的场景。现在通过自己了解的知识对大模型的各种启动方式做一个总结。
模型下载 首先启动大模型，首先需要下载一个模型，模型的下载首选地址为HF官网模型库：https://huggingface.co/models
假设你的网络不能进入上述网址，可以进入魔搭社区下载，大部分模型都会同步到这里：https://www.modelscope.cn/models
我们这里使用Qwen1.5系列的模型做示例，具体的模型型号为Qwen1.5-4B-Chat。
模型启动 原生Transformers库启动 使用最原始的Transformer框架启动，需要写一小部分代码，官方已经为你写好了Demo，地址在https://www.modelscope.cn/models/qwen/Qwen1.5-4B-Chat/summary。
为了通用性，我对代码做了略微的修改：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 import torch from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer from threading import Thread import os from .</description>
    </item>
    
    <item>
      <title>vLLM如何提高并发？</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/vllm%E5%B9%B6%E5%8F%91/</link>
      <pubDate>Mon, 22 Apr 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/vllm%E5%B9%B6%E5%8F%91/</guid>
      <description>vLLM并发加速 论文：Efficient Memory Management for Large Language Model Serving with PagedAttention
链接：https://dl.acm.org/doi/pdf/10.1145/3600006.3613165
随着LLM模型能力的逐步提升，相关的业务应用也在井喷式增长。作为一个商业服务，必须承受高用户量的并发，vLLM的出现很好解决了这个需求。
LLM服务吞吐量的挑战 大模型服务有以下几个特点：
高吞吐量的服务需要批量处理足够多的请求。 kv缓存巨大，并且会动态增长和收缩。 KV cache
LLM权重的特点如下，大部分参数在服务期间保持静态，近30%的内存用于存储请求的动态状态，这些状态在模型结构中属于注意力机制中的key和value组成的张量组，通常被称为kv cache。
在标准的自注意力计算中，对于序列中的每个元素，模型都需要计算其与序列中所有其他元素的注意力权重，并根据这些权重聚合信息。这个过程涉及大量的计算，尤其是当序列很长时，计算量会呈平方级增长。KV缓存是一种优化方法，它通过缓存已经计算过的键（Key, K）和值（Value, V）对，来减少重复计算。具体来说，当模型处理序列中的下一个元素时，它只需要计算新的查询（Query, Q）与之前缓存的键值对之间的注意力权重，而不需要重新计算整个序列的键值对。这样可以显著减少计算量，提高处理长序列时的效率。
LLM的原理 LLM的生成原理为根据上文生成下文，学习大量数据的生成规律。在生成每一个token时，需要不断对前文（先前的序列）做注意力。对于每个请求，都会重复这个昂贵的过程，直到模型输出终止令牌。这种顺序生成过程使工作负载被内存绑定，没有充分利用GPU的计算能力，并限制了服务吞吐量。
现有深度学习框架的限制 大多数深度学习框架要求张量存储在连续的内存中。而KV 缓存随时间动态增长的特性使其在两个方面效率低下：
预先分配了具有请求最大长度的连续内存块（例如2048个令牌）。这可能会导致严重的内部碎片化，因为请求的实际长度可能比其最大长度短得多。此外，即使实际长度是先验的，预分配仍然效率低下：由于整个块在请求的生命周期内被保留，其他较短的请求无法利用当前未使用的块的任何部分。 现有的LLM使用的均为并行采样等高级算法，请求通常由多个序列组成,而连续存储决定了其不能共享权重。 LLM的批处理挑战 虽然批次足够大的时候，可以提高LLM的计算利用率（请求共享模型权重，能摊销）。 但是LLM的批次请求受两个点限制：
请求长度不一会产生等待导致的排队延迟， 对长度不一的序列使用填充策略平衡长度会造成很大的内存和计算浪费。 vLLM的作者沿用了先前提出的蜂窝批处理和迭代级调度。这些技术在迭代级别工作。每次迭代后，已完成的请求将从批处理中删除，并添加新的请求。因此，可以在等待单次迭代后处理新请求，而不是等待整个批次完成。此外，使用特殊的GPU内核，这些技术无需填充输入和输出。通过减少排队延迟和填充效率低下，细粒度的配料机制显著增加了LLM服务的吞吐量。
LLM中的内存挑战 从GPU制造的成本来说，计算能力的增长时很快的，但GPU的内存的增长却很缓慢。对于大模型的巨大参数量，内存已经成为主要的瓶颈。
缓存共享的挑战： 大模型的输入输出缓存主要分为两个部分，输入请求的提示词KV缓存（在作者的实验中占用12%的内存），它时可以共享的，可以减少内存使用量。另一部分是自动回归模型阶段生成的KV缓存，由于不同的样本结果对其上下文和位置的依赖，该阶段的KV缓存应保持不共享的状态。KV缓存共享的程度取决于特定的解码算法。
输入输出内存的调度： 调度未知的输入和输出长度。对LLM服务的请求在输入和输出长度上表现出可变性。这需要内存管理系统来适应广泛的提示长度。此外，随着请求的输出长度在解码时增加，其KV缓存所需的内存也会扩展，并可能耗尽传入请求的可用内存或现有提示的持续生成。系统需要做出调度决策，例如从GPU内存中删除或交换一些请求的KV缓存。
现有系统中的内存管理 现有深度学习框架（Pytorch等）的特点：
张量存储在连续内存中，一个请求的KV缓存被存储为跨越不同位置的连续张量。 LLM的输出长度不可预测，无论请求的实际输入或最终输出长度如何，他们根据请求的最大可能序列长度静态地为请求分配一块内存。 vLLM框架 作者开发了一个LLM的服务引擎vLLM应对一般深度学习框架推理LLM出现的挑战。vLLM采用集中式调度器来协调分布式GPU线程执行，KV缓存管理器通过PagedAttention启用，以分页方式有效管理KV缓存。
PageAttention PagedAttention允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV缓存划分为KV块，每个块包含固定数量令牌的密钥和值向量，将其表示为KV块大小（B）。
如下图：键（k）和值（v）向量分布在三个块上，这三个块在物理内存上不毗连。这中管理策略使得内存碎片较小，可以分配更大的需要连续内存的大对象，防止出现总内存足够，而因为内存碎片无法分配大的连续内存的情况。
总之，PagedAttention算法允许KV块存储在非连续的物理内存中，从而在vLLM中实现更灵活的分页内存管理。
KV缓存管理器 </description>
    </item>
    
    <item>
      <title>NLP &amp; LLM入门</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</link>
      <pubDate>Thu, 18 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/nlpllm/</guid>
      <description>NLP &amp;amp; LLM入门 对于一个LLM菜鸡来说，从头理解LLM主要架构和任务是很有意义的。
LLM发展 双向RNN中的注意力 论文：Neural Machine Translation by Jointly Learning to Align and Translate
链接：https://arxiv.org/pdf/1409.0473.pdf
该论文引入了循环神经网络（RNN）的注意力机制。传统的 RNN 在处理较长序列时可能会遇到梯度消失或梯度爆炸等问题，导致远程位置的信息难以传递。注意力机制能够通过给予不同位置的输入不同的权重，使模型更好地捕捉到远程位置的信息，从而提高模型处理远程序列的能力。后续Transformer网络的开发也是为了提高网络的远程序列建模能力。
Transformer 论文：Attention Is All You Need
链接：https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
无论是做NLP还是做CV，这篇论文大家应该都很熟悉。Transformer是一个编码器解码器结构，其引入了位置编码，使得模型能够直接看到全局的信息，更有利于序列的长距离建模。其主要的结构Transformer Block主要由一个多头自注意力和一个前向传播网络（FFN）组成，内部的归一化使用的是Layer Normal，这可以摆脱训练批次对模型训练性能的影响。
Post-LN vs Pre-LN Transformer 论文：On Layer Normalization in the Transformer Architecture
链接：https://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf
该论文提出Transformer结构中的Layer Normal结构的位置应该放置在多头注意力和FFN之前。该论文表明了Pre-LN的结构比先前的Post-LN效果更佳，解决了梯度问题，许多架构在实践中采用了这一点，但表示它有可能导致崩溃。关于使用Post-LN和Pre-LN的争论目前还在，可以关注后续的发展。
NLP中的训练范式 论文：Universal Language Model Fine-tuning for Text Classification
链接：https://arxiv.org/pdf/1801.06146.pdf
预训练微调的范式最早是在CV界广泛应用。早期NLP中的预训练微调的应用不广泛，大部分研究都是从头开始训练的，少数进行的预训练微调的效果并不好，比随机初始化的结果还差，或者需要很多的数据集作为预训练的语料库。该论文根据CV模型和NLP模型不同的架构，设计了三层完全相同的LSTM网络拼接的预训练网络。
上图所示即为该论文提出的语言模型的预训练微调的范式ULMFit，其主要分为三个阶段：
在大量文本语料库上训练语言模型 在特定任务数据上微调此预训练的语言模型，使其能适应文本的特定风格和词汇。 对特定任务数据的分类器进行微调，逐步解冻层，避免灾难性遗忘。 这个训练范式（在大型语料库上训练语言模型，然后在下游任务上进行微调）是后续基于Transformer的模型和基础模型（如BERT、GPT-2/3/4、RoBERTa等）中使用的中心方法。
然而，在使用Transformer架构时，逐渐解冻在实践中通常不会例行完成，所有层通常都同时进行微调。
BERT 论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</description>
    </item>
    
    <item>
      <title>LangChain:LLM应用框架</title>
      <link>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sun, 14 Jan 2024 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/langchain%E5%85%A5%E9%97%A8/</guid>
      <description>LangChain：LLM应用框架 下面是一个简单场景（LangChain结合本地知识库对用户的提问进行回答）的整体流程：
本地知识库处理 首先需要加载本地知识库文件，将其转化为知识向量库。
文本嵌入模型：它本身用于将高维的稀疏数据（文本）转化为低维稠密数据。主流的嵌入模型有Word2Vec、GloVe、BERT、ERNIE、text2vec。 向量索引库：它在不同领域中用于存储和高效查询向量数据，主要应用于图像、音频、视频、自然语言等领域的相似性搜索任务。主流的库有faiss、pgvector、Milvus、pinecone、weaviate、LanceDB、Chroma等。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from langchain.document_loaders import DirectoryLoader, unstructured from langchain.</description>
    </item>
    
    <item>
      <title>大模型高效微调技术（PEFT）</title>
      <link>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Tue, 24 Oct 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF/</guid>
      <description>大模型高效微调技术（PEFT） Adapter Tuning 技术原理 论文：Parameter-Efficient Transfer Learning for NLP 论文链接：http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf
该方法设计了一个Adapter结构，嵌入Transformer结构中。针对一个Transformer Block，增加两个Adapter结构，增加都放在残差结构之前。训练时，固定住原来的预训练模型参数不变，只对Adapter结构和Layer Normal层进行微调。Adapter层是一个类似于Squeeze-and-Excitation层思想的结构，首先使用较高维度的特征投影到较低维度的特征，中间通过一个非线性层，再将低维特征重新映射回原来的高维特征。其参数量主要低维特征的维度决定。
方法效果 对$BERT_{LARGE}$在各个任务上进行全量微调和Adapter-Tuning的结果如下，在不同的任务上，低维特征的维度m不同时效果不同。将m固定在64时，性能会略微下降。
Adapter通过引入额外的0.5%~5%参数可以媲美全量微调的效果，缺点是引入的参数在Transformer Block内部，在面对不同的任务时，需要保存不同的权重副本。
Adapter Fusion 技术原理 论文：AdapterFusion: Non-Destructive Task Composition for Transfer Learning
论文链接：https://arxiv.org/pdf/2005.00247.pdf
整合多个任务的知识，传统的两个方法是按一定顺序微调（Sequential fine-tuning）或者多任务学习（multi-task learning）。前者的问题是灾难性遗忘，后者的问题是不同的任务会相互影响，也难以平衡数据集差距很大的任务，对于新添加任务，需要进行完整的联合训练，这对于大成本的任务是不可取的。
作者在Adapter Tuning的启发下，考虑将多个任务的Adapter参数结合起来。作者提出了Adapter Fusion，这是一种两阶段学习方法，可以同时结合多个任务的知识。第一阶段知识提取，学习适配器的特定参数，这些参数封装了特定任务的信息；第二阶段知识组合，将所有的任务信息进行组合；按照这两步走，可以学习到多个任务中的表示，并且是非破坏性的。
Adapter Fusion根据了Adapter Tuning的优点和局限性，提出了两步训练的方法。
第一步：该步骤有两种方法，第一种是对每个任务进行单独微调，各个任务之间互不干扰；第二种方法是对所有任务进行多任务学习，联合优化。 第二步：为了避免特定任务接入的灾难性遗忘的问题。Adapter Fusion联合了第一个阶段的N个Adapter信息，新引入AdapterFusion结构的参数，目标函数也是学习针对特定任务m的AdapterFusion的参数。 AdapterFusion的结构：
AdapterFusion具体结构就是一个Attention，它的参数包括query，key, value的矩阵参数，在transformer的每一层都存在，它的query是transformer每个子模块的输出结果，它的key跟value则是N个任务的adapter的输出。通过AdapterFusion，模型可以为不同的任务对应的adapter分配不同的权重，聚合N个任务的信息，从而为特定任务输出更合适的结果。
方法效果 ST-A：单任务Adapter Tuning
MT-A：多任务Adapter Tuning 联合调优
F.w/ST-A：第一阶段使用ST-A，第二阶段使用AdapterFusion
F.w/MT-A：第一阶段使用MT-A，第二阶段使用AdapterFusion
可以看到，在第一阶段的只微调分类头的部分，效果并不好，ST-A微调大多数任务都能达到全量微调的水准，而MT-A在进行联合微调的时候发生了明显的任务不平衡的问题，这说明MT-A虽然可以学习到一个通用的表征，但是由于不同任务的差异性，很难保证在所有任务上都取得最优的效果。F.w/ST-A是最有效的方法，在多个数据集上的平均效果达到了最佳。而F.w/MT-A在于第一阶段其实已经联合了多个任务的信息了，所以AdapterFusion的作用没有那么明显，同时MT-A这种多任务联合训练的方式需要投入较多的成本，并不算一种高效的参数更新方式。
Adapter Drop 技术原理 论文：AdapterDrop: On the Efﬁciency of Adapters in Transformers
论文链接：https://arxiv.org/pdf/2010.11918.pdf
作者通过对Adapter的计算效率进行分析，发现与全量微调相比，Adapter在训练时快60%，但是在推理时慢4%-6%。
基于此，作者提出了AdapterDrop方法缓解该问题。
Adapter Drop 在不影响任务性能的情况下，对Adapter层进行动态删除，尽可能减少模型的参数量，提高模型在训练和特别是推理时的效率。
实验表明，从较低的 Transformer 层中删除Adapter可以显着提高多任务设置中的推理速度。 例如，将前五个Transformer层中的Adapter丢弃，在对 8 个任务进行推理时，速度提高了 39%。并且即使有多个丢弃层，AdapterDrop 也能保持良好的结果。</description>
    </item>
    
  </channel>
</rss>
