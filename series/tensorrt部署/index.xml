<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TensorRT部署 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/tensorrt%E9%83%A8%E7%BD%B2/</link>
    <description>Recent content in TensorRT部署 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 17 May 2023 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/tensorrt%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NVIDIA官方教程：第一节</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B_1/</link>
      <pubDate>Wed, 17 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E5%AE%98%E6%96%B9%E6%95%99%E7%A8%8B_1/</guid>
      <description>TensorRT简介 TensorRT是用于高效实现已经训练好的深度学习模型的推理过程的SDK。 TensorRT内含推理优化器和运行时环境。 TensorRT使Deep Learning模型能以更高的吞吐量和更低的延迟运行。 包含C++和python的API，完全等价可以混用。 一些reference： TensorRT文档:https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html C++ API文档:https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.html python API文档:https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html TensorRT下载:https://developer.nvidia.com/nvidia-tensorrt-download 该教程配套代码:https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/cookbook
TensorRT基本特性 TensorRT的基本流程 示例代码
基本流程：
构建期： 建立Buider（构建引擎器） 创建Network（计算图内容） 生成SerializedNetwork（网络的TRT内部表示） 运行期： 建立Engine和Context Buffer相关准备（Host端 + Device端 + 拷贝操作） 执行推理（Execute） TensorRT工作流 使用框架自带的TRT接口(TF-TRT、Torch-TensorRT) 简单灵活、部署仍然在原框架中，无需书写插件。 使用Parser(TF/Torch/&amp;hellip; -&amp;gt; ONNX -&amp;gt; TensorRT) 流程成熟，ONNX通用性好，方便网络调整，兼顾性能效率 使用TensorRT原生API搭建网络 性能最优，精细网络控制，兼容性最好 </description>
    </item>
    
    <item>
      <title>TensorRT动态Batch和动态宽高的实现</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/%E5%8A%A8%E6%80%81batch%E5%92%8C%E5%AE%BD%E9%AB%98/</link>
      <pubDate>Tue, 16 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/%E5%8A%A8%E6%80%81batch%E5%92%8C%E5%AE%BD%E9%AB%98/</guid>
      <description>TensorRT之动态Batch和动态宽高 动态Batch 该特性的需求主要源于TensorRT编译时对batch的处理，若静态batch则意味着无论你有多少图，都按照固定大小batch推理。耗时是固定的。
实现动态Batch的注意点：
1.onnx导出模型是，注意view操作不能固定batch维度数值，通常写-1。 2.onnx导出模型是，通常可以指定dynamic_axes（通常用于指定动态维度），实际上不指定也没关系。
动态宽高 该特性需求来自onnx导出时指定的宽高是固定的，TensorRT编译时也需要固定大小引擎，若你想得到另外一个不同大小的TensorRT引擎（一个eng模型只能支持一个输入分辨率）时，就需要动态宽高的存在。而直接使用TensorRT的动态宽高（一个eng模型能支持不同输入分辨率的推理）会带来不必要的复杂度，所以使用中间方案：在编译时修改onnx输入实现相对动态（一个onnx模型，修改参数可以得到不同输入分辨率大小的eng模型），避免重回Pytorch再做导出。
实现动态宽高的注意点：
1.不建议使用dynamic_axes指定Batch以外的维度为动态，这样带来的复杂度太高，并且存在有的layer不支持。 2.如果onnx文件已经导出，但是输入的shape固定了，此时希望修改onnx的输入shape： 步骤一：使用TRT::compile函数的inputsDimsSetup参数重新定义输入的shape。 步骤二：使用TRT::set_layer_hook_reshape钩子动态修改reshape的参数实现适配。
动态Batch demo：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 int max_batch_size = 5; /** 模型编译，onnx到trtmodel **/ TRT::compile( TRT::Model::FP32, max_batch_size, //最大batch size &amp;#34;model_name.onnx&amp;#34;, &amp;#34;model_name.fp32.trtmodel&amp;#34; ); /** 加载编译好的引擎 **/ auto infer = TRT::load_infer(&amp;#34;model_name.fp32.trtmodel&amp;#34;); /** 设置输入的值 **/ /** 修改input的0维度为1，最大可以是5 **/ infer-&amp;gt;input(0)-&amp;gt;resize_single_dim(0, 2); infer-&amp;gt;input(0)-&amp;gt;set_to(1.0f); /** 引擎进行推理 **/ infer-&amp;gt;forward(); /** 取出引擎的输出并打印 **/ auto out = infer-&amp;gt;output(0); INFO(&amp;#34;out.</description>
    </item>
    
    <item>
      <title>TensorRT部署方案介绍</title>
      <link>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</link>
      <pubDate>Thu, 11 May 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/tensorrt%E9%83%A8%E7%BD%B2/tensorrt%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/</guid>
      <description>TensorRT部署方案介绍 为每个模型写硬代码（c++） 仓库地址:https://github.com/wang-xinyu/tensorrtx
看图可知，也就是通过c++源代码直接调用TensorRT API，对每个不同的网络模型进行重写，再调用TensorRT Builder生成TensorRT Engine。
原理：
1.使用作者定义的gen_wts.py的存储权重。 2.使用C++硬代码调用TensorRT C++API构建模型，加载gen_wts.py产生的权重组成完整模型。
优点：
1.可以控制每个layer的细节和权重，直接面对TensorRT API。 2.这种方案不存在算子问题，如果存在不支持的算子可以自行增加插件。
缺点：
1.新模型需要对每个layer重写C++代码。 2.过于灵活，需要控制的细节多，技能要求很高。 3.部署时无法查看网络结构进行分析和排查。
为每个算子写Converter 仓库地址:https://github.com/NVIDIA-AI-IOT/torch2trt
原理：
1.作者为每一个算子（比如ReLU，Conv等），为每一个操醉的forward反射到自定义函数 2.通过反射torch的forward操作获取模块的权重，调用Python API接口实现模型
优点：
1.直接集成了Python、Pytorch，可以实现Pytorch模型到TensorRT模型的无缝转换。
缺点：
1.提供了Python的方案并没有提供c++的方案。 2.新的算子需要自己实现converter，需要维护新的算子库 3.直接用Pytorch赚到tensorRT存储的模型是TensorRT模型，如果跨设备必须在设备上安装pytoch，灵活度差，不利于部署。 4.部署时无法查看网络结构进行分析和排查。
基于ONNX路线提供C++和Python的接口 仓库地址:https://github.com/shouxieai/tensorRT_Pro
原理：
通过Pytorch官方和NVIDIA官方对torch-&amp;gt;onnx和onnx-&amp;gt;TRT算子库的支持。
优点：
1.集成工业级推理方案，支持TensorRT从模型导出到应用到项目中的全部工作 2.案例有YoloV5、YoloX、AlphaPose、RetinaFace、DeepSORT等，每个应用均为高性能工业级。 3.具有简单的模型导出方法和onnx问题的解决方案 4.具有简单的模型推理接口，封装tensorRT细节。支持插件。 5.依赖onnx，有两大官方进行维护。
缺点：
onnx存在各种兼容性问题。
如何正确导出onnx（避坑指南） 1.对于任何用到shape、size返回值的参数时，例如tensor.view(tensor.size(0), -1)，避免直接使用tensor.size的返回值，而是加上int转换，tensor.view(int(tensor.size(0)), -1)。（这里的tensor值的是一个具体的张量） 2.对于nn.Upsample或者nn.fucntional.interpolate函数，使用scale_factor指定倍率，而不是使用size参数指定大小。 3.对于reshape、view操作时，-1请指定到batch维度，其他维度计算出来即可。 4.torch.onnx.export指定dynamic_axes参数，并只指定batch维度，不指定其他维度。我们只需要动态batch，相对动态的宽高有其他方案。 这些做法的必要性体现在简化过程的复杂度，去掉gather、shape类的节点。
Example：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import torch import torch.</description>
    </item>
    
  </channel>
</rss>
