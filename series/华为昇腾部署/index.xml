<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>华为昇腾部署 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/</link>
    <description>Recent content in 华为昇腾部署 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 09 Aug 2023 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ATC模型转换工具</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/atc%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 09 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/atc%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7/</guid>
      <description>ATC模型转换工具 ATC模型转换工具聚焦于开源框架的网络模型（Caffe，TensorFlow，Pytorch）通过昇腾张量编译器ATC（Ascend Tensor Compiler）将其转换成昇腾AI处理器支持的离线模型。模型转换的过程中可以实现算子调度的优化、权重数据重排、内存使用优化等（自动），可以脱离设备完成模型的预处理。
ATC模型转换-快速入门 下图是整个ATC工具的功能架构：
以上支持Caffe，TensorFlow，ONNX，MindSpore四个框架转化为离线模型(.om)。
Caffe需要的文件：model.protoxt（caffe网络模型结构）、model.caffemodel（caffe网络模型权重） TensorFlow需要的文件：model.pb（包含TensorFlow模型的网络结构和权重） ONNX需要的文件（代表Pytorch框架）：model.pt、model.pth &amp;ndash;&amp;gt; model.onnx（包含ONNX模型的计算图和权重） MindSpore需要的文件（华为自家的框架）：model.air 需要注意的是MindSpore训练出来的模型文件有两种：.air和.mindir，其中.air经过转化之后可以用于离线推理，.mindir经过转化之后可以用于在线推理。
离线推理和在线推理的区别
在线推理是在AI框架内执行推理的场景，这通常只能用于验证结果。不利用在设备上进行迁移，因为它需要依赖厚重的AI框架，也没有对模型的推理速度进行优化。而离线模型不依赖于AI框架（需要推理框架，但通常在设备上会集成），可以直接在设备上进行推理。
在线推理类似于pytorch模型的推理，离线推理类似于NVIDIA的TensorRT的推理。
使用MindStudio进行模型转换
模型转化的过程已经在开发环境部署中介绍过了，不赘述。模型转换成功之后会在命令行提示的目录下生成.om离线模型文件和ModelConvert.txt模型转化日志文件。
ATC模型转换-高手进阶 如何在命令行窗口使用ATC工具？ 如何在模型转换的过程中加入一点动态输入形状信息？ 如何在模型转换过程中把一些预处理动作加入到模型中？ 命令行(CLI)调用ATC
在命令行窗口调用ATC工具，首先需要配置一些环境变量。这些环境变量在安装CANN-toolkit的时候已经展示过了，是第三组环境变量，需要把这些环境变量配置到~/.bashrc中，再source一下。
使用ATC工具的注意事项：
支持原始框架类型为Caffe、TensorFlow、MindSpore、ONNX的模型转换。
当原始框架为Caffe、MindSpore、ONNX时，输入数据类型为FP32，FP16（通过设置入参--input_fp16_nodes实现，MindSpore框架不支持该参数）、UINT8（通过配置数据预处理实现）。
当原始框架为TensorFlow时，输入数据类型为FP16、FP32、UINT8、INT32、BOOL（原始框架类型为TensorFlow时，不支持输入输出数据类型为INT64，需要用户自行将INT64修改为INT32类型）。
当原始框架为Caffe时，模型文件（.prototxt）和权重文件（.caffemodel）的输出名字、输出类型必须保持一致（名称包括大小写也要保持一致）。
当原始框架为TensorFlow时，只支持FrozenGraphDef（.pb）格式。
不支持动态shape的输入，模型转换时需要指定固定数值。
对于Caffe框架网络模型：输入数据最大支持四维，转维算子（reshape、expanddim等）不能输出五维。
模型中的所有层算子除了const算子外，输入和输出需要满足dim != 0。
只支持算子规格参考中的算子，并满足算子限制条件。
ATC模型转换命令举例：
CAFFE模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.prototxt&amp;#34; --weight=&amp;#34;/home/module/resnet50.caffemodel&amp;#34; --framework=0 --output=&amp;#34;/home/module/out/caffe_resnet50&amp;#34; --soc_version=Ascend310 TensorFlow模型转换: 1 $ atc --model=&amp;#34;/home/module/resnet_v1_50.pb&amp;#34; --framework=3 --output=&amp;#34;home/module/out/tf_resnet_v1_50&amp;#34; --soc_version=${soc_version} --input_shape=&amp;#34;input:1,224,224,3&amp;#34; ONNX模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.onnx&amp;#34; --framework=5 --output=&amp;#34;/home/module/out/onnx_resnet50&amp;#34; --soc_version=${soc_version} MindSpore模型转换： 1 $ atc --model=&amp;#34;/home/module/resnet50.air&amp;#34; --framework=1 --output=&amp;#34;/home/module/out/ResNet50_mindspore&amp;#34; --soc_version=${soc_version} 其中output参数代表的是输出离线模型的文件夹位置， soc_version代表要跑在哪一个昇腾AI处理器上，&amp;ldquo;&amp;ldquo;里面的input代表的是输入节点的名字。</description>
    </item>
    
    <item>
      <title>AscendCL快速入门</title>
      <link>https://caixiongjiang.github.io/blog/2023/hpc/ascendcl%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 08 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/hpc/ascendcl%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</guid>
      <description>AscendCL快速入门 ACL（Ascend Computing Language，昇腾计算语言），是华为提供的一套用于昇腾系列处理器上进行并行加速计算的API。基于这套的API，可以管理和使用昇腾软硬件计算资源，并进行机器学习相关计算。当前ACL提供了C/C++和Python的编程接口，这和TensorRT提供的接口一致。
鉴于之前使用TensorRT部署的语义分割模型使用的是Python接口，这次在昇腾处理器上准备使用C++进行处理。
ACL的主要功能 加载离线模型进行推理 加载单个算子做计算 对图形图像的数据进行预处理 使用ResNet50进行图片推理 准备工作：需要准备一张素材图片和一个转化好的ResNet50.om离线模型。
先用伪代码看一下main函数需要做什么：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int main() { char *picturePath = &amp;#34;./data/dog1_1024_683.bin&amp;#34;; char *modelPath = &amp;#34;./model/resnet50.om&amp;#34;; // 准备计算需要的资源	InitResource(); LoadModel(modelPath); LoadPicture(picturePath); inference(); PrintResult(); // 卸载模型 UnloadModel(); // 卸载图片 UnloadPicture(); // 销毁资源 DestroyResource(); } AscendCL接口函数 ACL的初始化和去初始化
在调用ACL的任何接口之前，首先要做ACL的初始化。初始化的代码如下：
aclInit(nullptr);: 这个接口调用会帮您准备好ACL的运行时环境。其中调用时传入的参数是一个配置文件在磁盘上的路径，这里暂时不需要关注。
aclFinalize();： 去初始化非常机简单，在确定完成了ACL的所有调用之后，要做去初始化操作，接口调用十分简单。
申请和释放计算资源
使用昇腾处理器提供的加速计算，首先在运行时申请计算资源。
aclrtSetDevice(0);： 这个接口会告诉运行时环境我们使用的设备，或者更具体一点在使用哪个芯片。但是需要注意的是，芯片和我们传入的编号之间并没有物理上的一一对应关系。
aclrtResetDevice(0);： 这里传入的设备编号和申请设备的时候使用的是同一个编号。调用这个接口会将对应设备上的所有计算资源进行复位。如果此时该设备上还有未完成的计算，则会等待该设备上的所有计算过程结束再复位设备。
加载数据</description>
    </item>
    
    <item>
      <title>昇腾计算开发环境配置</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Tue, 08 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid>
      <description>华为部署生态 CANN：华为昇腾的异构计算架构
Ascend：基于达芬奇架构的计算卡，昇腾310，昇腾910等。
MindSpore：基于昇腾架构研发的计算框架，类似于nvidia的TensorRT。
AscendCL：基于昇腾架构的计算语言，类似于nvidia的CUDA语言。
华为昇腾开发环境 首先有了硬件之后，需要先装驱动。驱动装完之后往上需要装用于推理的软件包，叫nnrt(Nerual Network RunTime)。这样我们就已经装完了运行的环境。为了我们开发方便，我们还需要在其之上装一个Toolkit。这些和nvidia都是非常像的，驱动 -&amp;gt; nvcc -&amp;gt; toolkit。
完成上述步骤，就可以在命令行编译运行我们的程序了。如果为了方便开发，还可以再安装一个MindStudio的软件。
驱动安装 首先关注一下华为昇腾社区的官网：https://www.hiascend.com
点击产品中的固件和驱动，选择加速卡的产品系列，由于此次公司的计算卡是昇腾910处理器，这里选择Atlas 300T Pro训练卡（型号：9000）。选择.run格式的驱动包，自行选择ARM和X86_64架构的包。需要注意的是这里选择的CANN版本与后面要安装的nnrt版本有关联。
CANN架构包下载 点击产品中的CANN，选择社区版下载，选择与驱动安装时相同的架构的选项，选择nnrt和toolkit的包进行下载。
nnrt是推理包，nnae是面向训练的包。
MindStudio下载 MindStudio提供您在AI开发所需的一站式开发环境，支持模型开发、算子开发以及应用开发三个主流程中的开发任务。 依靠模型可视化、算力测试、IDE本地仿真调试等功能，MindStudio能够帮助您在一个工具上就能高效便捷地完成AI应用开发 MindStudio采用了插件化扩展机制，开发者可以通过开发插件来扩展已有功能。
选择产品中的全流程开发工具链MindStudio，选择对应操作系统的软件包进行下载。
开发者文档 点击右上角的文档选项，进入昇腾文档，下滑，找到如下页面：
点击CANN软件安装，选择安装开发环境，按照文档进行对应系统的依赖。
然后按照安装驱动（包含重启）-&amp;gt;安装nnrt（包含配置环境变量）-&amp;gt;安装toolkit-&amp;gt;安装MindStudio：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ ./你的驱动安装包(.run包) --full # 重启之后进行nnrt安装 $ ./你的nnrt安装包(.run包) --install # 在.bashrc中配置一下环境变量（按照提示来），并source一下.bashrc $ ./你的toolkit安装包(.run包) --install # 这里会提示很多的环境变量配置（都是命令行开发所必须的） # 可以使用env.txt保存一下 $ vim env.txt # 复制并保存 # 环境变量分组：第一组是推理离线应用所需要的环境变量；第二组是推理在线应用所需要的环境变量；第三组是模型转换所需要的环境变量 $ tar zxvf .</description>
    </item>
    
  </channel>
</rss>
