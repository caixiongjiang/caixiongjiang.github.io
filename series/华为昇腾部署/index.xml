<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>华为昇腾部署 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/</link>
    <description>Recent content in 华为昇腾部署 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 08 Aug 2023 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AscendCL快速入门</title>
      <link>https://caixiongjiang.github.io/blog/2023/hpc/ascendcl%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 08 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/hpc/ascendcl%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</guid>
      <description>AscendCL快速入门 ACL（Ascend Computing Language，昇腾计算语言），是华为提供的一套用于昇腾系列处理器上进行并行加速计算的API。基于这套的API，可以管理和使用昇腾软硬件计算资源，并进行机器学习相关计算。当前ACL提供了C/C++和Python的编程接口，这和TensorRT提供的接口一致。
鉴于之前使用TensorRT部署的语义分割模型使用的是Python接口，这次在昇腾处理器上准备使用C++进行处理。
ACL的主要功能 加载离线模型进行推理 加载单个算子做计算 对图形图像的数据进行预处理 使用ResNet50进行图片推理 准备工作：需要准备一张素材图片和一个转化好的ResNet50.om离线模型。
先用伪代码看一下main函数需要做什么：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int main() { char *picturePath = &amp;#34;./data/dog1_1024_683.bin&amp;#34;; char *modelPath = &amp;#34;./model/resnet50.om&amp;#34;; // 准备计算需要的资源	InitResource(); LoadModel(modelPath); LoadPicture(picturePath); inference(); PrintResult(); // 卸载模型 UnloadModel(); // 卸载图片 UnloadPicture(); // 销毁资源 DestroyResource(); } AscendCL接口函数 ACL的初始化和去初始化
在调用ACL的任何接口之前，首先要做ACL的初始化。初始化的代码如下：
aclInit(nullptr);: 这个接口调用会帮您准备好ACL的运行时环境。其中调用时传入的参数是一个配置文件在磁盘上的路径，这里暂时不需要关注。
aclFinalize();： 去初始化非常机简单，在确定完成了ACL的所有调用之后，要做去初始化操作，接口调用十分简单。
申请和释放计算资源
使用昇腾处理器提供的加速计算，首先在运行时申请计算资源。
aclrtSetDevice(0);： 这个接口会告诉运行时环境我们使用的设备，或者更具体一点在使用哪个芯片。但是需要注意的是，芯片和我们传入的编号之间并没有物理上的一一对应关系。
aclrtResetDevice(0);： 这里传入的设备编号和申请设备的时候使用的是同一个编号。调用这个接口会将对应设备上的所有计算资源进行复位。如果此时该设备上还有未完成的计算，则会等待该设备上的所有计算过程结束再复位设备。
加载数据</description>
    </item>
    
    <item>
      <title>TensorRT部署方案介绍</title>
      <link>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Tue, 08 Aug 2023 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2023/%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE%E9%83%A8%E7%BD%B2/%E6%98%87%E8%85%BE%E8%AE%A1%E7%AE%97%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid>
      <description>华为部署生态 CANN：华为昇腾的异构计算架构
Ascend：基于达芬奇架构的计算卡，昇腾310，昇腾910等。
MindSpore：基于昇腾架构研发的计算框架，类似于nvidia的TensorRT。
AscendCL：基于昇腾架构的计算语言，类似于nvidia的CUDA语言。
华为昇腾开发环境 首先有了硬件之后，需要先装驱动。驱动装完之后往上需要装用于推理的软件包，叫nnrt(Nerual Network RunTime)。这样我们就已经装完了运行的环境。为了我们开发方便，我们还需要在其之上装一个Toolkit。这些和nvidia都是非常像的，驱动 -&amp;gt; nvcc -&amp;gt; toolkit。
完成上述步骤，就可以在命令行编译运行我们的程序了。如果为了方便开发，还可以再安装一个MindStudio的软件。
驱动安装 首先关注一下华为昇腾社区的官网：https://www.hiascend.com
点击产品中的固件和驱动，选择加速卡的产品系列，由于此次公司的计算卡是昇腾910处理器，这里选择Atlas 300T Pro训练卡（型号：9000）。选择.run格式的驱动包，自行选择ARM和X86_64架构的包。需要注意的是这里选择的CANN版本与后面要安装的nnrt版本有关联。
CANN架构包下载 点击产品中的CANN，选择社区版下载，选择与驱动安装时相同的架构的选项，选择nnrt和toolkit的包进行下载。
nnrt是推理包，nnae是面向训练的包。
MindStudio下载 MindStudio提供您在AI开发所需的一站式开发环境，支持模型开发、算子开发以及应用开发三个主流程中的开发任务。 依靠模型可视化、算力测试、IDE本地仿真调试等功能，MindStudio能够帮助您在一个工具上就能高效便捷地完成AI应用开发 MindStudio采用了插件化扩展机制，开发者可以通过开发插件来扩展已有功能。
选择产品中的全流程开发工具链MindStudio，选择对应操作系统的软件包进行下载。
开发者文档 点击右上角的文档选项，进入昇腾文档，下滑，找到如下页面：
点击CANN软件安装，选择安装开发环境，按照文档进行对应系统的依赖。
然后按照安装驱动（包含重启）-&amp;gt;安装nnrt（包含配置环境变量）-&amp;gt;安装toolkit-&amp;gt;安装MindStudio：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ ./你的驱动安装包(.run包) --full # 重启之后进行nnrt安装 $ ./你的nnrt安装包(.run包) --install # 在.bashrc中配置一下环境变量（按照提示来），并source一下.bashrc $ ./你的toolkit安装包(.run包) --install # 这里会提示很多的环境变量配置（都是命令行开发所必须的） # 可以使用env.txt保存一下 $ vim env.txt # 复制并保存 # 环境变量分组：第一组是推理离线应用所需要的环境变量；第二组是推理在线应用所需要的环境变量；第三组是模型转换所需要的环境变量 $ tar zxvf .</description>
    </item>
    
  </channel>
</rss>
