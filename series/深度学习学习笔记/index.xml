<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>《深度学习》学习笔记 on 🌀Jarson Cai&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/series/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 《深度学习》学习笔记 on 🌀Jarson Cai&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 22 Jul 2022 18:18:05 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/series/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习笔记（6-7节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</link>
      <pubDate>Fri, 22 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06-7%E8%8A%82/</guid>
      <description>深度学习（6-7节） 优化算法 Mini-batch梯度下降法 假设我们的样本数量为500w个，那么在进行梯度下降之前，我们需要先将500w个数据整合成一个大的向量$X$。Mini-batch的做法为将500w个样本按照每个子集为1000个样本等分。每个子集标记为$X^{\left{ 1\right} }X^{\left{ 2\right} },\dots,X^{\left{ 5000\right} }$。相应的，除了需要拆分$X$，也需要拆分标签$Y$，拆分的方法和$X$相同。
Mini-batch的原理是将同时原本对所有样本和标签同时进行梯度下降转变为同时只对一个子集进行梯度下降处理，处理5000次。需要注意代价函数也要改变，因为每次训练的样本个数改变了。
当你的训练集大小很大的时候，mini-batch梯度下降法比batch梯度下降法运行地更快。
batch梯度下降法和Mini-batch梯度下降法的代价随迭代的图像如下：
右边的图像出现波动的原因是：每次实现梯度下降的样本集不同，可能$X^{\left{ 1\right} }$和$Y^{\left{ 1\right} }$需要花费的代价更大，而$X^{\left{ 2\right} }$和$Y^{\left{ 2\right} }$花费的代价更少，从而形成一个噪声的现象。
那么mini-bash的大小如何决定呢？
先看两种极端情况：
如果子集的大小为m，那么mini-bash梯度下降就变成了batch梯度下降；
如果子集的大小为1，那么mini-bash梯度下降就变成了随机梯度下降法，每个样本都是一个子集；
batch梯度下降每次下降的噪声会小一点，幅度会大一点（这里的噪声是指梯度下降的方向偏离目标）；而随机梯度下降大部分时间会向着全局最小值逼近，但有时候会远离最小值（刚好该样本是一个&amp;rsquo;&amp;lsquo;坏&amp;rsquo;&amp;lsquo;样本），随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。
batch梯度下降在训练数据很大的时候，单次训练迭代时间过长，如果训练数据量较小的情况下效果较好；而随机梯度下降单次迭代很快，但却无法使用向量化技术对运算进行加速。我们的目的就是选择一个不大不小的size，使得我们的学习速率达到最快（梯度下降）。
最优的情况就是，单次选取的size大小的数据分布比较符合整体数据的分布，这样使得学习速率和运行效率都比较高。
指数加权平均 指数加权平均也称指数加权移动平均，通过它可以来计算局部的平均值，来描述数值的变化趋势，下面通过一个温度的例子来详细介绍一下。
上图是温度随时间变化的图像，我们通过温度的局部平均值（移动平均值）来描述温度的变化趋势，计算公式如下： $$ v_t=\beta v_{t-1}+(1-\beta)\theta_{t}\ v_0=0\ v_1=0.9v_0+0.1\theta_1\ v_2=0.9v_1+0.1\theta_2\ \theta 代表当天的温度，v代表局部平均值 $$ 当$\beta$为0.9时，可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
当$\beta$变得越小，移动平均值的波动越大。
通过上面的公式往下推到，可以得到$v_{100}$的表达式： $$ v_{100}=0.1\times\theta_{100}+0.1\times0.9\times\theta_{99}+\dots+0.1\times0.9^{99}\times\theta_1\ =0.1\times\sum_{i=1}^{100}0.9^{100-i}\times\theta_i $$ 当$\epsilon=1-\beta$时，$(1-\beta)^{\frac{1}{\epsilon}}\approx\frac{1}{e}\approx\frac{1}{1-\beta}$，所以可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。
简单来说，普通的加权求平均值的方法每一项的权重是$\frac{1}{n}$，指数加权平均每一项的权重是指数递减的。
指数加权平均的偏差修正 由于我们初始设置的$v_0$为0，这样会使前面几个$v_1,v_2\dots$的值与实际值相比偏小，我们通常会采取以下的办法来修正偏差： $$ v_t=\frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t} $$ 这样修正的效果为随着t的增加，分母越来越接近1。相当于时间越短，修正的幅度越大，所以这个公式主要是为了修正早期的偏差。
动量梯度下降法 我们将上面所说的指数加权平均的做法应用于神经网络的反向传播过程，如下： $$ V_{dW}=\beta V_{dW}+(1-\beta)dW\ V_{db}=\beta V_{db}+(1-\beta)db\ W:=W-\alpha V_{dW},b:=b-\alpha V_{db} $$ 这样做可以减缓梯度下降的幅度，因为梯度下降不一定朝着最快的方向前进。如下图所示：
原本为蓝色的梯度下降会变成红色，纵轴摆动的方向变小了且上下摆动的幅度均值大概为0。这样一来，即使我增加学习率或者步长也不会出现紫色线这种偏离函数的情况。</description>
    </item>
    
    <item>
      <title>深度学习笔记（4-5节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</link>
      <pubDate>Tue, 19 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04-5%E8%8A%82/</guid>
      <description>深度学习（4-5节） 多层的深层神经网络 神经网络的表示 1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4
2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量
3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$
深层网络中的前向传播 神经网络中每层的前向传播过程： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]})\ l代表层数 $$ 如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。
核对矩阵中的维数 如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。
通过一个具体的网络来手动计算一下维度：
可以写出该网络的部分参数如下： $$ n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1 $$ 由于前向传播的公式为： $$ Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\ a^{[l]}=g^{[l]}(z^{[l]}) $$
需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。
1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。
2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$
所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。
可以总结出来的是： $$ W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\ b^{[l]}的维度一定是n^{[l]}\times 1 $$ 同理在反向传播时： $$ dW和W的维度必须保持一致，db必须和b保持一致 $$ 因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。
参数vs超参数 参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$
超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。
作业三 一个神经网络工作原理的模型如下：
多层网络模型的前向传播和后向传播过程如下：
实现一个L层神经网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward import numpy as np # ------------------------------------------ def initialize_parameters_deep(layer_dims): &amp;#34;&amp;#34;&amp;#34; Arguments: layer_dims -- 包含网络中每一层的维度的Python List Returns: parameters -- Python参数字典 &amp;#34;W1&amp;#34;, &amp;#34;b1&amp;#34;, .</description>
    </item>
    
    <item>
      <title>深度学习笔记（1-3节） </title>
      <link>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</link>
      <pubDate>Tue, 12 Jul 2022 18:18:05 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2022/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01-3%E8%8A%82/</guid>
      <description>深度学习（1-3节） 深度学习介绍 线性整流函数（ReLU函数） 通常意义下，线性整流函数指代数学中的斜坡函数，即 $$ f(x)=max(0,x) $$ 而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出 $$ max(0,W^Tx+b) $$ 到下一层神经元或作为整个神经网络的输出。
神经网络介绍 神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。
监督学习 监督学习也称为带标签的学习方式。监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。
结构化数据vs非结构化数据 结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。
深度学习的准确率 取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。
Sigmoid函数 sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：
优点：平滑、易于求导。
缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。
Sigmoid函数的公式如下： $$ S(x)=\frac{1}{1+e^{-x}} $$ 函数图形如下：
深度学习基础 为了方便学习：
1.使用$(x,y)$来表示一个单独的样本
2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in {0,1}$代表标签$y$值为0或1
3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
4.$m=m_{train}+m_{test}$
5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&amp;amp;|&amp;amp;&amp;amp;|\ x^{\left( 1\right) }&amp;amp;x^{\left( 2\right) }&amp;amp;\cdots &amp;amp;x^{\left( m\right) }\ |&amp;amp;|&amp;amp;&amp;amp;|\end{matrix} \right] $，$m$是训练集样本的个数。
6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right) }&amp;amp;y^{\left( 2\right) }&amp;amp;\cdots &amp;amp;y^{\left( m\right) }\end{matrix} \right] $,$Y\in\R^{1\times m}$
Logistic回归 Logistic回归通常用于二元分类问题。
它通常的做法是将sigmoid函数作用于线性回归： $$ \hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}} $$ 这会使得$\hat{y}$的范围在0~1之间</description>
    </item>
    
  </channel>
</rss>
