<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 🌀riba2534&#39;s Blog</title>
    <link>https://caixiongjiang.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 🌀riba2534&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 02 Sep 2017 22:49:00 +0800</lastBuildDate><atom:link href="https://caixiongjiang.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>12306火车票余票查询器</title>
      <link>https://caixiongjiang.github.io/blog/2017/12306%E7%81%AB%E8%BD%A6%E7%A5%A8%E4%BD%99%E7%A5%A8%E6%9F%A5%E8%AF%A2%E5%99%A8/</link>
      <pubDate>Sat, 02 Sep 2017 22:49:00 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2017/12306%E7%81%AB%E8%BD%A6%E7%A5%A8%E4%BD%99%E7%A5%A8%E6%9F%A5%E8%AF%A2%E5%99%A8/</guid>
      <description>12306火车票余票查询器 今天写了一个12306火车票余票查询器的爬虫，在这里记录一下过程.
本博客同步更新:http://blog.csdn.net/riba2534
首先先看一下最终效果：
比如想查9月2日从西安—北京动车和特快的余票
tickets.py -dg 西安 北京 2017-09-02效果预览：
首先我们梳理一下用到的工具：
 Python3.x(必备) requests库，用来进行http请求的访问 docopt库，用来实现命令行参数处理(使用方法) prettytable,使信息以好看的表格形式呈现出来 colorama,用来设置命令行中显示的颜色  前期准备 我们的主程序就叫做tickets.py,因为我们是用的带参数的形式实现程序，所以我们要先进行参数的声明: 我们先：from docopt import docopt
&amp;quot;&amp;quot;&amp;quot;命令行火车余票查询器Usage:tickets [-gdtkz] &amp;lt;from&amp;gt; &amp;lt;to&amp;gt; &amp;lt;date&amp;gt;Options:-h,--help 显示帮助菜单-g 高铁-d 动车-t 特快-k 快速-z 直达&amp;quot;&amp;quot;&amp;quot;这些信息会存储在__doc__中，docopt会对这个信息进行解析然后返回我们需要的信息，解析的代码如下： 解析URL 既然要写爬虫，那么首先肯定是要对URL进行解析了，我们要爬取的是12306的网站，那么我们首先找到查询余票的网站：
https://kyfw.12306.cn/otn/leftTicket/init我们会看到这个： 然后进行抓包，我们会发现一个GET请求： 也就是这个链接：
https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2017-09-01&amp;amp;leftTicketDTO.from_station=XAY&amp;amp;leftTicketDTO.to_station=BJP&amp;amp;purpose_codes=ADULT我们通过观察就会发现这个请求里面有4个参数：
 leftTicketDTO.train_date=2017-09-01 leftTicketDTO.from_station=XAY leftTicketDTO.to_station=BJP purpose_codes=ADULT  我们通过观察就可以知道，这四个属性分别对应着：查询的时间，出发的站点，结束的站点，票的种类
现在发现了一个问题，出发站点和结束站点的值为什么都是英文，这些字母肯定表示的是城市的名称，那么我们现在就要去寻找这些城市的代码，我们查看网页源代码 发现了一个JS文件，打开以后会发现 这个页面竟然包含着全部的站点名称和对应的代码，所以我们只需要把这些东西解析一下，就可以得到车站和对应的代码了.
所以我们就有了思路了，我们应该先用requests来获取这个页面，然后再用正则表达式来把对应的信息解析一下，然后进行一些处理就好了，关于正则表达式的使用，不会的童鞋还是去百度百度吧~ 根据网页上面的信息，我们可以写出如下正则表达式：
([\u4e00-\u9fa5]+)\|([A-Z]+)先匹配汉字，然后后面匹配字母，中间有一个分隔符,完整的代码.
我们建一个文件parser_stations.py解析：</description>
    </item>
    
    <item>
      <title>HDU自动AC机，八小时爬上杭电首页</title>
      <link>https://caixiongjiang.github.io/blog/2017/hdu%E8%87%AA%E5%8A%A8ac%E6%9C%BA%E5%85%AB%E5%B0%8F%E6%97%B6%E7%88%AC%E4%B8%8A%E6%9D%AD%E7%94%B5%E9%A6%96%E9%A1%B5/</link>
      <pubDate>Mon, 24 Jul 2017 22:49:00 +0800</pubDate>
      
      <guid>https://caixiongjiang.github.io/blog/2017/hdu%E8%87%AA%E5%8A%A8ac%E6%9C%BA%E5%85%AB%E5%B0%8F%E6%97%B6%E7%88%AC%E4%B8%8A%E6%9D%AD%E7%94%B5%E9%A6%96%E9%A1%B5/</guid>
      <description>HDU自动AC机，八小时爬上杭电首页 前言 身为一个ACMer，整天面对电脑屏幕，一个题冥思苦想一整天，调bug调到绝望，看着自己可怜的AC数，想想都很绝望~
那么在学习之余我们也应该做一些有趣的事情，比如说 自动AC？ 体验体验AC的快感，最然是假的(捂脸)
说到自动AC，我们首先想到的是什么，当然是爬虫，说到爬虫，那么就要说一说python了，没有什么语言比它更适合了.
废话不多说，先放一张战果：
第十四名，AC率达到了百分之92，怕不怕~
工欲善其事，必先利其器，用到的工具有：
 python-3.6.2 BeautifulSoup4 库 requests库  那么现在，我们来说一说思路：
思路 获取 本来想好的思路是，在百度等搜索引擎，搜索hdu+题号，然后筛选出csdn和博客园的链接，然后保存提交的，但是学长给了一个网站，我发现了新大陆:
http://www.acmsearch.com/
就是这个网站，上面给出了，大部分题的题解，简直是开盖即食，按捺不住这种激动的心情，那么首先，就是获取链接了：
我们点击hdoj，输入题目的ID，发现底下一排排已经AC的代码，这时，看一看网页的地址栏：
http://www.acmsearch.com/article?ArticleListSearch%5BFoj%5D=hdoj&amp;amp;ArticleListSearch%5BFproblem_id%5D=2111&amp;amp;ArticleListSearch%5BFproblem_name%5D=&amp;amp;ArticleListSearch%5BFarticle_name%5D=&amp;amp;ArticleListSearch%5BFsource%5D=&amp;amp;ArticleListSearch%5BFread_num%5D=&amp;amp;ArticleListSearch%5BFstar_avg%5D=
什么鬼！竟然是一堆乱码，但是我们仔细观察就知道了，这是被URL加密过的链接，我们对这个链接进行解密，结果如下：
http://www.acmsearch.com/article?ArticleListSearch[Foj]=hdoj&amp;amp;ArticleListSearch[Fproblem_id]=2111&amp;amp;ArticleListSearch[Fproblem_name]=&amp;amp;ArticleListSearch[Farticle_name]=&amp;amp;ArticleListSearch[Fsource]=&amp;amp;ArticleListSearch[Fread_num]=&amp;amp;ArticleListSearch[Fstar_avg]=
这一次链接是不是很清晰了，里面的ID后面加数字，就是我们要搜的题目编号，我们查询的时候是需要给这个链接发GET请求就好了。
这时我们利用F12大法查看一下网页代码：
发现题解的链接都是在一个 tbody-&amp;gt;tr-&amp;gt;data-key 的标签里面，然后我们再构造链接：
http://www.acmsearch.com/article/show/+编号
这样就获得到了代码的页面,以HDU2111这个题来说明，我们获取了链接以后，一个GET请求发过去，会进入一个这样的页面：
没错，我们看到了AC代码，这时候我们要做的是把代码从网页中提取出来,那么我们继续查看网页源代码：
发现了啥，没错，代码就在一个标签名叫做 textarea 的标签下,我们只需要提取这个标签 里面的内容.
放一部分关键代码：
这时候，我们的代码就获取完毕了，这时候，重要的一步来了，我们要登录杭电并且提交代码
登陆HDU 我们先打开杭电首页:
我们把账号密码输进去，按F12监听数据抓包：
我们发现一个POST地址，这就是我们待会要登录的地址，然后看看其他发送过去的内容
我们要发送的请求要包含：
username,userpass,login分别代表账户名，密码，操作
我们还需要带上一个User-Agent,来告诉页面我们是一个正常的浏览器访问：
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36然后我们构造请求头，和发送的数据，本来我打算用urllib库里面的cooklib对象，但是requests里面直接把cookie的事情给解决了，附上官方文档的链接：
http://docs.python-requests.org/zh_CN/latest/user/quickstart.html
这一步关键代码如下：
当登录成功以后，我们要做的就是提交代码了
提交 我们找一道题目，比如2111，来进行提交，首先，先抓包：
然后进行抓包：
我们得到了post地址，
提交的时候要带上cookie，以及构造的DATA数据。
比如：
data = {&#39;check&#39;: &#39;0&#39;,&#39;problemid&#39;: str(id),&#39;language&#39;: &#39;2&#39;,&#39;usercode&#39;: str(code)}headers = {&#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.</description>
    </item>
    
  </channel>
</rss>
